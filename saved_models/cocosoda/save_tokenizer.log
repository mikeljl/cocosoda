03/23/2025 21:07:35 - INFO - __main__ -   device: cuda, n_gpu: 1
03/23/2025 21:07:37 - INFO - __main__ -    new token {'additional_special_tokens': ['keyword', 'string_content', 'raw_string_literal', 'decimal_integer_literal', '"', 'regex_flags', 'instance_variable', 'shorthand_property_identifier_pattern', 'type_identifier', 'rune_literal', 'hex_integer_literal', 'statement_identifier', 'int_literal', 'php_tag', 'shorthand_property_identifier', 'ERROR', 'string_literal', 'null_literal', 'hash_key_symbol', 'number', 'class_variable', 'package_identifier', 'separators', 'operator', 'regex_pattern', 'namespace', 'heredoc_end', 'class', 'constant', 'float_literal', 'extends', 'none', 'void_type', 'string_fragment', 'heredoc_beginning', 'character_literal', 'simple_symbol', 'global_variable', 'boolean_type', 'name', 'string', 'field_identifier', 'comment', 'boolean', 'identifier', 'escape_sequence', 'decimal_floating_point_literal', 'text', 'property_identifier', 'heredoc_content', 'label_name', 'integer']}
03/23/2025 21:07:38 - INFO - __main__ -   +-------------------------------------------------------------------+--------------+----------+
| Layer Name                                                        | Output Shape |  Param # |
+-------------------------------------------------------------------+--------------+----------+
| code_encoder_q.embeddings.word_embeddings.weight                  | [51451, 768] | 39514368 |
| code_encoder_q.embeddings.position_embeddings.weight              |  [1026, 768] |   787968 |
| code_encoder_q.embeddings.token_type_embeddings.weight            |    [10, 768] |     7680 |
| code_encoder_q.embeddings.LayerNorm.weight                        |        [768] |      768 |
| code_encoder_q.embeddings.LayerNorm.bias                          |        [768] |      768 |
| code_encoder_q.encoder.layer.0.attention.self.query.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.0.attention.self.query.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.0.attention.self.key.weight          |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.0.attention.self.key.bias            |        [768] |      768 |
| code_encoder_q.encoder.layer.0.attention.self.value.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.0.attention.self.value.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.0.attention.output.dense.weight      |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.0.attention.output.dense.bias        |        [768] |      768 |
| code_encoder_q.encoder.layer.0.attention.output.LayerNorm.weight  |        [768] |      768 |
| code_encoder_q.encoder.layer.0.attention.output.LayerNorm.bias    |        [768] |      768 |
| code_encoder_q.encoder.layer.0.intermediate.dense.weight          |  [3072, 768] |  2359296 |
| code_encoder_q.encoder.layer.0.intermediate.dense.bias            |       [3072] |     3072 |
| code_encoder_q.encoder.layer.0.output.dense.weight                |  [768, 3072] |  2359296 |
| code_encoder_q.encoder.layer.0.output.dense.bias                  |        [768] |      768 |
| code_encoder_q.encoder.layer.0.output.LayerNorm.weight            |        [768] |      768 |
| code_encoder_q.encoder.layer.0.output.LayerNorm.bias              |        [768] |      768 |
| code_encoder_q.encoder.layer.1.attention.self.query.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.1.attention.self.query.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.1.attention.self.key.weight          |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.1.attention.self.key.bias            |        [768] |      768 |
| code_encoder_q.encoder.layer.1.attention.self.value.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.1.attention.self.value.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.1.attention.output.dense.weight      |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.1.attention.output.dense.bias        |        [768] |      768 |
| code_encoder_q.encoder.layer.1.attention.output.LayerNorm.weight  |        [768] |      768 |
| code_encoder_q.encoder.layer.1.attention.output.LayerNorm.bias    |        [768] |      768 |
| code_encoder_q.encoder.layer.1.intermediate.dense.weight          |  [3072, 768] |  2359296 |
| code_encoder_q.encoder.layer.1.intermediate.dense.bias            |       [3072] |     3072 |
| code_encoder_q.encoder.layer.1.output.dense.weight                |  [768, 3072] |  2359296 |
| code_encoder_q.encoder.layer.1.output.dense.bias                  |        [768] |      768 |
| code_encoder_q.encoder.layer.1.output.LayerNorm.weight            |        [768] |      768 |
| code_encoder_q.encoder.layer.1.output.LayerNorm.bias              |        [768] |      768 |
| code_encoder_q.encoder.layer.2.attention.self.query.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.2.attention.self.query.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.2.attention.self.key.weight          |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.2.attention.self.key.bias            |        [768] |      768 |
| code_encoder_q.encoder.layer.2.attention.self.value.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.2.attention.self.value.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.2.attention.output.dense.weight      |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.2.attention.output.dense.bias        |        [768] |      768 |
| code_encoder_q.encoder.layer.2.attention.output.LayerNorm.weight  |        [768] |      768 |
| code_encoder_q.encoder.layer.2.attention.output.LayerNorm.bias    |        [768] |      768 |
| code_encoder_q.encoder.layer.2.intermediate.dense.weight          |  [3072, 768] |  2359296 |
| code_encoder_q.encoder.layer.2.intermediate.dense.bias            |       [3072] |     3072 |
| code_encoder_q.encoder.layer.2.output.dense.weight                |  [768, 3072] |  2359296 |
| code_encoder_q.encoder.layer.2.output.dense.bias                  |        [768] |      768 |
| code_encoder_q.encoder.layer.2.output.LayerNorm.weight            |        [768] |      768 |
| code_encoder_q.encoder.layer.2.output.LayerNorm.bias              |        [768] |      768 |
| code_encoder_q.encoder.layer.3.attention.self.query.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.3.attention.self.query.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.3.attention.self.key.weight          |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.3.attention.self.key.bias            |        [768] |      768 |
| code_encoder_q.encoder.layer.3.attention.self.value.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.3.attention.self.value.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.3.attention.output.dense.weight      |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.3.attention.output.dense.bias        |        [768] |      768 |
| code_encoder_q.encoder.layer.3.attention.output.LayerNorm.weight  |        [768] |      768 |
| code_encoder_q.encoder.layer.3.attention.output.LayerNorm.bias    |        [768] |      768 |
| code_encoder_q.encoder.layer.3.intermediate.dense.weight          |  [3072, 768] |  2359296 |
| code_encoder_q.encoder.layer.3.intermediate.dense.bias            |       [3072] |     3072 |
| code_encoder_q.encoder.layer.3.output.dense.weight                |  [768, 3072] |  2359296 |
| code_encoder_q.encoder.layer.3.output.dense.bias                  |        [768] |      768 |
| code_encoder_q.encoder.layer.3.output.LayerNorm.weight            |        [768] |      768 |
| code_encoder_q.encoder.layer.3.output.LayerNorm.bias              |        [768] |      768 |
| code_encoder_q.encoder.layer.4.attention.self.query.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.4.attention.self.query.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.4.attention.self.key.weight          |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.4.attention.self.key.bias            |        [768] |      768 |
| code_encoder_q.encoder.layer.4.attention.self.value.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.4.attention.self.value.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.4.attention.output.dense.weight      |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.4.attention.output.dense.bias        |        [768] |      768 |
| code_encoder_q.encoder.layer.4.attention.output.LayerNorm.weight  |        [768] |      768 |
| code_encoder_q.encoder.layer.4.attention.output.LayerNorm.bias    |        [768] |      768 |
| code_encoder_q.encoder.layer.4.intermediate.dense.weight          |  [3072, 768] |  2359296 |
| code_encoder_q.encoder.layer.4.intermediate.dense.bias            |       [3072] |     3072 |
| code_encoder_q.encoder.layer.4.output.dense.weight                |  [768, 3072] |  2359296 |
| code_encoder_q.encoder.layer.4.output.dense.bias                  |        [768] |      768 |
| code_encoder_q.encoder.layer.4.output.LayerNorm.weight            |        [768] |      768 |
| code_encoder_q.encoder.layer.4.output.LayerNorm.bias              |        [768] |      768 |
| code_encoder_q.encoder.layer.5.attention.self.query.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.5.attention.self.query.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.5.attention.self.key.weight          |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.5.attention.self.key.bias            |        [768] |      768 |
| code_encoder_q.encoder.layer.5.attention.self.value.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.5.attention.self.value.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.5.attention.output.dense.weight      |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.5.attention.output.dense.bias        |        [768] |      768 |
| code_encoder_q.encoder.layer.5.attention.output.LayerNorm.weight  |        [768] |      768 |
| code_encoder_q.encoder.layer.5.attention.output.LayerNorm.bias    |        [768] |      768 |
| code_encoder_q.encoder.layer.5.intermediate.dense.weight          |  [3072, 768] |  2359296 |
| code_encoder_q.encoder.layer.5.intermediate.dense.bias            |       [3072] |     3072 |
| code_encoder_q.encoder.layer.5.output.dense.weight                |  [768, 3072] |  2359296 |
| code_encoder_q.encoder.layer.5.output.dense.bias                  |        [768] |      768 |
| code_encoder_q.encoder.layer.5.output.LayerNorm.weight            |        [768] |      768 |
| code_encoder_q.encoder.layer.5.output.LayerNorm.bias              |        [768] |      768 |
| code_encoder_q.encoder.layer.6.attention.self.query.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.6.attention.self.query.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.6.attention.self.key.weight          |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.6.attention.self.key.bias            |        [768] |      768 |
| code_encoder_q.encoder.layer.6.attention.self.value.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.6.attention.self.value.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.6.attention.output.dense.weight      |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.6.attention.output.dense.bias        |        [768] |      768 |
| code_encoder_q.encoder.layer.6.attention.output.LayerNorm.weight  |        [768] |      768 |
| code_encoder_q.encoder.layer.6.attention.output.LayerNorm.bias    |        [768] |      768 |
| code_encoder_q.encoder.layer.6.intermediate.dense.weight          |  [3072, 768] |  2359296 |
| code_encoder_q.encoder.layer.6.intermediate.dense.bias            |       [3072] |     3072 |
| code_encoder_q.encoder.layer.6.output.dense.weight                |  [768, 3072] |  2359296 |
| code_encoder_q.encoder.layer.6.output.dense.bias                  |        [768] |      768 |
| code_encoder_q.encoder.layer.6.output.LayerNorm.weight            |        [768] |      768 |
| code_encoder_q.encoder.layer.6.output.LayerNorm.bias              |        [768] |      768 |
| code_encoder_q.encoder.layer.7.attention.self.query.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.7.attention.self.query.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.7.attention.self.key.weight          |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.7.attention.self.key.bias            |        [768] |      768 |
| code_encoder_q.encoder.layer.7.attention.self.value.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.7.attention.self.value.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.7.attention.output.dense.weight      |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.7.attention.output.dense.bias        |        [768] |      768 |
| code_encoder_q.encoder.layer.7.attention.output.LayerNorm.weight  |        [768] |      768 |
| code_encoder_q.encoder.layer.7.attention.output.LayerNorm.bias    |        [768] |      768 |
| code_encoder_q.encoder.layer.7.intermediate.dense.weight          |  [3072, 768] |  2359296 |
| code_encoder_q.encoder.layer.7.intermediate.dense.bias            |       [3072] |     3072 |
| code_encoder_q.encoder.layer.7.output.dense.weight                |  [768, 3072] |  2359296 |
| code_encoder_q.encoder.layer.7.output.dense.bias                  |        [768] |      768 |
| code_encoder_q.encoder.layer.7.output.LayerNorm.weight            |        [768] |      768 |
| code_encoder_q.encoder.layer.7.output.LayerNorm.bias              |        [768] |      768 |
| code_encoder_q.encoder.layer.8.attention.self.query.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.8.attention.self.query.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.8.attention.self.key.weight          |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.8.attention.self.key.bias            |        [768] |      768 |
| code_encoder_q.encoder.layer.8.attention.self.value.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.8.attention.self.value.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.8.attention.output.dense.weight      |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.8.attention.output.dense.bias        |        [768] |      768 |
| code_encoder_q.encoder.layer.8.attention.output.LayerNorm.weight  |        [768] |      768 |
| code_encoder_q.encoder.layer.8.attention.output.LayerNorm.bias    |        [768] |      768 |
| code_encoder_q.encoder.layer.8.intermediate.dense.weight          |  [3072, 768] |  2359296 |
| code_encoder_q.encoder.layer.8.intermediate.dense.bias            |       [3072] |     3072 |
| code_encoder_q.encoder.layer.8.output.dense.weight                |  [768, 3072] |  2359296 |
| code_encoder_q.encoder.layer.8.output.dense.bias                  |        [768] |      768 |
| code_encoder_q.encoder.layer.8.output.LayerNorm.weight            |        [768] |      768 |
| code_encoder_q.encoder.layer.8.output.LayerNorm.bias              |        [768] |      768 |
| code_encoder_q.encoder.layer.9.attention.self.query.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.9.attention.self.query.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.9.attention.self.key.weight          |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.9.attention.self.key.bias            |        [768] |      768 |
| code_encoder_q.encoder.layer.9.attention.self.value.weight        |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.9.attention.self.value.bias          |        [768] |      768 |
| code_encoder_q.encoder.layer.9.attention.output.dense.weight      |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.9.attention.output.dense.bias        |        [768] |      768 |
| code_encoder_q.encoder.layer.9.attention.output.LayerNorm.weight  |        [768] |      768 |
| code_encoder_q.encoder.layer.9.attention.output.LayerNorm.bias    |        [768] |      768 |
| code_encoder_q.encoder.layer.9.intermediate.dense.weight          |  [3072, 768] |  2359296 |
| code_encoder_q.encoder.layer.9.intermediate.dense.bias            |       [3072] |     3072 |
| code_encoder_q.encoder.layer.9.output.dense.weight                |  [768, 3072] |  2359296 |
| code_encoder_q.encoder.layer.9.output.dense.bias                  |        [768] |      768 |
| code_encoder_q.encoder.layer.9.output.LayerNorm.weight            |        [768] |      768 |
| code_encoder_q.encoder.layer.9.output.LayerNorm.bias              |        [768] |      768 |
| code_encoder_q.encoder.layer.10.attention.self.query.weight       |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.10.attention.self.query.bias         |        [768] |      768 |
| code_encoder_q.encoder.layer.10.attention.self.key.weight         |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.10.attention.self.key.bias           |        [768] |      768 |
| code_encoder_q.encoder.layer.10.attention.self.value.weight       |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.10.attention.self.value.bias         |        [768] |      768 |
| code_encoder_q.encoder.layer.10.attention.output.dense.weight     |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.10.attention.output.dense.bias       |        [768] |      768 |
| code_encoder_q.encoder.layer.10.attention.output.LayerNorm.weight |        [768] |      768 |
| code_encoder_q.encoder.layer.10.attention.output.LayerNorm.bias   |        [768] |      768 |
| code_encoder_q.encoder.layer.10.intermediate.dense.weight         |  [3072, 768] |  2359296 |
| code_encoder_q.encoder.layer.10.intermediate.dense.bias           |       [3072] |     3072 |
| code_encoder_q.encoder.layer.10.output.dense.weight               |  [768, 3072] |  2359296 |
| code_encoder_q.encoder.layer.10.output.dense.bias                 |        [768] |      768 |
| code_encoder_q.encoder.layer.10.output.LayerNorm.weight           |        [768] |      768 |
| code_encoder_q.encoder.layer.10.output.LayerNorm.bias             |        [768] |      768 |
| code_encoder_q.encoder.layer.11.attention.self.query.weight       |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.11.attention.self.query.bias         |        [768] |      768 |
| code_encoder_q.encoder.layer.11.attention.self.key.weight         |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.11.attention.self.key.bias           |        [768] |      768 |
| code_encoder_q.encoder.layer.11.attention.self.value.weight       |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.11.attention.self.value.bias         |        [768] |      768 |
| code_encoder_q.encoder.layer.11.attention.output.dense.weight     |   [768, 768] |   589824 |
| code_encoder_q.encoder.layer.11.attention.output.dense.bias       |        [768] |      768 |
| code_encoder_q.encoder.layer.11.attention.output.LayerNorm.weight |        [768] |      768 |
| code_encoder_q.encoder.layer.11.attention.output.LayerNorm.bias   |        [768] |      768 |
| code_encoder_q.encoder.layer.11.intermediate.dense.weight         |  [3072, 768] |  2359296 |
| code_encoder_q.encoder.layer.11.intermediate.dense.bias           |       [3072] |     3072 |
| code_encoder_q.encoder.layer.11.output.dense.weight               |  [768, 3072] |  2359296 |
| code_encoder_q.encoder.layer.11.output.dense.bias                 |        [768] |      768 |
| code_encoder_q.encoder.layer.11.output.LayerNorm.weight           |        [768] |      768 |
| code_encoder_q.encoder.layer.11.output.LayerNorm.bias             |        [768] |      768 |
| code_encoder_q.pooler.dense.weight                                |   [768, 768] |   589824 |
| code_encoder_q.pooler.dense.bias                                  |        [768] |      768 |
+-------------------------------------------------------------------+--------------+----------+
03/23/2025 21:07:38 - INFO - __main__ -   Training/evaluation parameters Namespace(agg_way='avg', aug_type_way='random_replace_type', code_length=256, codebase_file='dataset/ruby/codebase.jsonl', config_name='microsoft/unixcoder-base', couninue_pre_train_data_files=['toy_dataset/java/train.jsonl', 'toy_dataset/javascript/train.jsonl', 'toy_dataset/python/train.jsonl', 'toy_dataset/php/train.jsonl', 'toy_dataset/go/train.jsonl', 'toy_dataset/ruby/train.jsonl'], data_aug_type='other', data_flow_length=0, debug=False, device=device(type='cuda'), do_avg=False, do_continue_pre_trained=False, do_eval=False, do_fine_tune=False, do_ineer_loss=False, do_multi_lang_continue_pre_train=True, do_single_lang_continue_pre_train=False, do_test=True, do_train=False, do_whitening=False, do_zero_short=False, epoch=50, eval_batch_size=32, eval_data_file='dataset/ruby/valid.jsonl', eval_frequency=100, fp16=False, gradient_accumulation_steps=1, hidden_size=768, lang='ruby', learning_rate=2e-05, loaded_codebert_model_filename=None, loaded_model_filename=None, local_rank=-1, logging_steps=50, max_codeblock_num=10, max_grad_norm=1.0, max_steps=100000, mlm_probability=0.1, mlp=False, moco_dim=768, moco_k=1024, moco_m=0.999, moco_t=0.07, moco_type='encoder_queue', model_name_or_path='microsoft/unixcoder-base', model_type='multi-loss-cocosoda', n_debug_samples=100, n_gpu=1, nl_length=128, num_train_epochs=10, num_warmup_steps=0, only_save_the_nl_code_vec=False, output_dir='./saved_models/cocosoda/', print_align_unif_loss=False, save_evaluation_reuslt=False, save_evaluation_reuslt_dir=None, save_steps=1000, seed=123456, test_data_file='dataset/ruby/test.jsonl', time_score=1, tokenizer_name='microsoft/unixcoder-base', train_batch_size=64, train_data_file='dataset/ruby/train.jsonl', use_best_mrr_model=False, weight_decay=0.01)
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 0
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', '@', '_Override', '_public', '_Image', 'Source', '_apply', '_(', '_Image', 'Source', '_input', '_)', '_{', '_final', '_int', '_[', '_]', '_[', '_]', '_pixel', 'Matrix', '_=', '_new', '_int', '_[', '_3', '_]', '_[', '_3', '_]', '_;', '_int', '_w', '_=', '_input', '_.', '_getWidth', '_(', '_)', '_;', '_int', '_h', '_=', '_input', '_.', '_getHeight', '_(', '_)', '_;', '_int', '_[', '_]', '_[', '_]', '_output', '_=', '_new', '_int', '_[', '_h', '_]', '_[', '_w', '_]', '_;', '_for', '_(', '_int', '_j', '_=', '_1', '_;', '_j', '_<', '_h', '_-', '_1', '_;', '_j', '_++', '_)', '_{', '_for', '_(', '_int', '_i', '_=', '_1', '_;', '_i', '_<', '_w', '_-', '_1', '_;', '_i', '_++', '_)', '_{', '_pixel', 'Matrix', '_[', '_0', '_]', '_[', '_0', '_]', '_=', '_input', '_.', '_get', 'R', '_(', '_i', '_-', '_1', '_,', '_j', '_-', '_1', '_)', '_;', '_pixel', 'Matrix', '_[', '_0', '_]', '_[', '_1', '_]', '_=', '_input', '_.', '_get', 'RGB', '_(', '_i', '_-', '_1', '_,', '_j', '_)', '_;', '_pixel', 'Matrix', '_[', '_0', '_]', '_[', '_2', '_]', '_=', '_input', '_.', '_get', 'RGB', '_(', '_i', '_-', '_1', '_,', '_j', '_+', '_1', '_)', '_;', '_pixel', 'Matrix', '_[', '_1', '_]', '_[', '_0', '_]', '_=', '_input', '_.', '_get', 'RGB', '_(', '_i', '_,', '_j', '_-', '_1', '_)', '_;', '_pixel', 'Matrix', '_[', '_1', '_]', '_[', '_2', '_]', '_=', '_input', '_.', '_get', 'RGB', '_(', '_i', '_,', '_j', '_+', '_1', '_)', '_;', '_pixel', 'Matrix', '_[', '_2', '_]', '_[', '_0', '_]', '_=', '_input', '_.', '_get', 'RGB', '_(', '_i', '_+', '_1', '_,', '_j', '_-', '_1', '_)', '_;', '_pixel', 'Matrix', '_[', '_2', '_]', '_[', '_1', '_]', '_=', '_input', '_.', '_get', 'RGB', '_(', '_i', '_+', '_1', '_,', '_j', '_)', '_;', '_pixel', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 150 19505 1240 6085 1768 5230 400 6085 1768 1586 743 399 1920 554 626 2406 626 2406 5578 3679 385 579 554 626 995 2406 626 995 2406 2476 554 477 385 1586 746 32671 400 743 2476 554 566 385 1586 746 32720 400 743 2476 554 626 2406 626 2406 1721 385 579 554 626 566 2406 626 477 2406 2476 563 400 554 913 385 524 2476 913 517 566 581 524 2476 913 1932 743 399 563 400 554 548 385 524 2476 548 517 477 581 524 2476 548 1932 743 399 5578 3679 626 461 2406 626 461 2406 385 1586 746 744 168 400 548 581 524 2019 913 581 524 743 2476 5578 3679 626 461 2406 626 524 2406 385 1586 746 744 7664 400 548 581 524 2019 913 743 2476 5578 3679 626 461 2406 626 688 2406 385 1586 746 744 7664 400 548 581 524 2019 913 513 524 743 2476 5578 3679 626 524 2406 626 461 2406 385 1586 746 744 7664 400 548 2019 913 581 524 743 2476 5578 3679 626 524 2406 626 688 2406 385 1586 746 744 7664 400 548 2019 913 513 524 743 2476 5578 3679 626 688 2406 626 461 2406 385 1586 746 744 7664 400 548 513 524 2019 913 581 524 743 2476 5578 3679 626 688 2406 626 524 2406 385 1586 746 744 7664 400 548 513 524 2019 913 743 2476 5578 2
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Expect', 's', '_a', '_height', '_mat', '_as', '_input', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 7871 201 434 3082 5772 880 1586 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 1
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'public', '_<', '_L', 'extends', 'Listener', '_>', '_void', '_pop', 'Event', '_(', '_Event', '_<', '_?', '_,', '_L', '_>', '_expected', '_)', '_{', '_synchronized', '_(', '_this', '_.', '_stack', '_)', '_{', '_final', '_Event', '_<', '_?', '_,', '_?', '_>', '_actual', '_=', '_this', '_.', '_stack', '_.', '_pop', '_(', '_)', '_;', '_if', '_(', '_actual', '_!=', '_expected', '_)', '_{', '_throw', '_new', '_IllegalStateException', '_(', '_String', '_.', '_format', '_(', '"', 'Un', 'balanced', '_pop', ':', '_expected', "_'%", 's', "'", '_but', '_encountered', "_'%", 's', "'", '"', ',', '_expected', '_.', '_get', 'Listener', 'Class', '_(', '_)', '_,', '_actual', '_)', '_)', '_;', '_}', '_}', '_}', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 653 517 747 13125 2486 711 723 5012 1089 400 3916 517 999 2019 747 711 2048 743 399 9401 400 547 746 3325 743 399 1920 3916 517 999 2019 999 711 3780 385 547 746 3325 746 5012 400 743 2476 462 400 3780 620 2048 743 399 1185 579 16219 400 1167 746 2021 400 120 965 37707 5012 144 2048 3421 201 125 2107 17038 3421 201 125 120 130 2048 746 744 2486 1128 400 743 2019 3780 743 743 2476 425 425 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'P', 'ops', '_the', '_top', '_event', '_off', '_the', '_current', '_event', '_stack', '_.', '_This', '_action', '_has', '_to', '_be', '_performed', '_immediately', '_after', '_the', '_event', '_has', '_been', '_dispatched', '_to', '_all', '_listeners', '_.', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 166 2489 448 3194 1488 3413 448 1434 1488 3325 746 1600 2657 1559 508 661 13181 10086 2493 448 1488 1559 3022 43340 508 1345 11839 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 2
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'protected', '_void', '_modify', '_(', '_Transaction', '_t', '_)', '_{', '_try', '_{', '_this', '_.', '_lock', '_.', '_write', 'Lock', '_(', '_)', '_.', '_lock', '_(', '_)', '_;', '_t', '_.', '_perform', '_(', '_)', '_;', '_}', '_finally', '_{', '_this', '_.', '_lock', '_.', '_write', 'Lock', '_(', '_)', '_.', '_unlock', '_(', '_)', '_;', '_}', '_}', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 1933 723 8660 400 13081 422 743 399 1568 399 547 746 3505 746 2250 2896 400 743 746 3505 400 743 2476 422 746 4729 400 743 2476 425 6110 399 547 746 3505 746 2250 2896 400 743 746 14552 400 743 2476 425 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Executes', '_the', '_given', '_transaction', '_within', '_the', '_con', 'text', 'of', '_a', '_write', '_lock', '_.', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 40551 448 2076 4993 5289 448 549 625 757 434 2250 3505 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 0
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'function', '_(', '_state', '_,', '_action', '_)', '_{', '_return', '__', '_.', '_defaults', '_(', '_{', '_isValid', 'ating', '_:', '_action', '_.', '_isValid', 'ating', '_,', '_last', 'Action', '_:', '_IS', '_', 'VALID', 'ATING', '_}', '_,', '_state', '_)', '_}', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 618 400 1404 2019 2657 743 399 483 623 746 7470 400 399 17002 2335 545 2657 746 17002 2335 2019 2023 1888 545 1947 181 7477 40173 425 2019 1404 743 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Update', '_is', '_validating', '_result', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 2056 555 38924 1046 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 1
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'function', '_add', 'Widget', 'For', 'Filter', '_(', '_view', '_,', '_filter', '_,', '_edit', 'Mode', 'Hint', '_)', '_{', '_var', '_grid', 'ster', '_=', '_view', '_.', '__', 'widgets', 'Grid', 'ster', '_;', '_var', '_row', '_=', '_filter', '_.', '_row', '_||', '_1', '_;', '_var', '_col', '_=', '_filter', '_.', '_col', '_||', '_1', '_;', '_var', '_size', 'X', '_=', '_filter', '_.', '_size', '_', 'x', '_||', '_3', '_;', '_var', '_size', 'Y', '_=', '_filter', '_.', '_size', '_', 'y', '_||', '_3', '_;', '_var', '_el', '_=', '_grid', 'ster', '_.', '_add', '_', 'widget', '_(', "_'<", 'div', 'class', '=', '"', 'widget', 'Outer', 'Frame', '"', '></', 'div', ">'", '_,', '_size', 'X', '_,', '_size', 'Y', '_,', '_col', '_,', '_row', '_)', '_;', '_var', '_frame', 'View', '_=', '_new', '_Widget', 'Frame', 'View', '_(', '_{', '_model', '_:', '_filter', '_}', '_)', '_;', '_//', '_render', ',', '_and', '_render', '_content', '_of', '_widget', '_frame', '_view', '_.', '_render', 'Sub', 'view', '_(', '_frame', 'View', '_,', '_el', '_[', '_0', '_]', '_)', '_;', '_frame', 'View', '_.', '_render', 'Content', '_(', '_)', '_;', '_//', '_link', '_element', '_and', '_view', '_so', '_we', '_can', ':', '_//', '_a', ')', '_on', '_remove', ',', '_get', '_to', '_the', '_HTMLElement', '_from', '_the', '_Widget', 'Frame', 'View', '_//', '_b', ')', '_on', '_resize', ',', '_get', '_to', '_the', '_Widget', 'Frame', 'View', '_from', '_the', '_HTMLElement', '_frame', 'View', '_.', '_grid', 'ster', 'Hook', '_=', '_el', '_[', '_0', '_]', '_;', '_$', '_(', '_el', '_[', '_0', '_]', '_)', '_.', '_data', '_(', "_'", 'spot', 'Widget', 'Frame', 'View', "'", '_,', '_frame', 'View', '_)', '_;', '_//', '_try', '_to', '_initialize', '_and', '_render', '_possibly', '_present', '_data', '_//', '_only', '_follow', '_edit', 'Mode', 'Hint', '_when', '_the', '_widget', '_is', '_configured', ',', '_default', '_to', '_true', '_var', '_chart', 'View', '_=', '_frame', 'View', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 618 1103 3104 1459 2274 400 2859 2019 2866 2019 7277 1649 7641 743 399 660 6335 7400 385 2859 746 623 14718 3981 7400 2476 660 2562 385 2866 746 2562 853 524 2476 660 1253 385 2866 746 1253 853 524 2476 660 1014 174 385 2866 746 1014 181 206 853 995 2476 660 1014 175 385 2866 746 1014 181 207 853 995 2476 660 2207 385 6335 7400 746 1103 181 4635 400 3478 1873 1149 147 120 4635 13369 1803 120 2698 1873 5103 2019 1014 174 2019 1014 175 2019 1253 2019 2562 743 2476 660 2568 1336 385 579 19258 1803 1336 400 399 2358 545 2866 425 743 2476 518 4342 130 706 4342 2264 595 6949 2568 2859 746 4342 1682 1791 400 2568 1336 2019 2207 626 461 2406 743 2476 2568 1336 746 4342 1646 400 743 2476 518 2905 1398 706 2859 1769 937 1347 144 518 434 127 854 3033 130 744 508 448 27430 1029 448 19258 1803 1336 518 442 127 854 11625 130 744 508 448 19258 1803 1336 1029 448 27430 2568 1336 746 6335 7400 7211 385 2207 626 461 2406 2476 440 400 2207 626 461 2406 743 746 869 400 464 17499 3104 1803 1336 125 2019 2568 1336 743 2476 518 1568 508 6211 706 4342 15700 5564 869 518 1845 3651 7277 1649 7641 1672 448 6949 555 10320 130 1361 508 769 660 9818 1336 385 2568 1336 2
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Add', '_a', '_widget', '_to', '_the', '_analyze', '_page', '_for', '_the', '_given', '_filter', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 972 434 6949 508 448 25087 2303 563 448 2076 2866 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 2
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'function', '_in', 'Range', '_(', '_value', '_,', '_min', '_,', '_max', '_)', '_{', '_const', '_int', '_=', '_parseInt', '_(', '_value', '_,', '_10', '_)', '_return', '_(', '_`', '_${', '_int', '_}', '_`', '_===', '_`', '_${', '_value', '_.', '_replace', '_(', '_/', '_^', '0', '_/', '_,', "_''", '_)', '_}', '_`', '_&&', '_int', '_>=', '_min', '_&&', '_int', '_<=', '_max', '_)', '_}', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 618 488 2228 400 767 2019 2069 2019 1621 743 399 925 554 385 9998 400 767 2019 1865 743 483 400 1222 5593 554 425 1222 1246 1222 5593 767 746 4126 400 1017 3855 134 1017 2019 3606 743 425 1222 698 554 1451 2069 698 554 1826 1621 743 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Determine', '_if', '_value', '_is', '_within', '_a', '_numeric', '_range', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 17591 462 767 555 5289 434 10397 1780 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 0
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_split', '_', 'phy', 'log', 'en', 'y', '_(', '_p', '_,', '_level', '_=', '"', 's', '"', ')', '_:', '_level', '_=', '_level', '_+', '"', '__', '"', 'result', '_=', '_p', '_.', '_split', '_(', '_level', '_)', '_return', '_result', '_[', '_0', '_]', '_+', '_level', '_+', '_result', '_[', '_1', '_]', '_.', '_split', '_(', '"', ';', '"', ')', '_[', '_0', '_]', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 729 5192 181 3258 896 386 207 400 428 2019 3144 385 120 201 120 127 545 3144 385 3144 513 120 876 120 1125 385 428 746 5192 400 3144 743 483 1046 626 461 2406 513 3144 513 1046 626 524 2406 746 5192 400 120 145 120 127 626 461 2406 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Return', '_either', '_the', '_full', '_or', '_truncated', '_version', '_of', '_a', '_Q', 'II', 'ME', '_-', '_formatted', '_taxonomy', 'string', '.', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 1675 4759 448 3662 872 19307 2229 595 434 1152 4300 1098 581 10440 29021 571 132 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 1
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_ensure', '_', 'dir', '_(', '_d', '_)', '_:', '_if', '_not', '_os', '_.', '_path', '_.', '_exists', '_(', '_d', '_)', '_:', '_try', '_:', '_os', '_.', '_m', 'akedirs', '_(', '_d', '_)', '_except', '_OSError', '_as', '_oe', '_:', '_#', '_should', '_not', '_happen', '_with', '_os', '.', 'makedirs', '_#', '_ENOENT', ':', '_No', '_such', '_file', '_or', '_directory', '_if', '_os', '_.', '_errno', '_==', '_errno', '_.', '_ENOENT', '_:', '_msg', '_=', '_tw', 'dd', '_(', '"', '"', '"', 'One', '_or', '_more', '_directories', '_in', '_the', '_path', '_({})', '_do', '_not', '_exist', '.', '_If', 'Ċ', '__________________________', '_you', '_are', '_specifying', '_a', '_new', '_directory', '_for', '_output', ',', '_please', '_ensure', 'Ċ', '__________________________', '_all', '_other', '_directories', '_in', '_the', '_path', '_currently', '_exist', '.', '"', '"', '"', ')', '_return', '_msg', '_.', '_format', '_(', '_d', '_)', '_else', '_:', '_msg', '_=', '_tw', 'dd', '_(', '"', '"', '"', 'An', '_error', '_occurred', '_trying', '_to', '_create', '_the', '_output', '_directory', 'Ċ', '__________________________', '_({})', '_with', '_message', ':', '_{}', '"', '"', '"', ')', '_return', '_msg', '_.', '_format', '_(', '_d', '_,', '_oe', '_.', '_strerror', '_)', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 729 6229 181 1282 400 480 743 545 462 800 2215 746 1391 746 4534 400 480 743 545 1568 545 2215 746 446 23328 400 480 743 3552 22934 880 44902 545 830 1570 800 7564 918 2215 132 24429 830 41059 144 4038 5632 1012 872 3456 462 2215 746 2341 550 2341 746 41059 545 2345 385 7916 443 400 120 120 120 3533 872 2726 11613 488 448 1391 46072 1000 800 3040 132 1359 317 4584 2713 1147 15323 434 579 3456 563 1721 130 13874 6229 317 4584 1345 1946 11613 488 448 1391 6418 3040 132 120 120 120 127 483 2345 746 2021 400 480 743 669 545 2345 385 7916 443 400 120 120 120 1088 843 10058 11749 508 1738 448 1721 3456 317 4584 46072 918 1841 144 2334 120 120 120 127 483 2345 746 2021 400 480 2019 44902 746 20115 743 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Check', '_to', '_make', '_sure', '_the', '_supplied', '_directory', '_path', '_does', '_not', '_exist', '_if', '_so', '_create', '_it', '_.', '_The', '_method', '_catch', 'es', '_OSError', '_exceptions', '_and', '_returns', '_a', '_desc', 'riptive', '_message', '_instead', '_of', '_re', '_-', '_raising', '_the', '_error', '_.', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 1749 508 2002 3984 448 8813 3456 1391 2129 800 3040 462 1769 1738 835 746 1044 1454 2092 482 22934 12300 706 2060 434 2162 44105 1841 4488 595 479 581 47183 448 843 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 2
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_file', '_', 'handle', '_(', '_fn', 'h', '_,', '_mode', '_=', '"', 'r', 'U', '"', ')', '_:', '_handle', '_=', '_None', '_if', '_isinstance', '_(', '_fn', 'h', '_,', '_file', '_)', '_:', '_if', '_fn', 'h', '_.', '_closed', '_:', '_raise', '_ValueError', '_(', '"', 'Input', '_file', '_is', '_closed', '.', '"', ')', '_handle', '_=', '_fn', 'h', '_elif', '_isinstance', '_(', '_fn', 'h', '_,', '_str', '_)', '_:', '_handle', '_=', '_open', '_(', '_fn', 'h', '_,', '_mode', '_)', '_return', '_handle', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 729 1012 181 2133 400 4065 190 2019 2119 385 120 200 171 120 127 545 2384 385 1938 462 5408 400 4065 190 2019 1012 743 545 462 4065 190 746 8264 545 3085 6052 400 120 1834 1012 555 8264 132 120 127 2384 385 4065 190 3625 5408 400 4065 190 2019 1113 743 545 2384 385 2717 400 4065 190 2019 2119 743 483 2384 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Takes', '_either', '_a', '_file', '_path', '_or', '_an', '_open', '_file', '_handle', '_checks', '_validity', '_and', '_returns', '_an', '_open', '_file', '_handle', '_or', '_raises', '_an', '_appropriate', '_Exception', '_.', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 27408 4759 434 1012 1391 872 817 2717 1012 2384 7825 25911 706 2060 817 2717 1012 2384 872 23154 817 7900 2654 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 0
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'public', '_function', '_on', 'Channel', 'Pre', 'Delete', '_(', '_Resource', 'Controller', 'Event', '_$', '_event', '_)', '_:', '_void', '_{', '_$', '_channel', '_=', '_$', '_event', '_->', '_get', 'Subject', '_(', '_)', '_;', '_if', '_(', '_!', '_$', '_channel', '_instanceof', '_Channel', 'Interface', '_)', '_{', '_throw', '_new', '_Unexpected', 'TypeException', '_(', '_$', '_channel', '_,', '_Channel', 'Interface', '_::', 'class', ')', '_;', '_}', '_$', '_results', '_=', '_$', '_this', '_->', '_channel', 'Repository', '_->', '_find', 'By', '_(', '_[', "_'", 'enabled', "'", '_=>', '_true', '_]', '_)', '_;', '_if', '_(', '_!', '_$', '_results', '_||', '_(', '_count', '_(', '_$', '_results', '_)', '_===', '_1', '_&&', '_current', '_(', '_$', '_results', '_)', '_===', '_$', '_channel', '_)', '_)', '_{', '_$', '_event', '_->', '_stop', '_(', "_'", 'sy', 'li', 'us', '.', 'channel', '.', 'delete', '_', 'error', "'", '_)', '_;', '_}', '_}', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 653 603 854 3267 1782 2843 400 7606 3357 1089 440 1488 743 545 723 399 440 3225 385 440 1488 1703 744 7562 400 743 2476 462 400 552 440 3225 3052 11322 2285 743 399 1185 579 23297 48098 400 440 3225 2019 11322 2285 5431 1149 127 2476 425 440 3286 385 440 547 1703 3225 5674 1703 2523 1541 400 626 464 4499 125 771 769 2406 743 2476 462 400 552 440 3286 853 400 1752 400 440 3286 743 1246 524 698 1434 400 440 3286 743 1246 440 3225 743 743 399 440 1488 1703 4244 400 464 6028 617 516 132 2505 132 2602 181 1082 125 743 2476 425 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Prevent', '_channel', '_deletion', '_if', '_no', '_more', '_channels', '_enabled', '_.', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 42669 3225 19744 462 1375 2726 8630 5334 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 1
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'public', '_function', '_get', 'Tax', 'Total', '_(', '_)', '_:', '_int', '_{', '_$', '_tax', 'Total', '_=', '_0', '_;', '_foreach', '_(', '_$', '_this', '_->', '_get', 'Adjustments', '_(', '_Adjust', 'ment', 'Interface', '_::', '_T', 'AX', '_', 'ADJUST', 'MENT', '_)', '_as', '_$', '_tax', 'Adjustment', '_)', '_{', '_$', '_tax', 'Total', '_+=', '_$', '_tax', 'Adjustment', '_->', '_get', 'Amount', '_(', '_)', '_;', '_}', '_foreach', '_(', '_$', '_this', '_->', '_units', '_as', '_$', '_unit', '_)', '_{', '_$', '_tax', 'Total', '_+=', '_$', '_unit', '_->', '_get', 'Tax', 'Total', '_(', '_)', '_;', '_}', '_return', '_$', '_tax', 'Total', '_;', '_}', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 653 603 744 11266 4703 400 743 545 554 399 440 14990 4703 385 461 2476 2315 400 440 547 1703 744 39930 400 16203 564 2285 5431 515 3383 181 44094 4332 743 880 440 14990 21585 743 399 440 14990 4703 1054 440 14990 21585 1703 744 6933 400 743 2476 425 2315 400 440 547 1703 10931 880 440 5108 743 399 440 14990 4703 1054 440 5108 1703 744 11266 4703 400 743 2476 425 483 440 14990 4703 2476 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Returns', '_sum', '_of', '_ne', 'utral', '_and', '_non', '_ne', 'utral', '_tax', '_adjust', 'ments', '_on', '_order', '_item', '_and', '_total', '_tax', '_of', '_units', '_.', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 2853 3863 595 1472 22943 706 2514 1472 22943 14990 7780 2067 854 2991 1573 706 3704 14990 595 10931 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 2
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'private', '_function', '_is', 'Last', 'Enabled', 'Entity', '_(', '_$', '_result', '_,', '_$', '_entity', '_)', '_:', '_bool', '_{', '_return', '_!', '_$', '_result', '_||', '_0', '_===', '_count', '_(', '_$', '_result', '_)', '_||', '_(', '_1', '_===', '_count', '_(', '_$', '_result', '_)', '_&&', '_$', '_entity', '_===', '_(', '_$', '_result', '_instanceof', '_\\', '_Iterator', '_?', '_$', '_result', '_->', '_current', '_(', '_)', '_:', '_current', '_(', '_$', '_result', '_)', '_)', '_)', '_;', '_}', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 1335 603 555 2954 3060 2268 400 440 1046 2019 440 4498 743 545 1223 399 483 552 440 1046 853 461 1246 1752 400 440 1046 743 853 400 524 1246 1752 400 440 1046 743 698 440 4498 1246 400 440 1046 3052 1216 13119 999 440 1046 1703 1434 400 743 545 1434 400 440 1046 743 743 743 2476 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'If', '_no', '_entity', '_matched', '_the', '_query', '_criteria', '_or', '_a', '_single', '_entity', '_matched', '_which', '_is', '_the', '_same', '_as', '_the', '_entity', '_being', '_validated', '_the', '_entity', '_is', '_the', '_last', '_enabled', '_entity', '_available', '_.', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 2815 1375 4498 5865 448 2616 14677 872 434 3501 4498 5865 1839 555 448 2641 880 448 4498 4251 20709 448 4498 555 448 2023 5334 4498 3777 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 0
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'func', '_getAll', 'Dep', 'Types', '_(', '_)', '_[', '_]', 'string', '{', '_dep', 'Types', '_:=', '_make', '_(', '_[', '_]', 'string', ',', '_0', '_,', '_len', '_(', '_cmds', '_)', '_)', '_Ċ', '_for', '_dep', 'Type', '_:=', '_range', '_cmds', '_{', '_dep', 'Types', '_=', '_append', '_(', '_dep', 'Types', '_,', '_dep', 'Type', '_)', '_Ċ', '_}', '_Ċ', '_sort', '_.', '_Strings', '_(', '_dep', 'Types', '_)', '_Ċ', '_return', '_dep', 'Types', '_Ċ', '_}', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 763 21556 15010 2531 400 743 626 2406 571 209 13994 2531 716 2002 400 626 2406 571 130 461 2019 1015 400 22803 743 743 1022 563 13994 641 716 1780 22803 399 13994 2531 385 2746 400 13994 2531 2019 13994 641 743 1022 425 1022 4821 746 23012 400 13994 2531 743 1022 483 13994 2531 1022 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'getAll', 'Dep', 'Types', '_returns', '_a', '_sorted', '_list', '_of', 'name', 's', '_of', '_all', '_dep', '_type', '_commands', '_.', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 12199 15010 2531 2060 434 6977 1182 595 616 201 595 1345 13994 889 7997 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 1
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'func', '_get', 'Io', 'Progress', 'Reader', '_(', '_label', 'string', ',', '_res', '_*', '_http', '_.', '_Response', '_)', '_io', '_.', '_Reader', '_{', '_prefix', '_:=', '"', '"', '+', '_label', '_Ċ', '_fmt', 'Bytes', 'Size', '_:=', '_18', '_Ċ', '_bar', 'Size', '_:=', '_int', '64', '_(', '_80', '_-', '_len', '_(', '_prefix', '_)', '_-', '_fmt', 'Bytes', 'Size', '_)', '_Ċ', '_bar', '_:=', '_i', 'opro', 'gress', '_.', '_Draw', 'Text', 'Format', 'Bar', 'For', 'W', '_(', '_bar', 'Size', '_,', '_os', '_.', '_Std', 'err', '_)', '_Ċ', '_fmt', 'func', '_:=', '_func', '_(', '_progress', '_,', '_total', '_int', '64', '_)', 'string', '{', '_//', '_Content', '-', 'Length', '_is', '_set', '_to', '_-', '1', '_when', '_unknown', '.', '_if', '_total', '_==', '_-', '_1', '_{', '_return', '_fmt', '_.', '_S', 'printf', '_(', '"', '"', ',', '_prefix', '_,', '_i', 'opro', 'gress', '_.', '_Byte', 'Unit', 'Str', '_(', '_progress', '_)', '_,', '_)', '_Ċ', '_}', '_Ċ', '_return', '_fmt', '_.', '_S', 'printf', '_(', '"', '"', ',', '_prefix', '_,', '_bar', '_(', '_progress', '_,', '_total', '_)', '_,', '_i', 'opro', 'gress', '_.', '_Draw', 'Text', 'Format', 'Bytes', '_(', '_progress', '_,', '_total', '_)', '_,', '_)', '_Ċ', '_}', '_Ċ', '_return', '_&', '_i', 'opro', 'gress', '_.', '_Reader', '_{', '_Reader', '_:', '_res', '_.', '_Body', '_,', '_Size', '_:', '_res', '_.', '_Content', 'Length', '_,', '_Draw', 'Func', '_:', '_i', 'opro', 'gress', '_.', '_Draw', 'Terminal', 'f', '_(', '_os', '_.', '_Std', 'err', '_,', '_fmt', 'func', '_)', '_,', '_Draw', 'Interval', '_:', '_time', '_.', '_Second', '_,', '_}', '_Ċ', '_}', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 763 744 8499 4909 2692 400 2649 571 130 705 426 2014 746 6397 743 3095 746 15471 399 3603 716 120 120 129 2649 1022 2771 2240 939 716 7837 1022 5252 939 716 554 848 400 8967 581 1015 400 3603 743 581 2771 2240 939 743 1022 5252 716 548 31375 2639 746 8548 1072 1660 3238 1459 173 400 5252 939 2019 2215 746 21619 484 743 1022 2771 763 716 2666 400 6687 2019 3704 554 848 743 571 209 518 8174 131 1463 555 827 508 581 135 1672 7078 132 462 3704 550 581 524 399 483 2771 746 506 1481 400 120 120 130 3603 2019 548 31375 2639 746 3446 2762 1564 400 6687 743 2019 743 1022 425 1022 483 2771 746 506 1481 400 120 120 130 3603 2019 5252 400 6687 2019 3704 743 2019 548 31375 2639 746 8548 1072 1660 2240 400 6687 2019 3704 743 2019 743 1022 425 1022 483 519 548 31375 2639 746 15471 399 15471 545 705 746 12795 2019 6198 545 705 746 8174 1463 2019 8548 3017 545 548 31375 2639 746 8548 12650 188 400 2215 746 21619 484 2019 2771 763 743 2019 8548 4655 545 1342 746 14795 2019 425 1022 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'get', 'Io', 'Progress', 'Reader', '_returns', '_a', '_reader', '_that', '_wraps', '_the', '_HTTP', '_response', '_body', '_so', '_it', '_prints', '_a', '_pretty', '_progress', '_bar', '_when', '_reading', '_data', '_from', '_it', '_.', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 459 8499 4909 2692 2060 434 4636 922 28232 448 4383 1925 3444 1769 835 22199 434 15344 6687 5252 1672 8267 869 1029 835 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 2
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'func', '_(', '_f', '_*', '_remove', 'OnClose', '_)', '_Close', '_(', '_)', '_error', '_{', '_if', '_f', '_==', '_nil', '_||', '_f', '_.', '_File', '_==', '_nil', '_{', '_return', '_nil', '_Ċ', '_}', 'name', ':', '=', '_f', '_.', '_File', '_.', '_Name', '_(', '_)', '_Ċ', '_if', '_err', '_:=', '_f', '_.', '_File', '_.', '_Close', '_(', '_)', '_;', '_err', '_!=', '_nil', '_{', '_return', '_err', '_Ċ', '_}', '_Ċ', '_if', '_err', '_:=', '_os', '_.', '_Remove', '_(', 'name', ')', '_;', '_err', '_!=', '_nil', '_&&', '_!', '_os', '_.', '_Is', 'NotExist', '_(', '_err', '_)', '_{', '_return', '_err', '_Ċ', '_}', '_Ċ', '_return', '_nil', '_Ċ', '_}', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 763 400 412 426 3033 45359 743 5832 400 743 843 399 462 412 550 845 853 412 746 2536 550 845 399 483 845 1022 425 616 144 147 412 746 2536 746 3725 400 743 1022 462 573 716 412 746 2536 746 5832 400 743 2476 573 620 845 399 483 573 1022 425 1022 462 573 716 2215 746 5294 400 616 127 2476 573 620 845 698 552 2215 746 3028 41664 400 573 743 399 483 573 1022 425 1022 483 845 1022 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Close', '_closes', '_the', '_file', '_and', '_then', '_removes', '_it', '_from', '_disk', '_.', '_No', '_error', '_is', '_returned', '_if', '_the', '_file', '_did', '_not', '_exist', '_at', '_the', '_point', '_of', '_removal', '_.', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 3108 19735 448 1012 706 2270 15719 835 1029 8236 746 4038 843 555 2862 462 448 1012 6088 800 3040 1035 448 1704 595 23066 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 0
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_render', '_', 'body', '_(', '_con', 'text', ',', '_options', '_)', '_if', '_options', '_.', '_key', '?', '_(', '_:', 'partial', '_)', '_[', '_render', '_', 'partial', '_(', '_con', 'text', ',', '_options', '_)', '_]', '_else', '_Streaming', 'Template', 'Renderer', '_.', '_new', '_(', '_@', 'lookup', '_', 'con', 'text', ')', '_.', '_render', '_(', '_con', 'text', ',', '_options', '_)', '_end', '_end', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 729 4342 181 1995 400 549 625 130 1466 743 462 1466 746 1129 149 400 545 7609 743 626 4342 181 7609 400 549 625 130 1466 743 2406 669 47128 3057 6412 746 579 400 890 4961 181 525 625 127 746 4342 400 549 625 130 1466 743 1013 1013 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Render', '_but', '_returns', '_a', '_valid', '_R', 'ack', '_body', '_.', '_If', '_fib', 'ers', '_are', '_defined', '_we', '_return', '_a', '_streaming', '_body', '_that', '_renders', '_the', '_template', '_piece', '_by', '_piece', '_.', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 3726 2107 2060 434 1976 821 598 3444 746 1359 24766 560 1147 3474 937 483 434 22676 3444 922 40840 448 3636 18781 1243 18781 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 1
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_attribute', '_', 'missing', '_(', '_match', '_,', '_*', '_args', '_,', '_&', '_block', '_)', '___', 'send', '__', '_(', '_match', '_.', '_target', '_,', '_match', '_.', '_attr', '_', 'name', ',', '_args', '_,', '_block', '_)', '_end', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 729 2416 181 8487 400 1655 2019 426 1822 2019 519 1818 743 1267 2414 876 400 1655 746 1744 2019 1655 746 3526 181 616 130 1822 2019 1818 743 1013 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', '+', '_attribute', '_', 'missing', '_+', '_is', '_like', '_+', '_method', '_', 'missing', '_+', '_but', '_for', '_attributes', '_.', '_When', '_+', '_method', '_', 'missing', '_+', '_is', '_called', '_we', '_check', '_to', '_see', '_if', '_there', '_is', '_a', '_matching', '_attribute', '_method', '_.', '_If', '_so', '_we', '_tell', '_+', '_attribute', '_', 'missing', '_+', '_to', '_dispatch', '_the', '_attribute', '_.', '_This', '_method', '_can', '_be', '_overloaded', '_to', '_customize', '_the', '_behavior', '_.', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 129 2416 181 8487 513 555 4401 513 1454 181 8487 513 2107 563 4402 746 5919 513 1454 181 8487 513 555 2953 937 1382 508 3986 462 2550 555 434 6506 2416 1454 746 1359 1769 937 11931 513 2416 181 8487 513 508 9363 448 2416 746 1600 1454 1347 661 45869 508 36145 448 9050 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   *** Example ***
03/23/2025 21:07:42 - INFO - __main__ -   idx: 2
03/23/2025 21:07:42 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_matched', '_', 'attribute', '_', 'method', '_(', '_method', '_', 'name', ')', '_matches', '_=', '_self', '_.', 'class', '.', '_send', '_(', '_:', 'attribute', '_', 'method', '_', 'matchers', '_', 'matching', '_,', '_method', '_', 'name', ')', '_matches', '_.', '_detect', '_{', '_|', '_match', '_|', '_attribute', '_', 'method', '?', '_(', '_match', '_.', '_attr', '_', 'name', ')', '_}', '_end', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   code_ids: 0 6 2 729 5865 181 2163 181 1521 400 1454 181 616 127 5288 385 1358 746 1149 132 2904 400 545 2163 181 1521 181 38734 181 13575 2019 1454 181 616 127 5288 746 10241 399 649 1655 649 2416 181 1521 149 400 1655 746 3526 181 616 127 425 1013 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Returns', '_a', '_struct', '_representing', '_the', '_matching', '_attribute', '_method', '_.', '_The', '_struct', '_s', '_attributes', '_are', '_prefix', '_base', '_and', '_suffix', '_.', '</s>']
03/23/2025 21:07:42 - INFO - __main__ -   nl_ids: 0 6 2 2853 434 1277 8466 448 6506 2416 1454 746 1044 1277 431 4402 1147 3603 1712 706 8436 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
03/23/2025 21:07:42 - INFO - __main__ -   ***** Running training *****
03/23/2025 21:07:42 - INFO - __main__ -     Num examples = 192
03/23/2025 21:07:42 - INFO - __main__ -     Num Epochs = 10
03/23/2025 21:07:42 - INFO - __main__ -     Num quene = 1024
03/23/2025 21:07:42 - INFO - __main__ -     Instantaneous batch size per GPU = 64
03/23/2025 21:07:42 - INFO - __main__ -     Total train batch size  = 64
/scratch/st-jjnunez-1/jialinlu/scratch/CoCoSoDa
/scratch/st-jjnunez-1/jialinlu/scratch/CoCoSoDa
/scratch/st-jjnunez-1/jialinlu/scratch/CoCoSoDa
/scratch/st-jjnunez-1/jialinlu/scratch/CoCoSoDa
/scratch/st-jjnunez-1/jialinlu/scratch/CoCoSoDa
/scratch/st-jjnunez-1/jialinlu/scratch/CoCoSoDa
Traceback (most recent call last):
  File "run.py", line 1196, in <module>
    main()
  File "run.py", line 1156, in main
    multi_lang_continue_pre_train(args, model, tokenizer, pool)
  File "run.py", line 719, in multi_lang_continue_pre_train
    batch=next(train_dataloader)
StopIteration
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          03/23/2025 21:09:31 - INFO - __main__ -   step 1000 loss 0.83639
03/23/2025 21:09:33 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 1000 loss: -0.874677
/home/jialinlu/.conda/envs/CoCoSoDa/lib/python3.6/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
03/23/2025 21:09:38 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/23/2025 21:09:38 - INFO - __main__ -     Num queries = 1400
03/23/2025 21:09:38 - INFO - __main__ -     Num codes = 4360
03/23/2025 21:09:38 - INFO - __main__ -     Batch size = 32
03/23/2025 21:10:05 - INFO - __main__ -     eval_mrr = 0.767
03/23/2025 21:10:05 - INFO - __main__ -     ********************
03/23/2025 21:10:05 - INFO - __main__ -     Best mrr:0.767
03/23/2025 21:10:05 - INFO - __main__ -     ********************
03/23/2025 21:10:08 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-best-mrr/model.bin
03/23/2025 21:10:08 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-1000-0.767
03/23/2025 21:10:11 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/23/2025 21:13:01 - INFO - __main__ -   step 1100 loss 0.75431
03/23/2025 21:13:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 1100 loss: -0.647289
03/23/2025 21:15:52 - INFO - __main__ -   step 1200 loss 0.69994
03/23/2025 21:15:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 1200 loss: -0.728531
03/23/2025 21:18:44 - INFO - __main__ -   step 1300 loss 0.70195
03/23/2025 21:18:46 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 1300 loss: -0.697983
03/23/2025 21:21:35 - INFO - __main__ -   step 1400 loss 0.52
03/23/2025 21:21:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 1400 loss: -0.512553
03/23/2025 21:24:27 - INFO - __main__ -   step 1500 loss 0.59772
03/23/2025 21:24:29 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 1500 loss: -0.654832
03/23/2025 21:27:19 - INFO - __main__ -   step 1600 loss 0.52432
03/23/2025 21:27:21 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 1600 loss: -0.530255
03/23/2025 21:30:11 - INFO - __main__ -   step 1700 loss 0.46478
03/23/2025 21:30:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 1700 loss: -0.45391
03/23/2025 21:33:02 - INFO - __main__ -   step 1800 loss 0.43374
03/23/2025 21:33:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 1800 loss: -0.372325
03/23/2025 21:35:54 - INFO - __main__ -   step 1900 loss 0.39123
03/23/2025 21:35:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 1900 loss: -0.424137
03/23/2025 21:38:46 - INFO - __main__ -   step 2000 loss 0.44186
03/23/2025 21:38:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 2000 loss: -0.458637
03/23/2025 21:38:53 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/23/2025 21:38:53 - INFO - __main__ -     Num queries = 1400
03/23/2025 21:38:53 - INFO - __main__ -     Num codes = 4360
03/23/2025 21:38:53 - INFO - __main__ -     Batch size = 32
03/23/2025 21:39:20 - INFO - __main__ -     eval_mrr = 0.784
03/23/2025 21:39:20 - INFO - __main__ -     ********************
03/23/2025 21:39:20 - INFO - __main__ -     Best mrr:0.784
03/23/2025 21:39:20 - INFO - __main__ -     ********************
03/23/2025 21:39:23 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-best-mrr/model.bin
03/23/2025 21:39:23 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-2000-0.784
03/23/2025 21:39:26 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/23/2025 21:42:16 - INFO - __main__ -   step 2100 loss 0.4164
03/23/2025 21:42:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 2100 loss: -0.421133
03/23/2025 21:45:08 - INFO - __main__ -   step 2200 loss 0.35751
03/23/2025 21:45:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 2200 loss: -0.420781
03/23/2025 21:47:59 - INFO - __main__ -   step 2300 loss 0.41788
03/23/2025 21:48:01 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 2300 loss: -0.437185
03/23/2025 21:50:51 - INFO - __main__ -   step 2400 loss 0.38616
03/23/2025 21:50:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 2400 loss: -0.346751
03/23/2025 21:53:43 - INFO - __main__ -   step 2500 loss 0.3952
03/23/2025 21:53:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 2500 loss: -0.372589
03/23/2025 21:56:35 - INFO - __main__ -   step 2600 loss 0.40458
03/23/2025 21:56:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 2600 loss: -0.425244
03/23/2025 21:59:26 - INFO - __main__ -   step 2700 loss 0.37403
03/23/2025 21:59:28 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 2700 loss: -0.386613
03/23/2025 22:02:18 - INFO - __main__ -   step 2800 loss 0.37531
03/23/2025 22:02:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 2800 loss: -0.39653
03/23/2025 22:05:10 - INFO - __main__ -   step 2900 loss 0.38306
03/23/2025 22:05:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 2900 loss: -0.402196
03/23/2025 22:08:01 - INFO - __main__ -   step 3000 loss 0.3319
03/23/2025 22:08:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 3000 loss: -0.363443
03/23/2025 22:08:08 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/23/2025 22:08:08 - INFO - __main__ -     Num queries = 1400
03/23/2025 22:08:08 - INFO - __main__ -     Num codes = 4360
03/23/2025 22:08:08 - INFO - __main__ -     Batch size = 32
03/23/2025 22:08:36 - INFO - __main__ -     eval_mrr = 0.777
03/23/2025 22:08:37 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-3000-0.777
03/23/2025 22:08:39 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/23/2025 22:11:29 - INFO - __main__ -   step 3100 loss 0.3608
03/23/2025 22:11:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 3100 loss: -0.340162
03/23/2025 22:14:20 - INFO - __main__ -   step 3200 loss 0.27553
03/23/2025 22:14:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 3200 loss: -0.290137
03/23/2025 22:17:12 - INFO - __main__ -   step 3300 loss 0.35143
03/23/2025 22:17:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 3300 loss: -0.308632
03/23/2025 22:20:03 - INFO - __main__ -   step 3400 loss 0.34944
03/23/2025 22:20:05 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 3400 loss: -0.347358
03/23/2025 22:22:55 - INFO - __main__ -   step 3500 loss 0.35764
03/23/2025 22:22:57 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 3500 loss: -0.325771
03/23/2025 22:25:47 - INFO - __main__ -   step 3600 loss 0.33552
03/23/2025 22:25:49 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 3600 loss: -0.334378
03/23/2025 22:28:39 - INFO - __main__ -   step 3700 loss 0.3269
03/23/2025 22:28:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 3700 loss: -0.319324
03/23/2025 22:31:30 - INFO - __main__ -   step 3800 loss 0.37526
03/23/2025 22:31:32 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 3800 loss: -0.3312
03/23/2025 22:34:22 - INFO - __main__ -   step 3900 loss 0.35818
03/23/2025 22:34:24 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 3900 loss: -0.38989
03/23/2025 22:37:13 - INFO - __main__ -   step 4000 loss 0.32168
03/23/2025 22:37:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 4000 loss: -0.344355
03/23/2025 22:37:20 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/23/2025 22:37:20 - INFO - __main__ -     Num queries = 1400
03/23/2025 22:37:20 - INFO - __main__ -     Num codes = 4360
03/23/2025 22:37:20 - INFO - __main__ -     Batch size = 32
03/23/2025 22:37:48 - INFO - __main__ -     eval_mrr = 0.785
03/23/2025 22:37:48 - INFO - __main__ -     ********************
03/23/2025 22:37:48 - INFO - __main__ -     Best mrr:0.785
03/23/2025 22:37:48 - INFO - __main__ -     ********************
03/23/2025 22:37:51 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-best-mrr/model.bin
03/23/2025 22:37:51 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-4000-0.785
03/23/2025 22:37:54 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/23/2025 22:40:43 - INFO - __main__ -   step 4100 loss 0.32954
03/23/2025 22:40:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 4100 loss: -0.29595
03/23/2025 22:43:35 - INFO - __main__ -   step 4200 loss 0.33366
03/23/2025 22:43:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 4200 loss: -0.319939
03/23/2025 22:46:27 - INFO - __main__ -   step 4300 loss 0.37944
03/23/2025 22:46:28 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 4300 loss: -0.390523
03/23/2025 22:49:18 - INFO - __main__ -   step 4400 loss 0.34175
03/23/2025 22:49:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 4400 loss: -0.354548
03/23/2025 22:52:10 - INFO - __main__ -   step 4500 loss 0.41781
03/23/2025 22:52:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 4500 loss: -0.410914
03/23/2025 22:55:02 - INFO - __main__ -   step 4600 loss 0.29804
03/23/2025 22:55:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 4600 loss: -0.271185
03/23/2025 22:57:54 - INFO - __main__ -   step 4700 loss 0.32588
03/23/2025 22:57:55 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 4700 loss: -0.330374
03/23/2025 23:00:45 - INFO - __main__ -   step 4800 loss 0.33063
03/23/2025 23:00:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 4800 loss: -0.315541
03/23/2025 23:03:37 - INFO - __main__ -   step 4900 loss 0.32375
03/23/2025 23:03:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 4900 loss: -0.295633
03/23/2025 23:06:29 - INFO - __main__ -   step 5000 loss 0.36785
03/23/2025 23:06:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 5000 loss: -0.355104
03/23/2025 23:06:35 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/23/2025 23:06:35 - INFO - __main__ -     Num queries = 1400
03/23/2025 23:06:35 - INFO - __main__ -     Num codes = 4360
03/23/2025 23:06:35 - INFO - __main__ -     Batch size = 32
03/23/2025 23:07:03 - INFO - __main__ -     eval_mrr = 0.785
03/23/2025 23:07:29 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-5000-0.785
03/23/2025 23:07:32 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/23/2025 23:10:22 - INFO - __main__ -   step 5100 loss 0.28672
03/23/2025 23:10:24 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 5100 loss: -0.30578
03/23/2025 23:13:13 - INFO - __main__ -   step 5200 loss 0.34766
03/23/2025 23:13:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 5200 loss: -0.349111
03/23/2025 23:16:05 - INFO - __main__ -   step 5300 loss 0.37812
03/23/2025 23:16:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 5300 loss: -0.354877
03/23/2025 23:18:57 - INFO - __main__ -   step 5400 loss 0.36411
03/23/2025 23:18:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 5400 loss: -0.376058
03/23/2025 23:21:49 - INFO - __main__ -   step 5500 loss 0.31064
03/23/2025 23:21:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 5500 loss: -0.366561
03/23/2025 23:24:40 - INFO - __main__ -   step 5600 loss 0.3111
03/23/2025 23:24:42 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 5600 loss: -0.308652
03/23/2025 23:27:32 - INFO - __main__ -   step 5700 loss 0.37963
03/23/2025 23:27:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 5700 loss: -0.378144
03/23/2025 23:30:24 - INFO - __main__ -   step 5800 loss 0.36275
03/23/2025 23:30:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 5800 loss: -0.374127
03/23/2025 23:33:15 - INFO - __main__ -   step 5900 loss 0.34089
03/23/2025 23:33:17 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 5900 loss: -0.374662
03/23/2025 23:36:07 - INFO - __main__ -   step 6000 loss 0.29599
03/23/2025 23:36:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 6000 loss: -0.302767
03/23/2025 23:36:13 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/23/2025 23:36:13 - INFO - __main__ -     Num queries = 1400
03/23/2025 23:36:13 - INFO - __main__ -     Num codes = 4360
03/23/2025 23:36:13 - INFO - __main__ -     Batch size = 32
03/23/2025 23:36:42 - INFO - __main__ -     eval_mrr = 0.782
03/23/2025 23:37:12 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-6000-0.782
03/23/2025 23:37:14 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/23/2025 23:40:03 - INFO - __main__ -   step 6100 loss 0.31446
03/23/2025 23:40:05 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 6100 loss: -0.358706
03/23/2025 23:42:55 - INFO - __main__ -   step 6200 loss 0.32008
03/23/2025 23:42:57 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 6200 loss: -0.29665
03/23/2025 23:45:46 - INFO - __main__ -   step 6300 loss 0.32149
03/23/2025 23:45:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 6300 loss: -0.359766
03/23/2025 23:48:38 - INFO - __main__ -   step 6400 loss 0.31793
03/23/2025 23:48:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 6400 loss: -0.343557
03/23/2025 23:51:30 - INFO - __main__ -   step 6500 loss 0.33652
03/23/2025 23:51:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 6500 loss: -0.339751
03/23/2025 23:54:21 - INFO - __main__ -   step 6600 loss 0.30614
03/23/2025 23:54:23 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 6600 loss: -0.346496
03/23/2025 23:57:13 - INFO - __main__ -   step 6700 loss 0.34994
03/23/2025 23:57:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 6700 loss: -0.330601
03/24/2025 00:00:05 - INFO - __main__ -   step 6800 loss 0.33616
03/24/2025 00:00:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 6800 loss: -0.31854
03/24/2025 00:02:57 - INFO - __main__ -   step 6900 loss 0.32553
03/24/2025 00:02:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 6900 loss: -0.354207
03/24/2025 00:05:48 - INFO - __main__ -   step 7000 loss 0.331
03/24/2025 00:05:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 7000 loss: -0.356748
03/24/2025 00:05:55 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 00:05:55 - INFO - __main__ -     Num queries = 1400
03/24/2025 00:05:55 - INFO - __main__ -     Num codes = 4360
03/24/2025 00:05:55 - INFO - __main__ -     Batch size = 32
03/24/2025 00:06:23 - INFO - __main__ -     eval_mrr = 0.79
03/24/2025 00:06:23 - INFO - __main__ -     ********************
03/24/2025 00:06:23 - INFO - __main__ -     Best mrr:0.79
03/24/2025 00:06:23 - INFO - __main__ -     ********************
03/24/2025 00:06:26 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-best-mrr/model.bin
03/24/2025 00:06:27 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-7000-0.79
03/24/2025 00:06:29 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 00:09:19 - INFO - __main__ -   step 7100 loss 0.34443
03/24/2025 00:09:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 7100 loss: -0.30468
03/24/2025 00:12:10 - INFO - __main__ -   step 7200 loss 0.33531
03/24/2025 00:12:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 7200 loss: -0.346846
03/24/2025 00:15:02 - INFO - __main__ -   step 7300 loss 0.32724
03/24/2025 00:15:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 7300 loss: -0.309389
03/24/2025 00:17:54 - INFO - __main__ -   step 7400 loss 0.25019
03/24/2025 00:17:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 7400 loss: -0.241828
03/24/2025 00:20:46 - INFO - __main__ -   step 7500 loss 0.29232
03/24/2025 00:20:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 7500 loss: -0.30699
03/24/2025 00:23:38 - INFO - __main__ -   step 7600 loss 0.33072
03/24/2025 00:23:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 7600 loss: -0.310473
03/24/2025 00:26:29 - INFO - __main__ -   step 7700 loss 0.39377
03/24/2025 00:26:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 7700 loss: -0.356478
03/24/2025 00:29:21 - INFO - __main__ -   step 7800 loss 0.31887
03/24/2025 00:29:23 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 7800 loss: -0.279858
03/24/2025 00:32:13 - INFO - __main__ -   step 7900 loss 0.35264
03/24/2025 00:32:14 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 7900 loss: -0.387219
03/24/2025 00:35:04 - INFO - __main__ -   step 8000 loss 0.36293
03/24/2025 00:35:06 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 8000 loss: -0.390391
03/24/2025 00:35:11 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 00:35:11 - INFO - __main__ -     Num queries = 1400
03/24/2025 00:35:11 - INFO - __main__ -     Num codes = 4360
03/24/2025 00:35:11 - INFO - __main__ -     Batch size = 32
03/24/2025 00:35:39 - INFO - __main__ -     eval_mrr = 0.781
03/24/2025 00:35:48 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-8000-0.781
03/24/2025 00:35:50 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 00:38:40 - INFO - __main__ -   step 8100 loss 0.30835
03/24/2025 00:38:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 8100 loss: -0.295444
03/24/2025 00:41:31 - INFO - __main__ -   step 8200 loss 0.35955
03/24/2025 00:41:33 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 8200 loss: -0.305707
03/24/2025 00:44:23 - INFO - __main__ -   step 8300 loss 0.30365
03/24/2025 00:44:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 8300 loss: -0.290725
03/24/2025 00:47:15 - INFO - __main__ -   step 8400 loss 0.31528
03/24/2025 00:47:17 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 8400 loss: -0.337966
03/24/2025 00:50:07 - INFO - __main__ -   step 8500 loss 0.35832
03/24/2025 00:50:08 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 8500 loss: -0.330752
03/24/2025 00:52:58 - INFO - __main__ -   step 8600 loss 0.33624
03/24/2025 00:53:00 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 8600 loss: -0.313199
03/24/2025 00:55:50 - INFO - __main__ -   step 8700 loss 0.34567
03/24/2025 00:55:52 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 8700 loss: -0.317318
03/24/2025 00:58:42 - INFO - __main__ -   step 8800 loss 0.36365
03/24/2025 00:58:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 8800 loss: -0.359404
03/24/2025 01:01:33 - INFO - __main__ -   step 8900 loss 0.34278
03/24/2025 01:01:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 8900 loss: -0.3862
03/24/2025 01:04:25 - INFO - __main__ -   step 9000 loss 0.35305
03/24/2025 01:04:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 9000 loss: -0.303363
03/24/2025 01:04:31 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 01:04:31 - INFO - __main__ -     Num queries = 1400
03/24/2025 01:04:31 - INFO - __main__ -     Num codes = 4360
03/24/2025 01:04:31 - INFO - __main__ -     Batch size = 32
03/24/2025 01:04:59 - INFO - __main__ -     eval_mrr = 0.787
03/24/2025 01:05:00 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-9000-0.787
03/24/2025 01:05:03 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 01:07:53 - INFO - __main__ -   step 9100 loss 0.3655
03/24/2025 01:07:55 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 9100 loss: -0.329569
03/24/2025 01:10:44 - INFO - __main__ -   step 9200 loss 0.35964
03/24/2025 01:10:46 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 9200 loss: -0.352558
03/24/2025 01:13:36 - INFO - __main__ -   step 9300 loss 0.34749
03/24/2025 01:13:38 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 9300 loss: -0.334489
03/24/2025 01:16:27 - INFO - __main__ -   step 9400 loss 0.34657
03/24/2025 01:16:29 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 9400 loss: -0.324691
03/24/2025 01:19:19 - INFO - __main__ -   step 9500 loss 0.40032
03/24/2025 01:19:21 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 9500 loss: -0.379867
03/24/2025 01:22:10 - INFO - __main__ -   step 9600 loss 0.33492
03/24/2025 01:22:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 9600 loss: -0.352062
03/24/2025 01:25:02 - INFO - __main__ -   step 9700 loss 0.34221
03/24/2025 01:25:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 9700 loss: -0.351364
03/24/2025 01:27:54 - INFO - __main__ -   step 9800 loss 0.35046
03/24/2025 01:27:55 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 9800 loss: -0.348804
03/24/2025 01:30:45 - INFO - __main__ -   step 9900 loss 0.34309
03/24/2025 01:30:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 9900 loss: -0.395302
03/24/2025 01:33:37 - INFO - __main__ -   step 10000 loss 0.34181
03/24/2025 01:33:38 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 10000 loss: -0.320662
03/24/2025 01:33:43 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 01:33:43 - INFO - __main__ -     Num queries = 1400
03/24/2025 01:33:43 - INFO - __main__ -     Num codes = 4360
03/24/2025 01:33:43 - INFO - __main__ -     Batch size = 32
03/24/2025 01:34:11 - INFO - __main__ -     eval_mrr = 0.79
03/24/2025 01:34:12 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-10000-0.79
03/24/2025 01:34:15 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 01:37:05 - INFO - __main__ -   step 10100 loss 0.34412
03/24/2025 01:37:06 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 10100 loss: -0.387263
03/24/2025 01:39:56 - INFO - __main__ -   step 10200 loss 0.31091
03/24/2025 01:39:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 10200 loss: -0.312572
03/24/2025 01:42:48 - INFO - __main__ -   step 10300 loss 0.30352
03/24/2025 01:42:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 10300 loss: -0.29271
03/24/2025 01:45:39 - INFO - __main__ -   step 10400 loss 0.35957
03/24/2025 01:45:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 10400 loss: -0.326421
03/24/2025 01:48:31 - INFO - __main__ -   step 10500 loss 0.29896
03/24/2025 01:48:33 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 10500 loss: -0.350161
03/24/2025 01:51:23 - INFO - __main__ -   step 10600 loss 0.35364
03/24/2025 01:51:24 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 10600 loss: -0.357055
03/24/2025 01:54:14 - INFO - __main__ -   step 10700 loss 0.38628
03/24/2025 01:54:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 10700 loss: -0.348119
03/24/2025 01:57:06 - INFO - __main__ -   step 10800 loss 0.29103
03/24/2025 01:57:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 10800 loss: -0.317854
03/24/2025 01:59:57 - INFO - __main__ -   step 10900 loss 0.35239
03/24/2025 01:59:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 10900 loss: -0.365553
03/24/2025 02:02:49 - INFO - __main__ -   step 11000 loss 0.3347
03/24/2025 02:02:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 11000 loss: -0.366144
03/24/2025 02:02:55 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 02:02:55 - INFO - __main__ -     Num queries = 1400
03/24/2025 02:02:55 - INFO - __main__ -     Num codes = 4360
03/24/2025 02:02:55 - INFO - __main__ -     Batch size = 32
03/24/2025 02:03:24 - INFO - __main__ -     eval_mrr = 0.783
03/24/2025 02:03:24 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-11000-0.783
03/24/2025 02:03:27 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 02:06:17 - INFO - __main__ -   step 11100 loss 0.31872
03/24/2025 02:06:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 11100 loss: -0.316363
03/24/2025 02:09:08 - INFO - __main__ -   step 11200 loss 0.37746
03/24/2025 02:09:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 11200 loss: -0.41383
03/24/2025 02:12:00 - INFO - __main__ -   step 11300 loss 0.33994
03/24/2025 02:12:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 11300 loss: -0.387313
03/24/2025 02:14:51 - INFO - __main__ -   step 11400 loss 0.32409
03/24/2025 02:14:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 11400 loss: -0.348656
03/24/2025 02:17:43 - INFO - __main__ -   step 11500 loss 0.34878
03/24/2025 02:17:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 11500 loss: -0.35322
03/24/2025 02:20:34 - INFO - __main__ -   step 11600 loss 0.33636
03/24/2025 02:20:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 11600 loss: -0.327573
03/24/2025 02:23:26 - INFO - __main__ -   step 11700 loss 0.33058
03/24/2025 02:23:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 11700 loss: -0.336509
03/24/2025 02:26:17 - INFO - __main__ -   step 11800 loss 0.33684
03/24/2025 02:26:19 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 11800 loss: -0.327284
03/24/2025 02:29:09 - INFO - __main__ -   step 11900 loss 0.37633
03/24/2025 02:29:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 11900 loss: -0.392216
03/24/2025 02:32:00 - INFO - __main__ -   step 12000 loss 0.36095
03/24/2025 02:32:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 12000 loss: -0.382447
03/24/2025 02:32:07 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 02:32:07 - INFO - __main__ -     Num queries = 1400
03/24/2025 02:32:07 - INFO - __main__ -     Num codes = 4360
03/24/2025 02:32:07 - INFO - __main__ -     Batch size = 32
03/24/2025 02:32:35 - INFO - __main__ -     eval_mrr = 0.788
03/24/2025 02:32:35 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-12000-0.788
03/24/2025 02:32:38 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 02:35:28 - INFO - __main__ -   step 12100 loss 0.34758
03/24/2025 02:35:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 12100 loss: -0.353194
03/24/2025 02:38:19 - INFO - __main__ -   step 12200 loss 0.29689
03/24/2025 02:38:21 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 12200 loss: -0.276505
03/24/2025 02:41:11 - INFO - __main__ -   step 12300 loss 0.314
03/24/2025 02:41:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 12300 loss: -0.361957
03/24/2025 02:44:02 - INFO - __main__ -   step 12400 loss 0.22656
03/24/2025 02:44:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 12400 loss: -0.186683
03/24/2025 02:46:54 - INFO - __main__ -   step 12500 loss 0.35815
03/24/2025 02:46:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 12500 loss: -0.410041
03/24/2025 02:49:46 - INFO - __main__ -   step 12600 loss 0.32882
03/24/2025 02:49:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 12600 loss: -0.303346
03/24/2025 02:52:37 - INFO - __main__ -   step 12700 loss 0.34841
03/24/2025 02:52:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 12700 loss: -0.356831
03/24/2025 02:55:29 - INFO - __main__ -   step 12800 loss 0.27198
03/24/2025 02:55:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 12800 loss: -0.29261
03/24/2025 02:58:20 - INFO - __main__ -   step 12900 loss 0.32538
03/24/2025 02:58:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 12900 loss: -0.306704
03/24/2025 03:01:12 - INFO - __main__ -   step 13000 loss 0.31839
03/24/2025 03:01:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 13000 loss: -0.322725
03/24/2025 03:01:25 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 03:01:25 - INFO - __main__ -     Num queries = 1400
03/24/2025 03:01:25 - INFO - __main__ -     Num codes = 4360
03/24/2025 03:01:25 - INFO - __main__ -     Batch size = 32
03/24/2025 03:01:53 - INFO - __main__ -     eval_mrr = 0.784
03/24/2025 03:01:54 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-13000-0.784
03/24/2025 03:01:57 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 03:04:46 - INFO - __main__ -   step 13100 loss 0.34975
03/24/2025 03:04:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 13100 loss: -0.373072
03/24/2025 03:07:38 - INFO - __main__ -   step 13200 loss 0.35786
03/24/2025 03:07:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 13200 loss: -0.360375
03/24/2025 03:10:29 - INFO - __main__ -   step 13300 loss 0.33602
03/24/2025 03:10:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 13300 loss: -0.317457
03/24/2025 03:13:20 - INFO - __main__ -   step 13400 loss 0.33102
03/24/2025 03:13:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 13400 loss: -0.37192
03/24/2025 03:16:12 - INFO - __main__ -   step 13500 loss 0.33496
03/24/2025 03:16:14 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 13500 loss: -0.29094
03/24/2025 03:19:04 - INFO - __main__ -   step 13600 loss 0.33691
03/24/2025 03:19:05 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 13600 loss: -0.343336
03/24/2025 03:21:55 - INFO - __main__ -   step 13700 loss 0.36038
03/24/2025 03:21:57 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 13700 loss: -0.366907
03/24/2025 03:24:47 - INFO - __main__ -   step 13800 loss 0.35421
03/24/2025 03:24:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 13800 loss: -0.332961
03/24/2025 03:27:38 - INFO - __main__ -   step 13900 loss 0.3203
03/24/2025 03:27:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 13900 loss: -0.291464
03/24/2025 03:30:30 - INFO - __main__ -   step 14000 loss 0.35577
03/24/2025 03:30:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 14000 loss: -0.338613
03/24/2025 03:30:36 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 03:30:36 - INFO - __main__ -     Num queries = 1400
03/24/2025 03:30:36 - INFO - __main__ -     Num codes = 4360
03/24/2025 03:30:36 - INFO - __main__ -     Batch size = 32
03/24/2025 03:31:05 - INFO - __main__ -     eval_mrr = 0.786
03/24/2025 03:31:05 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-14000-0.786
03/24/2025 03:31:08 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 03:33:57 - INFO - __main__ -   step 14100 loss 0.31553
03/24/2025 03:33:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 14100 loss: -0.335648
03/24/2025 03:36:49 - INFO - __main__ -   step 14200 loss 0.29185
03/24/2025 03:36:51 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 14200 loss: -0.294039
03/24/2025 03:39:40 - INFO - __main__ -   step 14300 loss 0.33187
03/24/2025 03:39:42 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 14300 loss: -0.338804
03/24/2025 03:42:32 - INFO - __main__ -   step 14400 loss 0.3593
03/24/2025 03:42:33 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 14400 loss: -0.348179
03/24/2025 03:45:23 - INFO - __main__ -   step 14500 loss 0.37485
03/24/2025 03:45:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 14500 loss: -0.419534
03/24/2025 03:48:15 - INFO - __main__ -   step 14600 loss 0.31482
03/24/2025 03:48:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 14600 loss: -0.333958
03/24/2025 03:51:06 - INFO - __main__ -   step 14700 loss 0.3468
03/24/2025 03:51:08 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 14700 loss: -0.363791
03/24/2025 03:53:57 - INFO - __main__ -   step 14800 loss 0.25572
03/24/2025 03:53:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 14800 loss: -0.274871
03/24/2025 03:56:49 - INFO - __main__ -   step 14900 loss 0.2709
03/24/2025 03:56:51 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 14900 loss: -0.287314
03/24/2025 03:59:40 - INFO - __main__ -   step 15000 loss 0.32075
03/24/2025 03:59:42 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 15000 loss: -0.318925
03/24/2025 03:59:47 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 03:59:47 - INFO - __main__ -     Num queries = 1400
03/24/2025 03:59:47 - INFO - __main__ -     Num codes = 4360
03/24/2025 03:59:47 - INFO - __main__ -     Batch size = 32
03/24/2025 04:00:15 - INFO - __main__ -     eval_mrr = 0.79
03/24/2025 04:00:16 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-15000-0.79
03/24/2025 04:00:18 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 04:03:08 - INFO - __main__ -   step 15100 loss 0.26965
03/24/2025 04:03:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 15100 loss: -0.287597
03/24/2025 04:05:59 - INFO - __main__ -   step 15200 loss 0.28436
03/24/2025 04:06:01 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 15200 loss: -0.256519
03/24/2025 04:08:50 - INFO - __main__ -   step 15300 loss 0.30315
03/24/2025 04:08:52 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 15300 loss: -0.321609
03/24/2025 04:11:42 - INFO - __main__ -   step 15400 loss 0.27012
03/24/2025 04:11:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 15400 loss: -0.270169
03/24/2025 04:14:33 - INFO - __main__ -   step 15500 loss 0.31718
03/24/2025 04:14:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 15500 loss: -0.318245
03/24/2025 04:17:25 - INFO - __main__ -   step 15600 loss 0.35653
03/24/2025 04:17:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 15600 loss: -0.369457
03/24/2025 04:20:16 - INFO - __main__ -   step 15700 loss 0.24814
03/24/2025 04:20:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 15700 loss: -0.262302
03/24/2025 04:23:08 - INFO - __main__ -   step 15800 loss 0.28004
03/24/2025 04:23:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 15800 loss: -0.263698
03/24/2025 04:25:59 - INFO - __main__ -   step 15900 loss 0.29383
03/24/2025 04:26:01 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 15900 loss: -0.344395
03/24/2025 04:28:50 - INFO - __main__ -   step 16000 loss 0.29663
03/24/2025 04:28:52 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 16000 loss: -0.313649
03/24/2025 04:28:57 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 04:28:57 - INFO - __main__ -     Num queries = 1400
03/24/2025 04:28:57 - INFO - __main__ -     Num codes = 4360
03/24/2025 04:28:57 - INFO - __main__ -     Batch size = 32
03/24/2025 04:29:25 - INFO - __main__ -     eval_mrr = 0.789
03/24/2025 04:29:26 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-16000-0.789
03/24/2025 04:29:28 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 04:32:18 - INFO - __main__ -   step 16100 loss 0.26974
03/24/2025 04:32:19 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 16100 loss: -0.272942
03/24/2025 04:35:09 - INFO - __main__ -   step 16200 loss 0.25408
03/24/2025 04:35:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 16200 loss: -0.263833
03/24/2025 04:38:00 - INFO - __main__ -   step 16300 loss 0.27763
03/24/2025 04:38:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 16300 loss: -0.295995
03/24/2025 04:40:51 - INFO - __main__ -   step 16400 loss 0.30723
03/24/2025 04:40:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 16400 loss: -0.325094
03/24/2025 04:43:43 - INFO - __main__ -   step 16500 loss 0.26294
03/24/2025 04:43:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 16500 loss: -0.254959
03/24/2025 04:46:34 - INFO - __main__ -   step 16600 loss 0.29892
03/24/2025 04:46:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 16600 loss: -0.365467
03/24/2025 04:49:25 - INFO - __main__ -   step 16700 loss 0.28364
03/24/2025 04:49:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 16700 loss: -0.303498
03/24/2025 04:52:16 - INFO - __main__ -   step 16800 loss 0.31505
03/24/2025 04:52:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 16800 loss: -0.308691
03/24/2025 04:55:07 - INFO - __main__ -   step 16900 loss 0.31294
03/24/2025 04:55:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 16900 loss: -0.316309
03/24/2025 04:57:58 - INFO - __main__ -   step 17000 loss 0.30942
03/24/2025 04:58:00 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 17000 loss: -0.294809
03/24/2025 04:58:05 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 04:58:05 - INFO - __main__ -     Num queries = 1400
03/24/2025 04:58:05 - INFO - __main__ -     Num codes = 4360
03/24/2025 04:58:05 - INFO - __main__ -     Batch size = 32
03/24/2025 04:58:33 - INFO - __main__ -     eval_mrr = 0.785
03/24/2025 04:58:34 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-17000-0.785
03/24/2025 04:58:36 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 05:01:26 - INFO - __main__ -   step 17100 loss 0.29209
03/24/2025 05:01:28 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 17100 loss: -0.305761
03/24/2025 05:04:17 - INFO - __main__ -   step 17200 loss 0.2559
03/24/2025 05:04:19 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 17200 loss: -0.255577
03/24/2025 05:07:09 - INFO - __main__ -   step 17300 loss 0.32327
03/24/2025 05:07:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 17300 loss: -0.303536
03/24/2025 05:10:00 - INFO - __main__ -   step 17400 loss 0.31074
03/24/2025 05:10:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 17400 loss: -0.316241
03/24/2025 05:12:52 - INFO - __main__ -   step 17500 loss 0.27685
03/24/2025 05:12:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 17500 loss: -0.28771
03/24/2025 05:15:43 - INFO - __main__ -   step 17600 loss 0.30108
03/24/2025 05:15:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 17600 loss: -0.29707
03/24/2025 05:18:34 - INFO - __main__ -   step 17700 loss 0.26494
03/24/2025 05:18:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 17700 loss: -0.271009
03/24/2025 05:21:26 - INFO - __main__ -   step 17800 loss 0.30495
03/24/2025 05:21:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 17800 loss: -0.325992
03/24/2025 05:24:17 - INFO - __main__ -   step 17900 loss 0.22865
03/24/2025 05:24:19 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 17900 loss: -0.195577
03/24/2025 05:27:08 - INFO - __main__ -   step 18000 loss 0.25057
03/24/2025 05:27:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 18000 loss: -0.27056
03/24/2025 05:27:15 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 05:27:15 - INFO - __main__ -     Num queries = 1400
03/24/2025 05:27:15 - INFO - __main__ -     Num codes = 4360
03/24/2025 05:27:15 - INFO - __main__ -     Batch size = 32
03/24/2025 05:27:43 - INFO - __main__ -     eval_mrr = 0.785
03/24/2025 05:27:44 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-18000-0.785
03/24/2025 05:27:46 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 05:30:35 - INFO - __main__ -   step 18100 loss 0.26914
03/24/2025 05:30:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 18100 loss: -0.256485
03/24/2025 05:33:27 - INFO - __main__ -   step 18200 loss 0.26321
03/24/2025 05:33:28 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 18200 loss: -0.275678
03/24/2025 05:36:18 - INFO - __main__ -   step 18300 loss 0.25593
03/24/2025 05:36:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 18300 loss: -0.270345
03/24/2025 05:39:09 - INFO - __main__ -   step 18400 loss 0.28504
03/24/2025 05:39:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 18400 loss: -0.285345
03/24/2025 05:42:00 - INFO - __main__ -   step 18500 loss 0.25625
03/24/2025 05:42:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 18500 loss: -0.276642
03/24/2025 05:44:52 - INFO - __main__ -   step 18600 loss 0.2626
03/24/2025 05:44:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 18600 loss: -0.289811
03/24/2025 05:47:43 - INFO - __main__ -   step 18700 loss 0.32942
03/24/2025 05:47:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 18700 loss: -0.367485
03/24/2025 05:50:34 - INFO - __main__ -   step 18800 loss 0.23781
03/24/2025 05:50:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 18800 loss: -0.268667
03/24/2025 05:53:25 - INFO - __main__ -   step 18900 loss 0.25629
03/24/2025 05:53:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 18900 loss: -0.258596
03/24/2025 05:56:17 - INFO - __main__ -   step 19000 loss 0.23641
03/24/2025 05:56:19 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 19000 loss: -0.226906
03/24/2025 05:56:24 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 05:56:24 - INFO - __main__ -     Num queries = 1400
03/24/2025 05:56:24 - INFO - __main__ -     Num codes = 4360
03/24/2025 05:56:24 - INFO - __main__ -     Batch size = 32
03/24/2025 05:56:52 - INFO - __main__ -     eval_mrr = 0.782
03/24/2025 05:56:53 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-19000-0.782
03/24/2025 05:56:55 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 05:59:45 - INFO - __main__ -   step 19100 loss 0.28876
03/24/2025 05:59:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 19100 loss: -0.303989
03/24/2025 06:02:36 - INFO - __main__ -   step 19200 loss 0.24398
03/24/2025 06:02:38 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 19200 loss: -0.240539
03/24/2025 06:05:27 - INFO - __main__ -   step 19300 loss 0.29042
03/24/2025 06:05:29 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 19300 loss: -0.313269
03/24/2025 06:08:18 - INFO - __main__ -   step 19400 loss 0.27191
03/24/2025 06:08:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 19400 loss: -0.279931
03/24/2025 06:11:10 - INFO - __main__ -   step 19500 loss 0.28965
03/24/2025 06:11:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 19500 loss: -0.301695
03/24/2025 06:14:01 - INFO - __main__ -   step 19600 loss 0.25947
03/24/2025 06:14:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 19600 loss: -0.243605
03/24/2025 06:16:52 - INFO - __main__ -   step 19700 loss 0.28413
03/24/2025 06:16:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 19700 loss: -0.290715
03/24/2025 06:19:44 - INFO - __main__ -   step 19800 loss 0.27591
03/24/2025 06:19:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 19800 loss: -0.25073
03/24/2025 06:22:35 - INFO - __main__ -   step 19900 loss 0.28773
03/24/2025 06:22:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 19900 loss: -0.302893
03/24/2025 06:25:26 - INFO - __main__ -   step 20000 loss 0.30781
03/24/2025 06:25:28 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 20000 loss: -0.361289
03/24/2025 06:25:33 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 06:25:33 - INFO - __main__ -     Num queries = 1400
03/24/2025 06:25:33 - INFO - __main__ -     Num codes = 4360
03/24/2025 06:25:33 - INFO - __main__ -     Batch size = 32
03/24/2025 06:26:01 - INFO - __main__ -     eval_mrr = 0.787
03/24/2025 06:26:02 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-20000-0.787
03/24/2025 06:26:04 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 06:28:53 - INFO - __main__ -   step 20100 loss 0.24314
03/24/2025 06:28:55 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 20100 loss: -0.259331
03/24/2025 06:31:45 - INFO - __main__ -   step 20200 loss 0.29655
03/24/2025 06:31:46 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 20200 loss: -0.301098
03/24/2025 06:34:36 - INFO - __main__ -   step 20300 loss 0.26242
03/24/2025 06:34:38 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 20300 loss: -0.260401
03/24/2025 06:37:27 - INFO - __main__ -   step 20400 loss 0.26655
03/24/2025 06:37:29 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 20400 loss: -0.266616
03/24/2025 06:40:18 - INFO - __main__ -   step 20500 loss 0.24939
03/24/2025 06:40:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 20500 loss: -0.256157
03/24/2025 06:43:09 - INFO - __main__ -   step 20600 loss 0.29803
03/24/2025 06:43:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 20600 loss: -0.302659
03/24/2025 06:46:00 - INFO - __main__ -   step 20700 loss 0.23992
03/24/2025 06:46:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 20700 loss: -0.282426
03/24/2025 06:48:51 - INFO - __main__ -   step 20800 loss 0.25787
03/24/2025 06:48:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 20800 loss: -0.273989
03/24/2025 06:51:42 - INFO - __main__ -   step 20900 loss 0.27886
03/24/2025 06:51:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 20900 loss: -0.284605
03/24/2025 06:54:34 - INFO - __main__ -   step 21000 loss 0.25519
03/24/2025 06:54:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 21000 loss: -0.234911
03/24/2025 06:54:40 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 06:54:40 - INFO - __main__ -     Num queries = 1400
03/24/2025 06:54:40 - INFO - __main__ -     Num codes = 4360
03/24/2025 06:54:40 - INFO - __main__ -     Batch size = 32
03/24/2025 06:55:08 - INFO - __main__ -     eval_mrr = 0.791
03/24/2025 06:55:08 - INFO - __main__ -     ********************
03/24/2025 06:55:08 - INFO - __main__ -     Best mrr:0.791
03/24/2025 06:55:08 - INFO - __main__ -     ********************
03/24/2025 06:55:11 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-best-mrr/model.bin
03/24/2025 06:55:12 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-21000-0.791
03/24/2025 06:55:14 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 06:58:04 - INFO - __main__ -   step 21100 loss 0.28149
03/24/2025 06:58:05 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 21100 loss: -0.272556
03/24/2025 07:00:55 - INFO - __main__ -   step 21200 loss 0.31767
03/24/2025 07:00:57 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 21200 loss: -0.333773
03/24/2025 07:03:46 - INFO - __main__ -   step 21300 loss 0.28564
03/24/2025 07:03:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 21300 loss: -0.279356
03/24/2025 07:06:37 - INFO - __main__ -   step 21400 loss 0.24064
03/24/2025 07:06:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 21400 loss: -0.237565
03/24/2025 07:09:29 - INFO - __main__ -   step 21500 loss 0.23756
03/24/2025 07:09:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 21500 loss: -0.278366
03/24/2025 07:12:20 - INFO - __main__ -   step 21600 loss 0.21908
03/24/2025 07:12:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 21600 loss: -0.19546
03/24/2025 07:15:11 - INFO - __main__ -   step 21700 loss 0.26041
03/24/2025 07:15:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 21700 loss: -0.229682
03/24/2025 07:18:02 - INFO - __main__ -   step 21800 loss 0.24039
03/24/2025 07:18:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 21800 loss: -0.225656
03/24/2025 07:20:54 - INFO - __main__ -   step 21900 loss 0.2901
03/24/2025 07:20:55 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 21900 loss: -0.316906
03/24/2025 07:23:45 - INFO - __main__ -   step 22000 loss 0.24621
03/24/2025 07:23:46 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 22000 loss: -0.217178
03/24/2025 07:23:51 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 07:23:51 - INFO - __main__ -     Num queries = 1400
03/24/2025 07:23:51 - INFO - __main__ -     Num codes = 4360
03/24/2025 07:23:51 - INFO - __main__ -     Batch size = 32
03/24/2025 07:24:20 - INFO - __main__ -     eval_mrr = 0.785
03/24/2025 07:24:21 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-22000-0.785
03/24/2025 07:24:23 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 07:27:12 - INFO - __main__ -   step 22100 loss 0.22702
03/24/2025 07:27:14 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 22100 loss: -0.242569
03/24/2025 07:30:04 - INFO - __main__ -   step 22200 loss 0.27641
03/24/2025 07:30:05 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 22200 loss: -0.266043
03/24/2025 07:32:55 - INFO - __main__ -   step 22300 loss 0.26987
03/24/2025 07:32:57 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 22300 loss: -0.265531
03/24/2025 07:35:46 - INFO - __main__ -   step 22400 loss 0.25419
03/24/2025 07:35:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 22400 loss: -0.26453
03/24/2025 07:38:37 - INFO - __main__ -   step 22500 loss 0.30231
03/24/2025 07:38:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 22500 loss: -0.300951
03/24/2025 07:41:28 - INFO - __main__ -   step 22600 loss 0.25066
03/24/2025 07:41:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 22600 loss: -0.268964
03/24/2025 07:44:19 - INFO - __main__ -   step 22700 loss 0.26216
03/24/2025 07:44:21 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 22700 loss: -0.244978
03/24/2025 07:47:10 - INFO - __main__ -   step 22800 loss 0.23506
03/24/2025 07:47:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 22800 loss: -0.254113
03/24/2025 07:50:02 - INFO - __main__ -   step 22900 loss 0.22818
03/24/2025 07:50:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 22900 loss: -0.241514
03/24/2025 07:52:53 - INFO - __main__ -   step 23000 loss 0.25271
03/24/2025 07:52:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 23000 loss: -0.253344
03/24/2025 07:52:59 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 07:52:59 - INFO - __main__ -     Num queries = 1400
03/24/2025 07:52:59 - INFO - __main__ -     Num codes = 4360
03/24/2025 07:52:59 - INFO - __main__ -     Batch size = 32
03/24/2025 07:53:27 - INFO - __main__ -     eval_mrr = 0.788
03/24/2025 07:53:28 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-23000-0.788
03/24/2025 07:53:30 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 07:56:20 - INFO - __main__ -   step 23100 loss 0.26532
03/24/2025 07:56:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 23100 loss: -0.244497
03/24/2025 07:59:11 - INFO - __main__ -   step 23200 loss 0.28108
03/24/2025 07:59:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 23200 loss: -0.297479
03/24/2025 08:02:02 - INFO - __main__ -   step 23300 loss 0.24082
03/24/2025 08:02:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 23300 loss: -0.263047
03/24/2025 08:04:54 - INFO - __main__ -   step 23400 loss 0.29547
03/24/2025 08:04:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 23400 loss: -0.291332
03/24/2025 08:07:45 - INFO - __main__ -   step 23500 loss 0.26153
03/24/2025 08:07:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 23500 loss: -0.261033
03/24/2025 08:10:37 - INFO - __main__ -   step 23600 loss 0.25188
03/24/2025 08:10:38 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 23600 loss: -0.22367
03/24/2025 08:13:28 - INFO - __main__ -   step 23700 loss 0.28915
03/24/2025 08:13:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 23700 loss: -0.28439
03/24/2025 08:16:19 - INFO - __main__ -   step 23800 loss 0.24496
03/24/2025 08:16:21 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 23800 loss: -0.25396
03/24/2025 08:19:10 - INFO - __main__ -   step 23900 loss 0.2812
03/24/2025 08:19:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 23900 loss: -0.259786
03/24/2025 08:22:02 - INFO - __main__ -   step 24000 loss 0.28208
03/24/2025 08:22:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 24000 loss: -0.294216
03/24/2025 08:22:08 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 08:22:08 - INFO - __main__ -     Num queries = 1400
03/24/2025 08:22:08 - INFO - __main__ -     Num codes = 4360
03/24/2025 08:22:08 - INFO - __main__ -     Batch size = 32
03/24/2025 08:22:36 - INFO - __main__ -     eval_mrr = 0.792
03/24/2025 08:22:36 - INFO - __main__ -     ********************
03/24/2025 08:22:36 - INFO - __main__ -     Best mrr:0.792
03/24/2025 08:22:36 - INFO - __main__ -     ********************
03/24/2025 08:22:39 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-best-mrr/model.bin
03/24/2025 08:22:40 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-24000-0.792
03/24/2025 08:22:42 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 08:25:32 - INFO - __main__ -   step 24100 loss 0.28575
03/24/2025 08:25:33 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 24100 loss: -0.292009
03/24/2025 08:28:23 - INFO - __main__ -   step 24200 loss 0.24637
03/24/2025 08:28:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 24200 loss: -0.245366
03/24/2025 08:31:14 - INFO - __main__ -   step 24300 loss 0.26108
03/24/2025 08:31:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 24300 loss: -0.264365
03/24/2025 08:34:06 - INFO - __main__ -   step 24400 loss 0.25697
03/24/2025 08:34:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 24400 loss: -0.247276
03/24/2025 08:36:57 - INFO - __main__ -   step 24500 loss 0.2596
03/24/2025 08:36:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 24500 loss: -0.287844
03/24/2025 08:39:48 - INFO - __main__ -   step 24600 loss 0.24817
03/24/2025 08:39:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 24600 loss: -0.270736
03/24/2025 08:42:39 - INFO - __main__ -   step 24700 loss 0.25892
03/24/2025 08:42:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 24700 loss: -0.252623
03/24/2025 08:45:31 - INFO - __main__ -   step 24800 loss 0.27704
03/24/2025 08:45:33 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 24800 loss: -0.294377
03/24/2025 08:48:22 - INFO - __main__ -   step 24900 loss 0.25211
03/24/2025 08:48:24 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 24900 loss: -0.260779
03/24/2025 08:51:13 - INFO - __main__ -   step 25000 loss 0.26791
03/24/2025 08:51:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 25000 loss: -0.224482
03/24/2025 08:51:20 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 08:51:20 - INFO - __main__ -     Num queries = 1400
03/24/2025 08:51:20 - INFO - __main__ -     Num codes = 4360
03/24/2025 08:51:20 - INFO - __main__ -     Batch size = 32
03/24/2025 08:51:48 - INFO - __main__ -     eval_mrr = 0.793
03/24/2025 08:51:48 - INFO - __main__ -     ********************
03/24/2025 08:51:48 - INFO - __main__ -     Best mrr:0.793
03/24/2025 08:51:48 - INFO - __main__ -     ********************
03/24/2025 08:51:51 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-best-mrr/model.bin
03/24/2025 08:51:51 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-25000-0.793
03/24/2025 08:51:54 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 08:54:43 - INFO - __main__ -   step 25100 loss 0.23471
03/24/2025 08:54:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 25100 loss: -0.266326
03/24/2025 08:57:35 - INFO - __main__ -   step 25200 loss 0.27683
03/24/2025 08:57:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 25200 loss: -0.2703
03/24/2025 09:00:26 - INFO - __main__ -   step 25300 loss 0.25459
03/24/2025 09:00:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 25300 loss: -0.263865
03/24/2025 09:03:17 - INFO - __main__ -   step 25400 loss 0.25263
03/24/2025 09:03:19 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 25400 loss: -0.233568
03/24/2025 09:06:08 - INFO - __main__ -   step 25500 loss 0.28138
03/24/2025 09:06:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 25500 loss: -0.286961
03/24/2025 09:08:59 - INFO - __main__ -   step 25600 loss 0.24923
03/24/2025 09:09:01 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 25600 loss: -0.230592
03/24/2025 09:11:50 - INFO - __main__ -   step 25700 loss 0.26132
03/24/2025 09:11:52 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 25700 loss: -0.262727
03/24/2025 09:14:41 - INFO - __main__ -   step 25800 loss 0.23704
03/24/2025 09:14:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 25800 loss: -0.23389
03/24/2025 09:17:32 - INFO - __main__ -   step 25900 loss 0.2972
03/24/2025 09:17:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 25900 loss: -0.307093
03/24/2025 09:20:24 - INFO - __main__ -   step 26000 loss 0.28202
03/24/2025 09:20:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 26000 loss: -0.308183
03/24/2025 09:20:30 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 09:20:30 - INFO - __main__ -     Num queries = 1400
03/24/2025 09:20:30 - INFO - __main__ -     Num codes = 4360
03/24/2025 09:20:30 - INFO - __main__ -     Batch size = 32
03/24/2025 09:20:58 - INFO - __main__ -     eval_mrr = 0.794
03/24/2025 09:20:58 - INFO - __main__ -     ********************
03/24/2025 09:20:58 - INFO - __main__ -     Best mrr:0.794
03/24/2025 09:20:58 - INFO - __main__ -     ********************
03/24/2025 09:21:01 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-best-mrr/model.bin
03/24/2025 09:21:02 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-26000-0.794
03/24/2025 09:21:05 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 09:23:54 - INFO - __main__ -   step 26100 loss 0.29377
03/24/2025 09:23:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 26100 loss: -0.275467
03/24/2025 09:26:45 - INFO - __main__ -   step 26200 loss 0.23159
03/24/2025 09:26:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 26200 loss: -0.234233
03/24/2025 09:29:36 - INFO - __main__ -   step 26300 loss 0.23398
03/24/2025 09:29:38 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 26300 loss: -0.254965
03/24/2025 09:32:28 - INFO - __main__ -   step 26400 loss 0.19835
03/24/2025 09:32:29 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 26400 loss: -0.201528
03/24/2025 09:35:19 - INFO - __main__ -   step 26500 loss 0.24347
03/24/2025 09:35:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 26500 loss: -0.255122
03/24/2025 09:38:10 - INFO - __main__ -   step 26600 loss 0.26448
03/24/2025 09:38:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 26600 loss: -0.268749
03/24/2025 09:41:01 - INFO - __main__ -   step 26700 loss 0.26554
03/24/2025 09:41:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 26700 loss: -0.283179
03/24/2025 09:43:51 - INFO - __main__ -   step 26800 loss 0.26961
03/24/2025 09:43:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 26800 loss: -0.293717
03/24/2025 09:46:42 - INFO - __main__ -   step 26900 loss 0.22846
03/24/2025 09:46:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 26900 loss: -0.203032
03/24/2025 09:49:33 - INFO - __main__ -   step 27000 loss 0.23857
03/24/2025 09:49:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 27000 loss: -0.236034
03/24/2025 09:49:40 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 09:49:40 - INFO - __main__ -     Num queries = 1400
03/24/2025 09:49:40 - INFO - __main__ -     Num codes = 4360
03/24/2025 09:49:40 - INFO - __main__ -     Batch size = 32
03/24/2025 09:50:08 - INFO - __main__ -     eval_mrr = 0.789
03/24/2025 09:50:09 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-27000-0.789
03/24/2025 09:50:11 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 09:53:00 - INFO - __main__ -   step 27100 loss 0.21554
03/24/2025 09:53:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 27100 loss: -0.205007
03/24/2025 09:55:51 - INFO - __main__ -   step 27200 loss 0.22098
03/24/2025 09:55:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 27200 loss: -0.207288
03/24/2025 09:58:42 - INFO - __main__ -   step 27300 loss 0.29708
03/24/2025 09:58:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 27300 loss: -0.258137
03/24/2025 10:01:33 - INFO - __main__ -   step 27400 loss 0.27106
03/24/2025 10:01:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 27400 loss: -0.285036
03/24/2025 10:04:24 - INFO - __main__ -   step 27500 loss 0.23467
03/24/2025 10:04:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 27500 loss: -0.218887
03/24/2025 10:07:15 - INFO - __main__ -   step 27600 loss 0.25724
03/24/2025 10:07:17 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 27600 loss: -0.247269
03/24/2025 10:10:06 - INFO - __main__ -   step 27700 loss 0.24704
03/24/2025 10:10:08 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 27700 loss: -0.241135
03/24/2025 10:12:57 - INFO - __main__ -   step 27800 loss 0.26252
03/24/2025 10:12:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 27800 loss: -0.291182
03/24/2025 10:15:48 - INFO - __main__ -   step 27900 loss 0.24117
03/24/2025 10:15:49 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 27900 loss: -0.252156
03/24/2025 10:18:39 - INFO - __main__ -   step 28000 loss 0.25926
03/24/2025 10:18:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 28000 loss: -0.255923
03/24/2025 10:18:45 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 10:18:45 - INFO - __main__ -     Num queries = 1400
03/24/2025 10:18:45 - INFO - __main__ -     Num codes = 4360
03/24/2025 10:18:45 - INFO - __main__ -     Batch size = 32
03/24/2025 10:19:14 - INFO - __main__ -     eval_mrr = 0.785
03/24/2025 10:19:14 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-28000-0.785
03/24/2025 10:19:17 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 10:22:06 - INFO - __main__ -   step 28100 loss 0.21607
03/24/2025 10:22:08 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 28100 loss: -0.173266
03/24/2025 10:24:57 - INFO - __main__ -   step 28200 loss 0.25765
03/24/2025 10:24:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 28200 loss: -0.249072
03/24/2025 10:27:48 - INFO - __main__ -   step 28300 loss 0.2626
03/24/2025 10:27:49 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 28300 loss: -0.26006
03/24/2025 10:30:39 - INFO - __main__ -   step 28400 loss 0.25313
03/24/2025 10:30:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 28400 loss: -0.241626
03/24/2025 10:33:29 - INFO - __main__ -   step 28500 loss 0.2686
03/24/2025 10:33:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 28500 loss: -0.255086
03/24/2025 10:36:20 - INFO - __main__ -   step 28600 loss 0.26094
03/24/2025 10:36:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 28600 loss: -0.297106
03/24/2025 10:39:11 - INFO - __main__ -   step 28700 loss 0.21525
03/24/2025 10:39:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 28700 loss: -0.206136
03/24/2025 10:42:02 - INFO - __main__ -   step 28800 loss 0.24496
03/24/2025 10:42:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 28800 loss: -0.229649
03/24/2025 10:44:53 - INFO - __main__ -   step 28900 loss 0.26724
03/24/2025 10:44:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 28900 loss: -0.251661
03/24/2025 10:47:43 - INFO - __main__ -   step 29000 loss 0.23289
03/24/2025 10:47:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 29000 loss: -0.240171
03/24/2025 10:47:50 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 10:47:50 - INFO - __main__ -     Num queries = 1400
03/24/2025 10:47:50 - INFO - __main__ -     Num codes = 4360
03/24/2025 10:47:50 - INFO - __main__ -     Batch size = 32
03/24/2025 10:48:18 - INFO - __main__ -     eval_mrr = 0.788
03/24/2025 10:48:19 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-29000-0.788
03/24/2025 10:48:22 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 10:51:11 - INFO - __main__ -   step 29100 loss 0.24067
03/24/2025 10:51:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 29100 loss: -0.241392
03/24/2025 10:54:01 - INFO - __main__ -   step 29200 loss 0.21754
03/24/2025 10:54:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 29200 loss: -0.227933
03/24/2025 10:56:52 - INFO - __main__ -   step 29300 loss 0.29946
03/24/2025 10:56:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 29300 loss: -0.29174
03/24/2025 10:59:43 - INFO - __main__ -   step 29400 loss 0.27488
03/24/2025 10:59:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 29400 loss: -0.277079
03/24/2025 11:02:34 - INFO - __main__ -   step 29500 loss 0.24466
03/24/2025 11:02:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 29500 loss: -0.235849
03/24/2025 11:05:25 - INFO - __main__ -   step 29600 loss 0.26598
03/24/2025 11:05:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 29600 loss: -0.279367
03/24/2025 11:08:16 - INFO - __main__ -   step 29700 loss 0.25974
03/24/2025 11:08:17 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 29700 loss: -0.27759
03/24/2025 11:11:07 - INFO - __main__ -   step 29800 loss 0.27266
03/24/2025 11:11:08 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 29800 loss: -0.271594
03/24/2025 11:13:57 - INFO - __main__ -   step 29900 loss 0.25189
03/24/2025 11:13:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 29900 loss: -0.250745
03/24/2025 11:16:48 - INFO - __main__ -   step 30000 loss 0.2607
03/24/2025 11:16:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 30000 loss: -0.268605
03/24/2025 11:16:55 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 11:16:55 - INFO - __main__ -     Num queries = 1400
03/24/2025 11:16:55 - INFO - __main__ -     Num codes = 4360
03/24/2025 11:16:55 - INFO - __main__ -     Batch size = 32
03/24/2025 11:17:23 - INFO - __main__ -     eval_mrr = 0.789
03/24/2025 11:17:24 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-30000-0.789
03/24/2025 11:17:26 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 11:20:15 - INFO - __main__ -   step 30100 loss 0.26457
03/24/2025 11:20:17 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 30100 loss: -0.260473
03/24/2025 11:23:06 - INFO - __main__ -   step 30200 loss 0.28623
03/24/2025 11:23:08 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 30200 loss: -0.295436
03/24/2025 11:25:57 - INFO - __main__ -   step 30300 loss 0.27242
03/24/2025 11:25:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 30300 loss: -0.238816
03/24/2025 11:28:48 - INFO - __main__ -   step 30400 loss 0.2664
03/24/2025 11:28:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 30400 loss: -0.277333
03/24/2025 11:31:39 - INFO - __main__ -   step 30500 loss 0.22083
03/24/2025 11:31:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 30500 loss: -0.216334
03/24/2025 11:34:30 - INFO - __main__ -   step 30600 loss 0.21214
03/24/2025 11:34:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 30600 loss: -0.229017
03/24/2025 11:37:20 - INFO - __main__ -   step 30700 loss 0.24242
03/24/2025 11:37:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 30700 loss: -0.251152
03/24/2025 11:40:11 - INFO - __main__ -   step 30800 loss 0.25573
03/24/2025 11:40:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 30800 loss: -0.217075
03/24/2025 11:43:02 - INFO - __main__ -   step 30900 loss 0.25538
03/24/2025 11:43:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 30900 loss: -0.250503
03/24/2025 11:45:53 - INFO - __main__ -   step 31000 loss 0.27324
03/24/2025 11:45:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 31000 loss: -0.262578
03/24/2025 11:45:59 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 11:45:59 - INFO - __main__ -     Num queries = 1400
03/24/2025 11:45:59 - INFO - __main__ -     Num codes = 4360
03/24/2025 11:45:59 - INFO - __main__ -     Batch size = 32
03/24/2025 11:46:27 - INFO - __main__ -     eval_mrr = 0.785
03/24/2025 11:46:28 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-31000-0.785
03/24/2025 11:46:30 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 11:49:19 - INFO - __main__ -   step 31100 loss 0.25752
03/24/2025 11:49:21 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 31100 loss: -0.259086
03/24/2025 11:52:10 - INFO - __main__ -   step 31200 loss 0.23292
03/24/2025 11:52:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 31200 loss: -0.223711
03/24/2025 11:55:01 - INFO - __main__ -   step 31300 loss 0.21703
03/24/2025 11:55:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 31300 loss: -0.237961
03/24/2025 11:57:52 - INFO - __main__ -   step 31400 loss 0.20581
03/24/2025 11:57:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 31400 loss: -0.183652
03/24/2025 12:00:43 - INFO - __main__ -   step 31500 loss 0.22216
03/24/2025 12:00:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 31500 loss: -0.185434
03/24/2025 12:03:34 - INFO - __main__ -   step 31600 loss 0.21991
03/24/2025 12:03:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 31600 loss: -0.207056
03/24/2025 12:06:25 - INFO - __main__ -   step 31700 loss 0.21207
03/24/2025 12:06:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 31700 loss: -0.192822
03/24/2025 12:09:16 - INFO - __main__ -   step 31800 loss 0.21521
03/24/2025 12:09:17 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 31800 loss: -0.238061
03/24/2025 12:12:06 - INFO - __main__ -   step 31900 loss 0.18349
03/24/2025 12:12:08 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 31900 loss: -0.200708
03/24/2025 12:14:57 - INFO - __main__ -   step 32000 loss 0.20574
03/24/2025 12:14:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 32000 loss: -0.172708
03/24/2025 12:15:04 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 12:15:04 - INFO - __main__ -     Num queries = 1400
03/24/2025 12:15:04 - INFO - __main__ -     Num codes = 4360
03/24/2025 12:15:04 - INFO - __main__ -     Batch size = 32
03/24/2025 12:15:32 - INFO - __main__ -     eval_mrr = 0.791
03/24/2025 12:15:49 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-32000-0.791
03/24/2025 12:15:52 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 12:18:41 - INFO - __main__ -   step 32100 loss 0.21583
03/24/2025 12:18:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 32100 loss: -0.230328
03/24/2025 12:21:32 - INFO - __main__ -   step 32200 loss 0.20897
03/24/2025 12:21:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 32200 loss: -0.19661
03/24/2025 12:24:23 - INFO - __main__ -   step 32300 loss 0.22725
03/24/2025 12:24:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 32300 loss: -0.207537
03/24/2025 12:27:14 - INFO - __main__ -   step 32400 loss 0.20624
03/24/2025 12:27:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 32400 loss: -0.176065
03/24/2025 12:30:05 - INFO - __main__ -   step 32500 loss 0.22292
03/24/2025 12:30:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 32500 loss: -0.232018
03/24/2025 12:32:56 - INFO - __main__ -   step 32600 loss 0.21108
03/24/2025 12:32:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 32600 loss: -0.206994
03/24/2025 12:35:47 - INFO - __main__ -   step 32700 loss 0.20972
03/24/2025 12:35:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 32700 loss: -0.225087
03/24/2025 12:38:37 - INFO - __main__ -   step 32800 loss 0.2185
03/24/2025 12:38:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 32800 loss: -0.210737
03/24/2025 12:41:28 - INFO - __main__ -   step 32900 loss 0.22031
03/24/2025 12:41:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 32900 loss: -0.264861
03/24/2025 12:44:19 - INFO - __main__ -   step 33000 loss 0.17753
03/24/2025 12:44:21 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 33000 loss: -0.186273
03/24/2025 12:44:26 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 12:44:26 - INFO - __main__ -     Num queries = 1400
03/24/2025 12:44:26 - INFO - __main__ -     Num codes = 4360
03/24/2025 12:44:26 - INFO - __main__ -     Batch size = 32
03/24/2025 12:44:54 - INFO - __main__ -     eval_mrr = 0.784
03/24/2025 12:44:55 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-33000-0.784
03/24/2025 12:44:58 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 12:47:47 - INFO - __main__ -   step 33100 loss 0.19505
03/24/2025 12:47:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 33100 loss: -0.208702
03/24/2025 12:50:38 - INFO - __main__ -   step 33200 loss 0.20925
03/24/2025 12:50:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 33200 loss: -0.222819
03/24/2025 12:53:28 - INFO - __main__ -   step 33300 loss 0.2132
03/24/2025 12:53:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 33300 loss: -0.218164
03/24/2025 12:56:19 - INFO - __main__ -   step 33400 loss 0.20682
03/24/2025 12:56:21 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 33400 loss: -0.207618
03/24/2025 12:59:10 - INFO - __main__ -   step 33500 loss 0.18428
03/24/2025 12:59:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 33500 loss: -0.181974
03/24/2025 13:02:01 - INFO - __main__ -   step 33600 loss 0.24164
03/24/2025 13:02:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 33600 loss: -0.250198
03/24/2025 13:04:52 - INFO - __main__ -   step 33700 loss 0.19278
03/24/2025 13:04:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 33700 loss: -0.16747
03/24/2025 13:07:43 - INFO - __main__ -   step 33800 loss 0.19461
03/24/2025 13:07:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 33800 loss: -0.225322
03/24/2025 13:10:34 - INFO - __main__ -   step 33900 loss 0.21891
03/24/2025 13:10:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 33900 loss: -0.21815
03/24/2025 13:13:24 - INFO - __main__ -   step 34000 loss 0.21441
03/24/2025 13:13:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 34000 loss: -0.187452
03/24/2025 13:13:31 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 13:13:31 - INFO - __main__ -     Num queries = 1400
03/24/2025 13:13:31 - INFO - __main__ -     Num codes = 4360
03/24/2025 13:13:31 - INFO - __main__ -     Batch size = 32
03/24/2025 13:13:59 - INFO - __main__ -     eval_mrr = 0.787
03/24/2025 13:14:00 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-34000-0.787
03/24/2025 13:14:02 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 13:16:52 - INFO - __main__ -   step 34100 loss 0.19876
03/24/2025 13:16:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 34100 loss: -0.183191
03/24/2025 13:19:42 - INFO - __main__ -   step 34200 loss 0.19112
03/24/2025 13:19:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 34200 loss: -0.202865
03/24/2025 13:22:33 - INFO - __main__ -   step 34300 loss 0.20863
03/24/2025 13:22:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 34300 loss: -0.201794
03/24/2025 13:25:24 - INFO - __main__ -   step 34400 loss 0.22066
03/24/2025 13:25:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 34400 loss: -0.218101
03/24/2025 13:28:16 - INFO - __main__ -   step 34500 loss 0.21135
03/24/2025 13:28:17 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 34500 loss: -0.226796
03/24/2025 13:31:06 - INFO - __main__ -   step 34600 loss 0.19276
03/24/2025 13:31:08 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 34600 loss: -0.195018
03/24/2025 13:33:57 - INFO - __main__ -   step 34700 loss 0.17958
03/24/2025 13:33:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 34700 loss: -0.185084
03/24/2025 13:36:48 - INFO - __main__ -   step 34800 loss 0.22222
03/24/2025 13:36:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 34800 loss: -0.19126
03/24/2025 13:39:39 - INFO - __main__ -   step 34900 loss 0.20264
03/24/2025 13:39:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 34900 loss: -0.19119
03/24/2025 13:42:30 - INFO - __main__ -   step 35000 loss 0.19636
03/24/2025 13:42:32 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 35000 loss: -0.194069
03/24/2025 13:42:37 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 13:42:37 - INFO - __main__ -     Num queries = 1400
03/24/2025 13:42:37 - INFO - __main__ -     Num codes = 4360
03/24/2025 13:42:37 - INFO - __main__ -     Batch size = 32
03/24/2025 13:43:05 - INFO - __main__ -     eval_mrr = 0.783
03/24/2025 13:43:06 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-35000-0.783
03/24/2025 13:43:08 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 13:45:57 - INFO - __main__ -   step 35100 loss 0.22114
03/24/2025 13:45:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 35100 loss: -0.226906
03/24/2025 13:48:48 - INFO - __main__ -   step 35200 loss 0.20218
03/24/2025 13:48:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 35200 loss: -0.18715
03/24/2025 13:51:39 - INFO - __main__ -   step 35300 loss 0.2215
03/24/2025 13:51:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 35300 loss: -0.217613
03/24/2025 13:54:30 - INFO - __main__ -   step 35400 loss 0.20936
03/24/2025 13:54:32 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 35400 loss: -0.214766
03/24/2025 13:57:21 - INFO - __main__ -   step 35500 loss 0.17983
03/24/2025 13:57:23 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 35500 loss: -0.182823
03/24/2025 14:00:12 - INFO - __main__ -   step 35600 loss 0.20446
03/24/2025 14:00:14 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 35600 loss: -0.187671
03/24/2025 14:03:03 - INFO - __main__ -   step 35700 loss 0.20005
03/24/2025 14:03:05 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 35700 loss: -0.209558
03/24/2025 14:05:54 - INFO - __main__ -   step 35800 loss 0.20672
03/24/2025 14:05:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 35800 loss: -0.239262
03/24/2025 14:08:45 - INFO - __main__ -   step 35900 loss 0.20333
03/24/2025 14:08:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 35900 loss: -0.203569
03/24/2025 14:11:36 - INFO - __main__ -   step 36000 loss 0.18162
03/24/2025 14:11:38 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 36000 loss: -0.177945
03/24/2025 14:11:43 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 14:11:43 - INFO - __main__ -     Num queries = 1400
03/24/2025 14:11:43 - INFO - __main__ -     Num codes = 4360
03/24/2025 14:11:43 - INFO - __main__ -     Batch size = 32
03/24/2025 14:12:11 - INFO - __main__ -     eval_mrr = 0.784
03/24/2025 14:12:17 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-36000-0.784
03/24/2025 14:12:20 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 14:15:09 - INFO - __main__ -   step 36100 loss 0.20888
03/24/2025 14:15:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 36100 loss: -0.212817
03/24/2025 14:18:00 - INFO - __main__ -   step 36200 loss 0.20887
03/24/2025 14:18:01 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 36200 loss: -0.214429
03/24/2025 14:20:51 - INFO - __main__ -   step 36300 loss 0.17849
03/24/2025 14:20:52 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 36300 loss: -0.161735
03/24/2025 14:23:41 - INFO - __main__ -   step 36400 loss 0.20601
03/24/2025 14:23:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 36400 loss: -0.194722
03/24/2025 14:26:32 - INFO - __main__ -   step 36500 loss 0.20726
03/24/2025 14:26:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 36500 loss: -0.192687
03/24/2025 14:29:23 - INFO - __main__ -   step 36600 loss 0.19813
03/24/2025 14:29:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 36600 loss: -0.211747
03/24/2025 14:32:14 - INFO - __main__ -   step 36700 loss 0.20422
03/24/2025 14:32:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 36700 loss: -0.167358
03/24/2025 14:35:05 - INFO - __main__ -   step 36800 loss 0.23526
03/24/2025 14:35:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 36800 loss: -0.241958
03/24/2025 14:37:56 - INFO - __main__ -   step 36900 loss 0.18669
03/24/2025 14:37:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 36900 loss: -0.182674
03/24/2025 14:40:47 - INFO - __main__ -   step 37000 loss 0.21413
03/24/2025 14:40:49 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 37000 loss: -0.22662
03/24/2025 14:40:54 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 14:40:54 - INFO - __main__ -     Num queries = 1400
03/24/2025 14:40:54 - INFO - __main__ -     Num codes = 4360
03/24/2025 14:40:54 - INFO - __main__ -     Batch size = 32
03/24/2025 14:41:22 - INFO - __main__ -     eval_mrr = 0.788
03/24/2025 14:41:23 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-37000-0.788
03/24/2025 14:41:25 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 14:44:14 - INFO - __main__ -   step 37100 loss 0.20887
03/24/2025 14:44:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 37100 loss: -0.214872
03/24/2025 14:47:05 - INFO - __main__ -   step 37200 loss 0.18893
03/24/2025 14:47:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 37200 loss: -0.177446
03/24/2025 14:49:56 - INFO - __main__ -   step 37300 loss 0.18072
03/24/2025 14:49:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 37300 loss: -0.198169
03/24/2025 14:52:47 - INFO - __main__ -   step 37400 loss 0.19349
03/24/2025 14:52:49 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 37400 loss: -0.210425
03/24/2025 14:55:38 - INFO - __main__ -   step 37500 loss 0.19494
03/24/2025 14:55:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 37500 loss: -0.204803
03/24/2025 14:58:29 - INFO - __main__ -   step 37600 loss 0.21378
03/24/2025 14:58:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 37600 loss: -0.215505
03/24/2025 15:01:20 - INFO - __main__ -   step 37700 loss 0.19733
03/24/2025 15:01:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 37700 loss: -0.184888
03/24/2025 15:04:11 - INFO - __main__ -   step 37800 loss 0.1991
03/24/2025 15:04:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 37800 loss: -0.198573
03/24/2025 15:07:02 - INFO - __main__ -   step 37900 loss 0.18804
03/24/2025 15:07:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 37900 loss: -0.165273
03/24/2025 15:09:53 - INFO - __main__ -   step 38000 loss 0.22817
03/24/2025 15:09:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 38000 loss: -0.217155
03/24/2025 15:09:59 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 15:09:59 - INFO - __main__ -     Num queries = 1400
03/24/2025 15:09:59 - INFO - __main__ -     Num codes = 4360
03/24/2025 15:09:59 - INFO - __main__ -     Batch size = 32
03/24/2025 15:10:27 - INFO - __main__ -     eval_mrr = 0.785
03/24/2025 15:10:28 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-38000-0.785
03/24/2025 15:10:30 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 15:13:19 - INFO - __main__ -   step 38100 loss 0.23462
03/24/2025 15:13:21 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 38100 loss: -0.219299
03/24/2025 15:16:09 - INFO - __main__ -   step 38200 loss 0.22368
03/24/2025 15:16:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 38200 loss: -0.191454
03/24/2025 15:18:59 - INFO - __main__ -   step 38300 loss 0.22397
03/24/2025 15:19:01 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 38300 loss: -0.235143
03/24/2025 15:21:49 - INFO - __main__ -   step 38400 loss 0.18998
03/24/2025 15:21:51 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 38400 loss: -0.225915
03/24/2025 15:24:40 - INFO - __main__ -   step 38500 loss 0.20261
03/24/2025 15:24:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 38500 loss: -0.219921
03/24/2025 15:27:30 - INFO - __main__ -   step 38600 loss 0.21445
03/24/2025 15:27:32 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 38600 loss: -0.177122
03/24/2025 15:30:20 - INFO - __main__ -   step 38700 loss 0.1905
03/24/2025 15:30:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 38700 loss: -0.204229
03/24/2025 15:33:10 - INFO - __main__ -   step 38800 loss 0.17658
03/24/2025 15:33:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 38800 loss: -0.184844
03/24/2025 15:36:01 - INFO - __main__ -   step 38900 loss 0.21021
03/24/2025 15:36:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 38900 loss: -0.172462
03/24/2025 15:38:51 - INFO - __main__ -   step 39000 loss 0.18934
03/24/2025 15:38:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 39000 loss: -0.173002
03/24/2025 15:38:57 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 15:38:57 - INFO - __main__ -     Num queries = 1400
03/24/2025 15:38:57 - INFO - __main__ -     Num codes = 4360
03/24/2025 15:38:57 - INFO - __main__ -     Batch size = 32
03/24/2025 15:39:25 - INFO - __main__ -     eval_mrr = 0.79
03/24/2025 15:39:26 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-39000-0.79
03/24/2025 15:39:29 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 15:42:17 - INFO - __main__ -   step 39100 loss 0.24224
03/24/2025 15:42:19 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 39100 loss: -0.23588
03/24/2025 15:45:08 - INFO - __main__ -   step 39200 loss 0.22014
03/24/2025 15:45:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 39200 loss: -0.239105
03/24/2025 15:47:58 - INFO - __main__ -   step 39300 loss 0.22918
03/24/2025 15:48:00 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 39300 loss: -0.242367
03/24/2025 15:50:49 - INFO - __main__ -   step 39400 loss 0.20664
03/24/2025 15:50:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 39400 loss: -0.196498
03/24/2025 15:53:39 - INFO - __main__ -   step 39500 loss 0.20219
03/24/2025 15:53:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 39500 loss: -0.198435
03/24/2025 15:56:30 - INFO - __main__ -   step 39600 loss 0.19913
03/24/2025 15:56:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 39600 loss: -0.191694
03/24/2025 15:59:20 - INFO - __main__ -   step 39700 loss 0.21819
03/24/2025 15:59:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 39700 loss: -0.226895
03/24/2025 16:02:10 - INFO - __main__ -   step 39800 loss 0.1932
03/24/2025 16:02:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 39800 loss: -0.212718
03/24/2025 16:05:01 - INFO - __main__ -   step 39900 loss 0.19037
03/24/2025 16:05:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 39900 loss: -0.180337
03/24/2025 16:07:51 - INFO - __main__ -   step 40000 loss 0.2098
03/24/2025 16:07:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 40000 loss: -0.22179
03/24/2025 16:07:58 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 16:07:58 - INFO - __main__ -     Num queries = 1400
03/24/2025 16:07:58 - INFO - __main__ -     Num codes = 4360
03/24/2025 16:07:58 - INFO - __main__ -     Batch size = 32
03/24/2025 16:08:25 - INFO - __main__ -     eval_mrr = 0.788
03/24/2025 16:08:27 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-40000-0.788
03/24/2025 16:08:29 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 16:11:18 - INFO - __main__ -   step 40100 loss 0.21569
03/24/2025 16:11:19 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 40100 loss: -0.21766
03/24/2025 16:14:08 - INFO - __main__ -   step 40200 loss 0.18929
03/24/2025 16:14:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 40200 loss: -0.210906
03/24/2025 16:16:59 - INFO - __main__ -   step 40300 loss 0.18194
03/24/2025 16:17:00 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 40300 loss: -0.176417
03/24/2025 16:19:49 - INFO - __main__ -   step 40400 loss 0.19404
03/24/2025 16:19:51 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 40400 loss: -0.19606
03/24/2025 16:22:40 - INFO - __main__ -   step 40500 loss 0.17583
03/24/2025 16:22:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 40500 loss: -0.187607
03/24/2025 16:25:30 - INFO - __main__ -   step 40600 loss 0.20946
03/24/2025 16:25:32 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 40600 loss: -0.223096
03/24/2025 16:28:21 - INFO - __main__ -   step 40700 loss 0.1991
03/24/2025 16:28:23 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 40700 loss: -0.207329
03/24/2025 16:31:12 - INFO - __main__ -   step 40800 loss 0.22406
03/24/2025 16:31:14 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 40800 loss: -0.216046
03/24/2025 16:34:03 - INFO - __main__ -   step 40900 loss 0.18964
03/24/2025 16:34:05 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 40900 loss: -0.15643
03/24/2025 16:36:54 - INFO - __main__ -   step 41000 loss 0.18259
03/24/2025 16:36:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 41000 loss: -0.159972
03/24/2025 16:37:01 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 16:37:01 - INFO - __main__ -     Num queries = 1400
03/24/2025 16:37:01 - INFO - __main__ -     Num codes = 4360
03/24/2025 16:37:01 - INFO - __main__ -     Batch size = 32
03/24/2025 16:37:29 - INFO - __main__ -     eval_mrr = 0.785
03/24/2025 16:37:29 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-41000-0.785
03/24/2025 16:37:32 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 16:40:21 - INFO - __main__ -   step 41100 loss 0.18815
03/24/2025 16:40:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 41100 loss: -0.183912
03/24/2025 16:43:12 - INFO - __main__ -   step 41200 loss 0.18869
03/24/2025 16:43:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 41200 loss: -0.170294
03/24/2025 16:46:02 - INFO - __main__ -   step 41300 loss 0.19981
03/24/2025 16:46:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 41300 loss: -0.163692
03/24/2025 16:48:53 - INFO - __main__ -   step 41400 loss 0.17986
03/24/2025 16:48:55 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 41400 loss: -0.202857
03/24/2025 16:51:44 - INFO - __main__ -   step 41500 loss 0.21159
03/24/2025 16:51:46 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 41500 loss: -0.191679
03/24/2025 16:54:35 - INFO - __main__ -   step 41600 loss 0.22858
03/24/2025 16:54:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 41600 loss: -0.225149
03/24/2025 16:57:26 - INFO - __main__ -   step 41700 loss 0.17973
03/24/2025 16:57:28 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 41700 loss: -0.154656
03/24/2025 17:00:17 - INFO - __main__ -   step 41800 loss 0.20046
03/24/2025 17:00:19 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 41800 loss: -0.220255
03/24/2025 17:03:08 - INFO - __main__ -   step 41900 loss 0.20197
03/24/2025 17:03:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 41900 loss: -0.194542
03/24/2025 17:05:59 - INFO - __main__ -   step 42000 loss 0.16863
03/24/2025 17:06:00 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 42000 loss: -0.160851
03/24/2025 17:06:12 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 17:06:12 - INFO - __main__ -     Num queries = 1400
03/24/2025 17:06:12 - INFO - __main__ -     Num codes = 4360
03/24/2025 17:06:12 - INFO - __main__ -     Batch size = 32
03/24/2025 17:06:40 - INFO - __main__ -     eval_mrr = 0.787
03/24/2025 17:06:41 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-42000-0.787
03/24/2025 17:06:44 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 17:09:32 - INFO - __main__ -   step 42100 loss 0.19536
03/24/2025 17:09:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 42100 loss: -0.187815
03/24/2025 17:12:23 - INFO - __main__ -   step 42200 loss 0.20119
03/24/2025 17:12:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 42200 loss: -0.203607
03/24/2025 17:15:14 - INFO - __main__ -   step 42300 loss 0.18802
03/24/2025 17:15:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 42300 loss: -0.174481
03/24/2025 17:18:04 - INFO - __main__ -   step 42400 loss 0.21825
03/24/2025 17:18:06 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 42400 loss: -0.243779
03/24/2025 17:20:55 - INFO - __main__ -   step 42500 loss 0.17291
03/24/2025 17:20:57 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 42500 loss: -0.157913
03/24/2025 17:23:46 - INFO - __main__ -   step 42600 loss 0.1733
03/24/2025 17:23:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 42600 loss: -0.201477
03/24/2025 17:26:37 - INFO - __main__ -   step 42700 loss 0.19913
03/24/2025 17:26:38 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 42700 loss: -0.194058
03/24/2025 17:29:27 - INFO - __main__ -   step 42800 loss 0.19428
03/24/2025 17:29:29 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 42800 loss: -0.178656
03/24/2025 17:32:18 - INFO - __main__ -   step 42900 loss 0.17697
03/24/2025 17:32:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 42900 loss: -0.184093
03/24/2025 17:35:09 - INFO - __main__ -   step 43000 loss 0.19728
03/24/2025 17:35:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 43000 loss: -0.192815
03/24/2025 17:35:15 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 17:35:15 - INFO - __main__ -     Num queries = 1400
03/24/2025 17:35:15 - INFO - __main__ -     Num codes = 4360
03/24/2025 17:35:15 - INFO - __main__ -     Batch size = 32
03/24/2025 17:35:43 - INFO - __main__ -     eval_mrr = 0.786
03/24/2025 17:35:44 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-43000-0.786
03/24/2025 17:35:46 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 17:38:35 - INFO - __main__ -   step 43100 loss 0.17566
03/24/2025 17:38:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 43100 loss: -0.175652
03/24/2025 17:41:26 - INFO - __main__ -   step 43200 loss 0.19165
03/24/2025 17:41:28 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 43200 loss: -0.187146
03/24/2025 17:44:17 - INFO - __main__ -   step 43300 loss 0.19703
03/24/2025 17:44:19 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 43300 loss: -0.180386
03/24/2025 17:47:08 - INFO - __main__ -   step 43400 loss 0.18946
03/24/2025 17:47:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 43400 loss: -0.201509
03/24/2025 17:49:58 - INFO - __main__ -   step 43500 loss 0.16894
03/24/2025 17:50:00 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 43500 loss: -0.169861
03/24/2025 17:52:49 - INFO - __main__ -   step 43600 loss 0.15692
03/24/2025 17:52:51 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 43600 loss: -0.148409
03/24/2025 17:55:40 - INFO - __main__ -   step 43700 loss 0.18593
03/24/2025 17:55:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 43700 loss: -0.197612
03/24/2025 17:58:30 - INFO - __main__ -   step 43800 loss 0.17683
03/24/2025 17:58:32 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 43800 loss: -0.17287
03/24/2025 18:01:21 - INFO - __main__ -   step 43900 loss 0.18988
03/24/2025 18:01:23 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 43900 loss: -0.210578
03/24/2025 18:04:12 - INFO - __main__ -   step 44000 loss 0.18647
03/24/2025 18:04:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 44000 loss: -0.20349
03/24/2025 18:04:18 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 18:04:18 - INFO - __main__ -     Num queries = 1400
03/24/2025 18:04:18 - INFO - __main__ -     Num codes = 4360
03/24/2025 18:04:18 - INFO - __main__ -     Batch size = 32
03/24/2025 18:04:46 - INFO - __main__ -     eval_mrr = 0.777
03/24/2025 18:04:47 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-44000-0.777
03/24/2025 18:04:50 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 18:07:38 - INFO - __main__ -   step 44100 loss 0.20407
03/24/2025 18:07:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 44100 loss: -0.203625
03/24/2025 18:10:29 - INFO - __main__ -   step 44200 loss 0.18607
03/24/2025 18:10:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 44200 loss: -0.190105
03/24/2025 18:13:20 - INFO - __main__ -   step 44300 loss 0.16714
03/24/2025 18:13:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 44300 loss: -0.164417
03/24/2025 18:16:11 - INFO - __main__ -   step 44400 loss 0.17993
03/24/2025 18:16:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 44400 loss: -0.206418
03/24/2025 18:19:01 - INFO - __main__ -   step 44500 loss 0.17685
03/24/2025 18:19:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 44500 loss: -0.175267
03/24/2025 18:21:52 - INFO - __main__ -   step 44600 loss 0.16698
03/24/2025 18:21:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 44600 loss: -0.1773
03/24/2025 18:24:43 - INFO - __main__ -   step 44700 loss 0.18048
03/24/2025 18:24:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 44700 loss: -0.19755
03/24/2025 18:27:34 - INFO - __main__ -   step 44800 loss 0.17477
03/24/2025 18:27:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 44800 loss: -0.161304
03/24/2025 18:30:24 - INFO - __main__ -   step 44900 loss 0.16499
03/24/2025 18:30:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 44900 loss: -0.172499
03/24/2025 18:33:15 - INFO - __main__ -   step 45000 loss 0.17712
03/24/2025 18:33:17 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 45000 loss: -0.194434
03/24/2025 18:33:22 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 18:33:22 - INFO - __main__ -     Num queries = 1400
03/24/2025 18:33:22 - INFO - __main__ -     Num codes = 4360
03/24/2025 18:33:22 - INFO - __main__ -     Batch size = 32
03/24/2025 18:33:50 - INFO - __main__ -     eval_mrr = 0.777
03/24/2025 18:33:51 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-45000-0.777
03/24/2025 18:33:53 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 18:36:42 - INFO - __main__ -   step 45100 loss 0.15801
03/24/2025 18:36:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 45100 loss: -0.14583
03/24/2025 18:39:33 - INFO - __main__ -   step 45200 loss 0.17935
03/24/2025 18:39:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 45200 loss: -0.182311
03/24/2025 18:42:23 - INFO - __main__ -   step 45300 loss 0.22012
03/24/2025 18:42:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 45300 loss: -0.211922
03/24/2025 18:45:14 - INFO - __main__ -   step 45400 loss 0.1989
03/24/2025 18:45:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 45400 loss: -0.203651
03/24/2025 18:48:05 - INFO - __main__ -   step 45500 loss 0.18452
03/24/2025 18:48:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 45500 loss: -0.174369
03/24/2025 18:50:56 - INFO - __main__ -   step 45600 loss 0.17643
03/24/2025 18:50:57 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 45600 loss: -0.179192
03/24/2025 18:53:46 - INFO - __main__ -   step 45700 loss 0.18217
03/24/2025 18:53:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 45700 loss: -0.17379
03/24/2025 18:56:37 - INFO - __main__ -   step 45800 loss 0.14977
03/24/2025 18:56:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 45800 loss: -0.148862
03/24/2025 18:59:28 - INFO - __main__ -   step 45900 loss 0.20618
03/24/2025 18:59:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 45900 loss: -0.192864
03/24/2025 19:02:19 - INFO - __main__ -   step 46000 loss 0.16898
03/24/2025 19:02:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 46000 loss: -0.139072
03/24/2025 19:02:25 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 19:02:25 - INFO - __main__ -     Num queries = 1400
03/24/2025 19:02:25 - INFO - __main__ -     Num codes = 4360
03/24/2025 19:02:25 - INFO - __main__ -     Batch size = 32
03/24/2025 19:02:54 - INFO - __main__ -     eval_mrr = 0.781
03/24/2025 19:02:55 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-46000-0.781
03/24/2025 19:02:57 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 19:05:46 - INFO - __main__ -   step 46100 loss 0.17564
03/24/2025 19:05:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 46100 loss: -0.152673
03/24/2025 19:08:36 - INFO - __main__ -   step 46200 loss 0.19169
03/24/2025 19:08:38 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 46200 loss: -0.218949
03/24/2025 19:11:27 - INFO - __main__ -   step 46300 loss 0.17048
03/24/2025 19:11:29 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 46300 loss: -0.184264
03/24/2025 19:14:18 - INFO - __main__ -   step 46400 loss 0.17537
03/24/2025 19:14:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 46400 loss: -0.197649
03/24/2025 19:17:08 - INFO - __main__ -   step 46500 loss 0.14747
03/24/2025 19:17:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 46500 loss: -0.14366
03/24/2025 19:19:59 - INFO - __main__ -   step 46600 loss 0.19146
03/24/2025 19:20:01 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 46600 loss: -0.19869
03/24/2025 19:22:50 - INFO - __main__ -   step 46700 loss 0.18766
03/24/2025 19:22:52 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 46700 loss: -0.17975
03/24/2025 19:25:40 - INFO - __main__ -   step 46800 loss 0.17519
03/24/2025 19:25:42 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 46800 loss: -0.180166
03/24/2025 19:28:31 - INFO - __main__ -   step 46900 loss 0.1615
03/24/2025 19:28:33 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 46900 loss: -0.145366
03/24/2025 19:31:21 - INFO - __main__ -   step 47000 loss 0.17643
03/24/2025 19:31:23 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 47000 loss: -0.187955
03/24/2025 19:31:28 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 19:31:28 - INFO - __main__ -     Num queries = 1400
03/24/2025 19:31:28 - INFO - __main__ -     Num codes = 4360
03/24/2025 19:31:28 - INFO - __main__ -     Batch size = 32
03/24/2025 19:31:56 - INFO - __main__ -     eval_mrr = 0.781
03/24/2025 19:31:57 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-47000-0.781
03/24/2025 19:31:59 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 19:34:48 - INFO - __main__ -   step 47100 loss 0.17023
03/24/2025 19:34:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 47100 loss: -0.189921
03/24/2025 19:37:39 - INFO - __main__ -   step 47200 loss 0.1559
03/24/2025 19:37:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 47200 loss: -0.148536
03/24/2025 19:40:29 - INFO - __main__ -   step 47300 loss 0.16165
03/24/2025 19:40:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 47300 loss: -0.166946
03/24/2025 19:43:20 - INFO - __main__ -   step 47400 loss 0.15788
03/24/2025 19:43:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 47400 loss: -0.154718
03/24/2025 19:46:11 - INFO - __main__ -   step 47500 loss 0.14642
03/24/2025 19:46:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 47500 loss: -0.153078
03/24/2025 19:49:02 - INFO - __main__ -   step 47600 loss 0.18376
03/24/2025 19:49:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 47600 loss: -0.173837
03/24/2025 19:51:52 - INFO - __main__ -   step 47700 loss 0.15257
03/24/2025 19:51:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 47700 loss: -0.147609
03/24/2025 19:54:43 - INFO - __main__ -   step 47800 loss 0.16836
03/24/2025 19:54:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 47800 loss: -0.166977
03/24/2025 19:57:33 - INFO - __main__ -   step 47900 loss 0.15093
03/24/2025 19:57:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 47900 loss: -0.160416
03/24/2025 20:00:24 - INFO - __main__ -   step 48000 loss 0.17114
03/24/2025 20:00:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 48000 loss: -0.16345
03/24/2025 20:00:31 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 20:00:31 - INFO - __main__ -     Num queries = 1400
03/24/2025 20:00:31 - INFO - __main__ -     Num codes = 4360
03/24/2025 20:00:31 - INFO - __main__ -     Batch size = 32
03/24/2025 20:00:59 - INFO - __main__ -     eval_mrr = 0.785
03/24/2025 20:01:00 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-48000-0.785
03/24/2025 20:01:02 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 20:03:51 - INFO - __main__ -   step 48100 loss 0.15897
03/24/2025 20:03:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 48100 loss: -0.164639
03/24/2025 20:06:41 - INFO - __main__ -   step 48200 loss 0.16728
03/24/2025 20:06:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 48200 loss: -0.185836
03/24/2025 20:09:32 - INFO - __main__ -   step 48300 loss 0.16924
03/24/2025 20:09:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 48300 loss: -0.171956
03/24/2025 20:12:23 - INFO - __main__ -   step 48400 loss 0.16315
03/24/2025 20:12:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 48400 loss: -0.185097
03/24/2025 20:15:13 - INFO - __main__ -   step 48500 loss 0.15941
03/24/2025 20:15:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 48500 loss: -0.141123
03/24/2025 20:18:04 - INFO - __main__ -   step 48600 loss 0.15956
03/24/2025 20:18:06 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 48600 loss: -0.153145
03/24/2025 20:20:55 - INFO - __main__ -   step 48700 loss 0.18031
03/24/2025 20:20:57 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 48700 loss: -0.176135
03/24/2025 20:23:46 - INFO - __main__ -   step 48800 loss 0.15027
03/24/2025 20:23:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 48800 loss: -0.145616
03/24/2025 20:26:36 - INFO - __main__ -   step 48900 loss 0.16595
03/24/2025 20:26:38 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 48900 loss: -0.157091
03/24/2025 20:29:27 - INFO - __main__ -   step 49000 loss 0.15996
03/24/2025 20:29:29 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 49000 loss: -0.133066
03/24/2025 20:29:34 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 20:29:34 - INFO - __main__ -     Num queries = 1400
03/24/2025 20:29:34 - INFO - __main__ -     Num codes = 4360
03/24/2025 20:29:34 - INFO - __main__ -     Batch size = 32
03/24/2025 20:30:02 - INFO - __main__ -     eval_mrr = 0.782
03/24/2025 20:30:03 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-49000-0.782
03/24/2025 20:30:05 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 20:32:54 - INFO - __main__ -   step 49100 loss 0.16313
03/24/2025 20:32:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 49100 loss: -0.168239
03/24/2025 20:35:44 - INFO - __main__ -   step 49200 loss 0.15087
03/24/2025 20:35:46 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 49200 loss: -0.155892
03/24/2025 20:38:35 - INFO - __main__ -   step 49300 loss 0.13032
03/24/2025 20:38:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 49300 loss: -0.104465
03/24/2025 20:41:26 - INFO - __main__ -   step 49400 loss 0.15604
03/24/2025 20:41:28 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 49400 loss: -0.134759
03/24/2025 20:44:17 - INFO - __main__ -   step 49500 loss 0.16421
03/24/2025 20:44:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 49500 loss: -0.178476
03/24/2025 20:47:07 - INFO - __main__ -   step 49600 loss 0.14764
03/24/2025 20:47:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 49600 loss: -0.148477
03/24/2025 20:49:58 - INFO - __main__ -   step 49700 loss 0.16764
03/24/2025 20:50:00 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 49700 loss: -0.159791
03/24/2025 20:52:48 - INFO - __main__ -   step 49800 loss 0.15655
03/24/2025 20:52:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 49800 loss: -0.14206
03/24/2025 20:55:39 - INFO - __main__ -   step 49900 loss 0.13889
03/24/2025 20:55:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 49900 loss: -0.124289
03/24/2025 20:58:30 - INFO - __main__ -   step 50000 loss 0.15522
03/24/2025 20:58:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 50000 loss: -0.187306
03/24/2025 20:58:36 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 20:58:36 - INFO - __main__ -     Num queries = 1400
03/24/2025 20:58:36 - INFO - __main__ -     Num codes = 4360
03/24/2025 20:58:36 - INFO - __main__ -     Batch size = 32
03/24/2025 20:59:05 - INFO - __main__ -     eval_mrr = 0.778
03/24/2025 20:59:05 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-50000-0.778
03/24/2025 20:59:08 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 21:01:57 - INFO - __main__ -   step 50100 loss 0.14514
03/24/2025 21:01:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 50100 loss: -0.132835
03/24/2025 21:04:47 - INFO - __main__ -   step 50200 loss 0.12982
03/24/2025 21:04:49 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 50200 loss: -0.123479
03/24/2025 21:07:38 - INFO - __main__ -   step 50300 loss 0.16652
03/24/2025 21:07:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 50300 loss: -0.150941
03/24/2025 21:10:28 - INFO - __main__ -   step 50400 loss 0.15228
03/24/2025 21:10:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 50400 loss: -0.166712
03/24/2025 21:13:19 - INFO - __main__ -   step 50500 loss 0.13426
03/24/2025 21:13:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 50500 loss: -0.12064
03/24/2025 21:16:09 - INFO - __main__ -   step 50600 loss 0.17113
03/24/2025 21:16:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 50600 loss: -0.182319
03/24/2025 21:19:00 - INFO - __main__ -   step 50700 loss 0.16824
03/24/2025 21:19:01 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 50700 loss: -0.152021
03/24/2025 21:21:50 - INFO - __main__ -   step 50800 loss 0.14366
03/24/2025 21:21:52 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 50800 loss: -0.13953
03/24/2025 21:24:41 - INFO - __main__ -   step 50900 loss 0.15941
03/24/2025 21:24:42 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 50900 loss: -0.159956
03/24/2025 21:27:31 - INFO - __main__ -   step 51000 loss 0.13998
03/24/2025 21:27:33 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 51000 loss: -0.136098
03/24/2025 21:27:38 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 21:27:38 - INFO - __main__ -     Num queries = 1400
03/24/2025 21:27:38 - INFO - __main__ -     Num codes = 4360
03/24/2025 21:27:38 - INFO - __main__ -     Batch size = 32
03/24/2025 21:28:06 - INFO - __main__ -     eval_mrr = 0.781
03/24/2025 21:28:07 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-51000-0.781
03/24/2025 21:28:09 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 21:30:58 - INFO - __main__ -   step 51100 loss 0.11884
03/24/2025 21:30:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 51100 loss: -0.129095
03/24/2025 21:33:48 - INFO - __main__ -   step 51200 loss 0.16389
03/24/2025 21:33:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 51200 loss: -0.1525
03/24/2025 21:36:39 - INFO - __main__ -   step 51300 loss 0.16929
03/24/2025 21:36:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 51300 loss: -0.16871
03/24/2025 21:39:29 - INFO - __main__ -   step 51400 loss 0.15512
03/24/2025 21:39:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 51400 loss: -0.152652
03/24/2025 21:42:20 - INFO - __main__ -   step 51500 loss 0.15151
03/24/2025 21:42:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 51500 loss: -0.153644
03/24/2025 21:45:11 - INFO - __main__ -   step 51600 loss 0.15412
03/24/2025 21:45:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 51600 loss: -0.150311
03/24/2025 21:48:01 - INFO - __main__ -   step 51700 loss 0.15682
03/24/2025 21:48:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 51700 loss: -0.161101
03/24/2025 21:50:52 - INFO - __main__ -   step 51800 loss 0.1591
03/24/2025 21:50:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 51800 loss: -0.16224
03/24/2025 21:53:42 - INFO - __main__ -   step 51900 loss 0.1391
03/24/2025 21:53:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 51900 loss: -0.134043
03/24/2025 21:56:33 - INFO - __main__ -   step 52000 loss 0.16346
03/24/2025 21:56:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 52000 loss: -0.158393
03/24/2025 21:56:40 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 21:56:40 - INFO - __main__ -     Num queries = 1400
03/24/2025 21:56:40 - INFO - __main__ -     Num codes = 4360
03/24/2025 21:56:40 - INFO - __main__ -     Batch size = 32
03/24/2025 21:57:08 - INFO - __main__ -     eval_mrr = 0.777
03/24/2025 21:57:08 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-52000-0.777
03/24/2025 21:57:11 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 22:00:00 - INFO - __main__ -   step 52100 loss 0.16357
03/24/2025 22:00:01 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 52100 loss: -0.140979
03/24/2025 22:02:50 - INFO - __main__ -   step 52200 loss 0.15482
03/24/2025 22:02:52 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 52200 loss: -0.145589
03/24/2025 22:05:41 - INFO - __main__ -   step 52300 loss 0.1641
03/24/2025 22:05:42 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 52300 loss: -0.154516
03/24/2025 22:08:31 - INFO - __main__ -   step 52400 loss 0.15233
03/24/2025 22:08:33 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 52400 loss: -0.148757
03/24/2025 22:11:22 - INFO - __main__ -   step 52500 loss 0.14618
03/24/2025 22:11:24 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 52500 loss: -0.152747
03/24/2025 22:14:12 - INFO - __main__ -   step 52600 loss 0.14775
03/24/2025 22:14:14 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 52600 loss: -0.152266
03/24/2025 22:17:03 - INFO - __main__ -   step 52700 loss 0.14391
03/24/2025 22:17:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 52700 loss: -0.154067
03/24/2025 22:19:53 - INFO - __main__ -   step 52800 loss 0.14703
03/24/2025 22:19:55 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 52800 loss: -0.130866
03/24/2025 22:22:44 - INFO - __main__ -   step 52900 loss 0.16703
03/24/2025 22:22:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 52900 loss: -0.178852
03/24/2025 22:25:34 - INFO - __main__ -   step 53000 loss 0.15619
03/24/2025 22:25:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 53000 loss: -0.143989
03/24/2025 22:25:41 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 22:25:41 - INFO - __main__ -     Num queries = 1400
03/24/2025 22:25:41 - INFO - __main__ -     Num codes = 4360
03/24/2025 22:25:41 - INFO - __main__ -     Batch size = 32
03/24/2025 22:26:08 - INFO - __main__ -     eval_mrr = 0.776
03/24/2025 22:26:09 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-53000-0.776
03/24/2025 22:26:12 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 22:29:01 - INFO - __main__ -   step 53100 loss 0.13786
03/24/2025 22:29:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 53100 loss: -0.161446
03/24/2025 22:31:51 - INFO - __main__ -   step 53200 loss 0.13161
03/24/2025 22:31:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 53200 loss: -0.142691
03/24/2025 22:34:42 - INFO - __main__ -   step 53300 loss 0.13101
03/24/2025 22:34:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 53300 loss: -0.135995
03/24/2025 22:37:32 - INFO - __main__ -   step 53400 loss 0.1755
03/24/2025 22:37:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 53400 loss: -0.177719
03/24/2025 22:40:23 - INFO - __main__ -   step 53500 loss 0.13238
03/24/2025 22:40:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 53500 loss: -0.148264
03/24/2025 22:43:13 - INFO - __main__ -   step 53600 loss 0.13371
03/24/2025 22:43:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 53600 loss: -0.124241
03/24/2025 22:46:04 - INFO - __main__ -   step 53700 loss 0.16188
03/24/2025 22:46:06 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 53700 loss: -0.197997
03/24/2025 22:48:54 - INFO - __main__ -   step 53800 loss 0.14904
03/24/2025 22:48:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 53800 loss: -0.150488
03/24/2025 22:51:45 - INFO - __main__ -   step 53900 loss 0.13742
03/24/2025 22:51:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 53900 loss: -0.13594
03/24/2025 22:54:35 - INFO - __main__ -   step 54000 loss 0.15599
03/24/2025 22:54:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 54000 loss: -0.13883
03/24/2025 22:54:42 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 22:54:42 - INFO - __main__ -     Num queries = 1400
03/24/2025 22:54:42 - INFO - __main__ -     Num codes = 4360
03/24/2025 22:54:42 - INFO - __main__ -     Batch size = 32
03/24/2025 22:55:10 - INFO - __main__ -     eval_mrr = 0.783
03/24/2025 22:55:11 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-54000-0.783
03/24/2025 22:55:13 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 22:58:02 - INFO - __main__ -   step 54100 loss 0.13569
03/24/2025 22:58:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 54100 loss: -0.136747
03/24/2025 23:00:53 - INFO - __main__ -   step 54200 loss 0.16165
03/24/2025 23:00:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 54200 loss: -0.145863
03/24/2025 23:03:43 - INFO - __main__ -   step 54300 loss 0.16031
03/24/2025 23:03:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 54300 loss: -0.177477
03/24/2025 23:06:34 - INFO - __main__ -   step 54400 loss 0.19471
03/24/2025 23:06:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 54400 loss: -0.214485
03/24/2025 23:09:24 - INFO - __main__ -   step 54500 loss 0.1375
03/24/2025 23:09:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 54500 loss: -0.128701
03/24/2025 23:12:15 - INFO - __main__ -   step 54600 loss 0.15166
03/24/2025 23:12:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 54600 loss: -0.159269
03/24/2025 23:15:05 - INFO - __main__ -   step 54700 loss 0.13182
03/24/2025 23:15:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 54700 loss: -0.138989
03/24/2025 23:17:56 - INFO - __main__ -   step 54800 loss 0.14026
03/24/2025 23:17:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 54800 loss: -0.157633
03/24/2025 23:20:46 - INFO - __main__ -   step 54900 loss 0.15557
03/24/2025 23:20:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 54900 loss: -0.159844
03/24/2025 23:23:37 - INFO - __main__ -   step 55000 loss 0.15394
03/24/2025 23:23:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 55000 loss: -0.146653
03/24/2025 23:23:44 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 23:23:44 - INFO - __main__ -     Num queries = 1400
03/24/2025 23:23:44 - INFO - __main__ -     Num codes = 4360
03/24/2025 23:23:44 - INFO - __main__ -     Batch size = 32
03/24/2025 23:24:11 - INFO - __main__ -     eval_mrr = 0.776
03/24/2025 23:24:12 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-55000-0.776
03/24/2025 23:24:15 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 23:27:04 - INFO - __main__ -   step 55100 loss 0.16252
03/24/2025 23:27:06 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 55100 loss: -0.166773
03/24/2025 23:29:54 - INFO - __main__ -   step 55200 loss 0.13338
03/24/2025 23:29:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 55200 loss: -0.115485
03/24/2025 23:32:45 - INFO - __main__ -   step 55300 loss 0.15663
03/24/2025 23:32:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 55300 loss: -0.157871
03/24/2025 23:35:35 - INFO - __main__ -   step 55400 loss 0.14967
03/24/2025 23:35:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 55400 loss: -0.161493
03/24/2025 23:38:26 - INFO - __main__ -   step 55500 loss 0.16778
03/24/2025 23:38:28 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 55500 loss: -0.152951
03/24/2025 23:41:17 - INFO - __main__ -   step 55600 loss 0.15146
03/24/2025 23:41:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 55600 loss: -0.141508
03/24/2025 23:44:07 - INFO - __main__ -   step 55700 loss 0.15579
03/24/2025 23:44:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 55700 loss: -0.143692
03/24/2025 23:46:58 - INFO - __main__ -   step 55800 loss 0.15718
03/24/2025 23:46:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 55800 loss: -0.12761
03/24/2025 23:49:48 - INFO - __main__ -   step 55900 loss 0.1478
03/24/2025 23:49:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 55900 loss: -0.147551
03/24/2025 23:52:39 - INFO - __main__ -   step 56000 loss 0.1346
03/24/2025 23:52:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 56000 loss: -0.134449
03/24/2025 23:52:45 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/24/2025 23:52:45 - INFO - __main__ -     Num queries = 1400
03/24/2025 23:52:45 - INFO - __main__ -     Num codes = 4360
03/24/2025 23:52:45 - INFO - __main__ -     Batch size = 32
03/24/2025 23:53:13 - INFO - __main__ -     eval_mrr = 0.779
03/24/2025 23:53:14 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-56000-0.779
03/24/2025 23:53:16 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/24/2025 23:56:05 - INFO - __main__ -   step 56100 loss 0.12935
03/24/2025 23:56:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 56100 loss: -0.120072
03/24/2025 23:58:56 - INFO - __main__ -   step 56200 loss 0.16891
03/24/2025 23:58:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 56200 loss: -0.140738
03/25/2025 00:01:47 - INFO - __main__ -   step 56300 loss 0.14395
03/25/2025 00:01:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 56300 loss: -0.142291
03/25/2025 00:04:37 - INFO - __main__ -   step 56400 loss 0.11884
03/25/2025 00:04:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 56400 loss: -0.122255
03/25/2025 00:07:28 - INFO - __main__ -   step 56500 loss 0.15405
03/25/2025 00:07:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 56500 loss: -0.146134
03/25/2025 00:10:19 - INFO - __main__ -   step 56600 loss 0.13542
03/25/2025 00:10:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 56600 loss: -0.15478
03/25/2025 00:13:09 - INFO - __main__ -   step 56700 loss 0.1455
03/25/2025 00:13:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 56700 loss: -0.152374
03/25/2025 00:16:00 - INFO - __main__ -   step 56800 loss 0.14103
03/25/2025 00:16:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 56800 loss: -0.134564
03/25/2025 00:18:50 - INFO - __main__ -   step 56900 loss 0.15332
03/25/2025 00:18:52 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 56900 loss: -0.142682
03/25/2025 00:21:41 - INFO - __main__ -   step 57000 loss 0.1535
03/25/2025 00:21:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 57000 loss: -0.156372
03/25/2025 00:21:48 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 00:21:48 - INFO - __main__ -     Num queries = 1400
03/25/2025 00:21:48 - INFO - __main__ -     Num codes = 4360
03/25/2025 00:21:48 - INFO - __main__ -     Batch size = 32
03/25/2025 00:22:16 - INFO - __main__ -     eval_mrr = 0.782
03/25/2025 00:22:16 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-57000-0.782
03/25/2025 00:22:19 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 00:25:08 - INFO - __main__ -   step 57100 loss 0.14615
03/25/2025 00:25:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 57100 loss: -0.146495
03/25/2025 00:27:58 - INFO - __main__ -   step 57200 loss 0.12429
03/25/2025 00:28:00 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 57200 loss: -0.117221
03/25/2025 00:30:49 - INFO - __main__ -   step 57300 loss 0.12207
03/25/2025 00:30:51 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 57300 loss: -0.104832
03/25/2025 00:33:40 - INFO - __main__ -   step 57400 loss 0.1599
03/25/2025 00:33:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 57400 loss: -0.143419
03/25/2025 00:36:30 - INFO - __main__ -   step 57500 loss 0.1447
03/25/2025 00:36:32 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 57500 loss: -0.152439
03/25/2025 00:39:21 - INFO - __main__ -   step 57600 loss 0.18032
03/25/2025 00:39:23 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 57600 loss: -0.186485
03/25/2025 00:42:12 - INFO - __main__ -   step 57700 loss 0.14956
03/25/2025 00:42:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 57700 loss: -0.152278
03/25/2025 00:45:02 - INFO - __main__ -   step 57800 loss 0.14385
03/25/2025 00:45:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 57800 loss: -0.149741
03/25/2025 00:47:53 - INFO - __main__ -   step 57900 loss 0.13089
03/25/2025 00:47:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 57900 loss: -0.134274
03/25/2025 00:50:43 - INFO - __main__ -   step 58000 loss 0.1427
03/25/2025 00:50:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 58000 loss: -0.132464
03/25/2025 00:50:50 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 00:50:50 - INFO - __main__ -     Num queries = 1400
03/25/2025 00:50:50 - INFO - __main__ -     Num codes = 4360
03/25/2025 00:50:50 - INFO - __main__ -     Batch size = 32
03/25/2025 00:51:18 - INFO - __main__ -     eval_mrr = 0.782
03/25/2025 00:51:19 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-58000-0.782
03/25/2025 00:51:21 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 00:54:10 - INFO - __main__ -   step 58100 loss 0.15077
03/25/2025 00:54:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 58100 loss: -0.124711
03/25/2025 00:57:01 - INFO - __main__ -   step 58200 loss 0.13693
03/25/2025 00:57:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 58200 loss: -0.1393
03/25/2025 00:59:52 - INFO - __main__ -   step 58300 loss 0.14544
03/25/2025 00:59:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 58300 loss: -0.138829
03/25/2025 01:02:42 - INFO - __main__ -   step 58400 loss 0.1301
03/25/2025 01:02:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 58400 loss: -0.143861
03/25/2025 01:05:33 - INFO - __main__ -   step 58500 loss 0.14892
03/25/2025 01:05:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 58500 loss: -0.156222
03/25/2025 01:08:23 - INFO - __main__ -   step 58600 loss 0.16176
03/25/2025 01:08:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 58600 loss: -0.165829
03/25/2025 01:11:14 - INFO - __main__ -   step 58700 loss 0.16791
03/25/2025 01:11:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 58700 loss: -0.185131
03/25/2025 01:14:04 - INFO - __main__ -   step 58800 loss 0.1316
03/25/2025 01:14:06 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 58800 loss: -0.145939
03/25/2025 01:16:55 - INFO - __main__ -   step 58900 loss 0.14369
03/25/2025 01:16:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 58900 loss: -0.15827
03/25/2025 01:19:45 - INFO - __main__ -   step 59000 loss 0.12703
03/25/2025 01:19:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 59000 loss: -0.136778
03/25/2025 01:19:52 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 01:19:52 - INFO - __main__ -     Num queries = 1400
03/25/2025 01:19:52 - INFO - __main__ -     Num codes = 4360
03/25/2025 01:19:52 - INFO - __main__ -     Batch size = 32
03/25/2025 01:20:20 - INFO - __main__ -     eval_mrr = 0.778
03/25/2025 01:20:20 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-59000-0.778
03/25/2025 01:20:23 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 01:23:12 - INFO - __main__ -   step 59100 loss 0.12917
03/25/2025 01:23:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 59100 loss: -0.104787
03/25/2025 01:26:02 - INFO - __main__ -   step 59200 loss 0.11156
03/25/2025 01:26:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 59200 loss: -0.110294
03/25/2025 01:28:53 - INFO - __main__ -   step 59300 loss 0.14061
03/25/2025 01:28:55 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 59300 loss: -0.109859
03/25/2025 01:31:44 - INFO - __main__ -   step 59400 loss 0.14419
03/25/2025 01:31:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 59400 loss: -0.146508
03/25/2025 01:34:34 - INFO - __main__ -   step 59500 loss 0.12712
03/25/2025 01:34:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 59500 loss: -0.146143
03/25/2025 01:37:25 - INFO - __main__ -   step 59600 loss 0.14437
03/25/2025 01:37:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 59600 loss: -0.150012
03/25/2025 01:40:15 - INFO - __main__ -   step 59700 loss 0.15235
03/25/2025 01:40:17 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 59700 loss: -0.159704
03/25/2025 01:43:06 - INFO - __main__ -   step 59800 loss 0.16309
03/25/2025 01:43:08 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 59800 loss: -0.157782
03/25/2025 01:45:57 - INFO - __main__ -   step 59900 loss 0.1563
03/25/2025 01:45:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 59900 loss: -0.159939
03/25/2025 01:48:47 - INFO - __main__ -   step 60000 loss 0.14985
03/25/2025 01:48:49 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 60000 loss: -0.153797
03/25/2025 01:48:54 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 01:48:54 - INFO - __main__ -     Num queries = 1400
03/25/2025 01:48:54 - INFO - __main__ -     Num codes = 4360
03/25/2025 01:48:54 - INFO - __main__ -     Batch size = 32
03/25/2025 01:49:22 - INFO - __main__ -     eval_mrr = 0.775
03/25/2025 01:49:23 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-60000-0.775
03/25/2025 01:49:25 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 01:52:14 - INFO - __main__ -   step 60100 loss 0.14437
03/25/2025 01:52:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 60100 loss: -0.144221
03/25/2025 01:55:04 - INFO - __main__ -   step 60200 loss 0.12859
03/25/2025 01:55:06 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 60200 loss: -0.144681
03/25/2025 01:57:55 - INFO - __main__ -   step 60300 loss 0.14056
03/25/2025 01:57:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 60300 loss: -0.1166
03/25/2025 02:00:45 - INFO - __main__ -   step 60400 loss 0.13727
03/25/2025 02:00:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 60400 loss: -0.119991
03/25/2025 02:03:36 - INFO - __main__ -   step 60500 loss 0.11425
03/25/2025 02:03:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 60500 loss: -0.133934
03/25/2025 02:06:26 - INFO - __main__ -   step 60600 loss 0.1204
03/25/2025 02:06:28 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 60600 loss: -0.112609
03/25/2025 02:09:17 - INFO - __main__ -   step 60700 loss 0.14046
03/25/2025 02:09:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 60700 loss: -0.152387
03/25/2025 02:12:07 - INFO - __main__ -   step 60800 loss 0.15006
03/25/2025 02:12:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 60800 loss: -0.146537
03/25/2025 02:14:58 - INFO - __main__ -   step 60900 loss 0.14048
03/25/2025 02:15:00 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 60900 loss: -0.127798
03/25/2025 02:17:48 - INFO - __main__ -   step 61000 loss 0.14859
03/25/2025 02:17:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 61000 loss: -0.128313
03/25/2025 02:17:55 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 02:17:55 - INFO - __main__ -     Num queries = 1400
03/25/2025 02:17:55 - INFO - __main__ -     Num codes = 4360
03/25/2025 02:17:55 - INFO - __main__ -     Batch size = 32
03/25/2025 02:18:23 - INFO - __main__ -     eval_mrr = 0.78
03/25/2025 02:18:24 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-61000-0.78
03/25/2025 02:18:26 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 02:21:15 - INFO - __main__ -   step 61100 loss 0.12809
03/25/2025 02:21:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 61100 loss: -0.12799
03/25/2025 02:24:05 - INFO - __main__ -   step 61200 loss 0.14149
03/25/2025 02:24:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 61200 loss: -0.14168
03/25/2025 02:26:56 - INFO - __main__ -   step 61300 loss 0.13233
03/25/2025 02:26:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 61300 loss: -0.140467
03/25/2025 02:29:46 - INFO - __main__ -   step 61400 loss 0.17667
03/25/2025 02:29:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 61400 loss: -0.160982
03/25/2025 02:32:37 - INFO - __main__ -   step 61500 loss 0.12718
03/25/2025 02:32:38 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 61500 loss: -0.111774
03/25/2025 02:35:27 - INFO - __main__ -   step 61600 loss 0.15654
03/25/2025 02:35:29 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 61600 loss: -0.17507
03/25/2025 02:38:18 - INFO - __main__ -   step 61700 loss 0.1491
03/25/2025 02:38:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 61700 loss: -0.153237
03/25/2025 02:41:09 - INFO - __main__ -   step 61800 loss 0.14641
03/25/2025 02:41:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 61800 loss: -0.141376
03/25/2025 02:43:59 - INFO - __main__ -   step 61900 loss 0.12881
03/25/2025 02:44:01 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 61900 loss: -0.12498
03/25/2025 02:46:49 - INFO - __main__ -   step 62000 loss 0.14748
03/25/2025 02:46:51 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 62000 loss: -0.147444
03/25/2025 02:46:56 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 02:46:56 - INFO - __main__ -     Num queries = 1400
03/25/2025 02:46:56 - INFO - __main__ -     Num codes = 4360
03/25/2025 02:46:56 - INFO - __main__ -     Batch size = 32
03/25/2025 02:47:24 - INFO - __main__ -     eval_mrr = 0.78
03/25/2025 02:47:25 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-62000-0.78
03/25/2025 02:47:27 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 02:50:16 - INFO - __main__ -   step 62100 loss 0.1328
03/25/2025 02:50:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 62100 loss: -0.126803
03/25/2025 02:53:06 - INFO - __main__ -   step 62200 loss 0.16051
03/25/2025 02:53:08 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 62200 loss: -0.163053
03/25/2025 02:55:57 - INFO - __main__ -   step 62300 loss 0.13136
03/25/2025 02:55:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 62300 loss: -0.113304
03/25/2025 02:58:47 - INFO - __main__ -   step 62400 loss 0.13076
03/25/2025 02:58:49 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 62400 loss: -0.125151
03/25/2025 03:01:38 - INFO - __main__ -   step 62500 loss 0.1524
03/25/2025 03:01:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 62500 loss: -0.156477
03/25/2025 03:04:29 - INFO - __main__ -   step 62600 loss 0.1478
03/25/2025 03:04:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 62600 loss: -0.139842
03/25/2025 03:07:19 - INFO - __main__ -   step 62700 loss 0.13765
03/25/2025 03:07:21 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 62700 loss: -0.136285
03/25/2025 03:10:10 - INFO - __main__ -   step 62800 loss 0.14406
03/25/2025 03:10:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 62800 loss: -0.156366
03/25/2025 03:13:00 - INFO - __main__ -   step 62900 loss 0.14832
03/25/2025 03:13:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 62900 loss: -0.135545
03/25/2025 03:15:51 - INFO - __main__ -   step 63000 loss 0.14325
03/25/2025 03:15:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 63000 loss: -0.148365
03/25/2025 03:15:57 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 03:15:57 - INFO - __main__ -     Num queries = 1400
03/25/2025 03:15:57 - INFO - __main__ -     Num codes = 4360
03/25/2025 03:15:57 - INFO - __main__ -     Batch size = 32
03/25/2025 03:16:25 - INFO - __main__ -     eval_mrr = 0.78
03/25/2025 03:16:26 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-63000-0.78
03/25/2025 03:16:29 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 03:19:17 - INFO - __main__ -   step 63100 loss 0.11518
03/25/2025 03:19:19 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 63100 loss: -0.123408
03/25/2025 03:22:08 - INFO - __main__ -   step 63200 loss 0.14206
03/25/2025 03:22:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 63200 loss: -0.13615
03/25/2025 03:24:58 - INFO - __main__ -   step 63300 loss 0.13924
03/25/2025 03:25:00 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 63300 loss: -0.132307
03/25/2025 03:27:49 - INFO - __main__ -   step 63400 loss 0.11439
03/25/2025 03:27:51 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 63400 loss: -0.113358
03/25/2025 03:30:39 - INFO - __main__ -   step 63500 loss 0.09968
03/25/2025 03:30:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 63500 loss: -0.094253
03/25/2025 03:33:30 - INFO - __main__ -   step 63600 loss 0.12063
03/25/2025 03:33:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 63600 loss: -0.115703
03/25/2025 03:36:20 - INFO - __main__ -   step 63700 loss 0.15548
03/25/2025 03:36:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 63700 loss: -0.157744
03/25/2025 03:39:11 - INFO - __main__ -   step 63800 loss 0.11139
03/25/2025 03:39:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 63800 loss: -0.125306
03/25/2025 03:42:01 - INFO - __main__ -   step 63900 loss 0.11589
03/25/2025 03:42:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 63900 loss: -0.107961
03/25/2025 03:44:52 - INFO - __main__ -   step 64000 loss 0.14637
03/25/2025 03:44:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 64000 loss: -0.117209
03/25/2025 03:44:59 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 03:44:59 - INFO - __main__ -     Num queries = 1400
03/25/2025 03:44:59 - INFO - __main__ -     Num codes = 4360
03/25/2025 03:44:59 - INFO - __main__ -     Batch size = 32
03/25/2025 03:45:27 - INFO - __main__ -     eval_mrr = 0.781
03/25/2025 03:45:28 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-64000-0.781
03/25/2025 03:45:30 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 03:48:19 - INFO - __main__ -   step 64100 loss 0.11662
03/25/2025 03:48:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 64100 loss: -0.105267
03/25/2025 03:51:09 - INFO - __main__ -   step 64200 loss 0.11008
03/25/2025 03:51:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 64200 loss: -0.123241
03/25/2025 03:54:00 - INFO - __main__ -   step 64300 loss 0.11555
03/25/2025 03:54:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 64300 loss: -0.128811
03/25/2025 03:56:50 - INFO - __main__ -   step 64400 loss 0.11724
03/25/2025 03:56:52 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 64400 loss: -0.112884
03/25/2025 03:59:41 - INFO - __main__ -   step 64500 loss 0.12129
03/25/2025 03:59:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 64500 loss: -0.143347
03/25/2025 04:02:31 - INFO - __main__ -   step 64600 loss 0.13475
03/25/2025 04:02:33 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 64600 loss: -0.128677
03/25/2025 04:05:22 - INFO - __main__ -   step 64700 loss 0.12041
03/25/2025 04:05:24 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 64700 loss: -0.114952
03/25/2025 04:08:13 - INFO - __main__ -   step 64800 loss 0.10871
03/25/2025 04:08:14 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 64800 loss: -0.106697
03/25/2025 04:11:03 - INFO - __main__ -   step 64900 loss 0.1246
03/25/2025 04:11:05 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 64900 loss: -0.115425
03/25/2025 04:13:54 - INFO - __main__ -   step 65000 loss 0.12196
03/25/2025 04:13:55 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 65000 loss: -0.125746
03/25/2025 04:14:00 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 04:14:00 - INFO - __main__ -     Num queries = 1400
03/25/2025 04:14:00 - INFO - __main__ -     Num codes = 4360
03/25/2025 04:14:00 - INFO - __main__ -     Batch size = 32
03/25/2025 04:14:28 - INFO - __main__ -     eval_mrr = 0.778
03/25/2025 04:14:29 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-65000-0.778
03/25/2025 04:14:31 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 04:17:20 - INFO - __main__ -   step 65100 loss 0.1269
03/25/2025 04:17:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 65100 loss: -0.112811
03/25/2025 04:20:11 - INFO - __main__ -   step 65200 loss 0.12887
03/25/2025 04:20:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 65200 loss: -0.111349
03/25/2025 04:23:01 - INFO - __main__ -   step 65300 loss 0.11516
03/25/2025 04:23:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 65300 loss: -0.122656
03/25/2025 04:25:52 - INFO - __main__ -   step 65400 loss 0.12441
03/25/2025 04:25:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 65400 loss: -0.117273
03/25/2025 04:28:42 - INFO - __main__ -   step 65500 loss 0.12267
03/25/2025 04:28:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 65500 loss: -0.13237
03/25/2025 04:31:33 - INFO - __main__ -   step 65600 loss 0.11399
03/25/2025 04:31:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 65600 loss: -0.123595
03/25/2025 04:34:23 - INFO - __main__ -   step 65700 loss 0.12303
03/25/2025 04:34:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 65700 loss: -0.132637
03/25/2025 04:37:14 - INFO - __main__ -   step 65800 loss 0.12014
03/25/2025 04:37:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 65800 loss: -0.129282
03/25/2025 04:40:04 - INFO - __main__ -   step 65900 loss 0.11854
03/25/2025 04:40:06 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 65900 loss: -0.133652
03/25/2025 04:42:55 - INFO - __main__ -   step 66000 loss 0.12577
03/25/2025 04:42:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 66000 loss: -0.132205
03/25/2025 04:43:01 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 04:43:01 - INFO - __main__ -     Num queries = 1400
03/25/2025 04:43:01 - INFO - __main__ -     Num codes = 4360
03/25/2025 04:43:01 - INFO - __main__ -     Batch size = 32
03/25/2025 04:43:29 - INFO - __main__ -     eval_mrr = 0.777
03/25/2025 04:43:29 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-66000-0.777
03/25/2025 04:43:32 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 04:46:21 - INFO - __main__ -   step 66100 loss 0.13851
03/25/2025 04:46:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 66100 loss: -0.129158
03/25/2025 04:49:11 - INFO - __main__ -   step 66200 loss 0.10765
03/25/2025 04:49:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 66200 loss: -0.094777
03/25/2025 04:52:02 - INFO - __main__ -   step 66300 loss 0.12184
03/25/2025 04:52:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 66300 loss: -0.114273
03/25/2025 04:54:52 - INFO - __main__ -   step 66400 loss 0.13505
03/25/2025 04:54:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 66400 loss: -0.153987
03/25/2025 04:57:43 - INFO - __main__ -   step 66500 loss 0.11143
03/25/2025 04:57:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 66500 loss: -0.119134
03/25/2025 05:00:34 - INFO - __main__ -   step 66600 loss 0.12979
03/25/2025 05:00:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 66600 loss: -0.116234
03/25/2025 05:03:24 - INFO - __main__ -   step 66700 loss 0.09513
03/25/2025 05:03:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 66700 loss: -0.105672
03/25/2025 05:06:15 - INFO - __main__ -   step 66800 loss 0.10115
03/25/2025 05:06:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 66800 loss: -0.104507
03/25/2025 05:09:05 - INFO - __main__ -   step 66900 loss 0.10659
03/25/2025 05:09:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 66900 loss: -0.100798
03/25/2025 05:11:56 - INFO - __main__ -   step 67000 loss 0.11342
03/25/2025 05:11:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 67000 loss: -0.090093
03/25/2025 05:12:03 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 05:12:03 - INFO - __main__ -     Num queries = 1400
03/25/2025 05:12:03 - INFO - __main__ -     Num codes = 4360
03/25/2025 05:12:03 - INFO - __main__ -     Batch size = 32
03/25/2025 05:12:30 - INFO - __main__ -     eval_mrr = 0.776
03/25/2025 05:12:31 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-67000-0.776
03/25/2025 05:12:34 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 05:15:22 - INFO - __main__ -   step 67100 loss 0.12004
03/25/2025 05:15:24 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 67100 loss: -0.124009
03/25/2025 05:18:13 - INFO - __main__ -   step 67200 loss 0.09666
03/25/2025 05:18:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 67200 loss: -0.107445
03/25/2025 05:21:03 - INFO - __main__ -   step 67300 loss 0.0985
03/25/2025 05:21:05 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 67300 loss: -0.116134
03/25/2025 05:23:54 - INFO - __main__ -   step 67400 loss 0.11884
03/25/2025 05:23:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 67400 loss: -0.117982
03/25/2025 05:26:45 - INFO - __main__ -   step 67500 loss 0.09831
03/25/2025 05:26:46 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 67500 loss: -0.097175
03/25/2025 05:29:35 - INFO - __main__ -   step 67600 loss 0.11843
03/25/2025 05:29:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 67600 loss: -0.125643
03/25/2025 05:32:26 - INFO - __main__ -   step 67700 loss 0.12368
03/25/2025 05:32:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 67700 loss: -0.127048
03/25/2025 05:35:16 - INFO - __main__ -   step 67800 loss 0.12179
03/25/2025 05:35:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 67800 loss: -0.120405
03/25/2025 05:38:07 - INFO - __main__ -   step 67900 loss 0.12964
03/25/2025 05:38:08 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 67900 loss: -0.1306
03/25/2025 05:40:57 - INFO - __main__ -   step 68000 loss 0.10955
03/25/2025 05:40:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 68000 loss: -0.097087
03/25/2025 05:41:04 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 05:41:04 - INFO - __main__ -     Num queries = 1400
03/25/2025 05:41:04 - INFO - __main__ -     Num codes = 4360
03/25/2025 05:41:04 - INFO - __main__ -     Batch size = 32
03/25/2025 05:41:32 - INFO - __main__ -     eval_mrr = 0.778
03/25/2025 05:41:32 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-68000-0.778
03/25/2025 05:41:35 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 05:44:24 - INFO - __main__ -   step 68100 loss 0.13261
03/25/2025 05:44:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 68100 loss: -0.127959
03/25/2025 05:47:14 - INFO - __main__ -   step 68200 loss 0.1241
03/25/2025 05:47:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 68200 loss: -0.114637
03/25/2025 05:50:05 - INFO - __main__ -   step 68300 loss 0.12438
03/25/2025 05:50:06 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 68300 loss: -0.116276
03/25/2025 05:52:55 - INFO - __main__ -   step 68400 loss 0.11554
03/25/2025 05:52:57 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 68400 loss: -0.113351
03/25/2025 05:55:46 - INFO - __main__ -   step 68500 loss 0.12422
03/25/2025 05:55:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 68500 loss: -0.123476
03/25/2025 05:58:37 - INFO - __main__ -   step 68600 loss 0.12549
03/25/2025 05:58:38 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 68600 loss: -0.121556
03/25/2025 06:01:27 - INFO - __main__ -   step 68700 loss 0.12325
03/25/2025 06:01:29 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 68700 loss: -0.116018
03/25/2025 06:04:18 - INFO - __main__ -   step 68800 loss 0.10809
03/25/2025 06:04:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 68800 loss: -0.100971
03/25/2025 06:07:08 - INFO - __main__ -   step 68900 loss 0.1151
03/25/2025 06:07:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 68900 loss: -0.096612
03/25/2025 06:09:59 - INFO - __main__ -   step 69000 loss 0.12268
03/25/2025 06:10:01 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 69000 loss: -0.127912
03/25/2025 06:10:06 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 06:10:06 - INFO - __main__ -     Num queries = 1400
03/25/2025 06:10:06 - INFO - __main__ -     Num codes = 4360
03/25/2025 06:10:06 - INFO - __main__ -     Batch size = 32
03/25/2025 06:10:34 - INFO - __main__ -     eval_mrr = 0.776
03/25/2025 06:10:34 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-69000-0.776
03/25/2025 06:10:37 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 06:13:26 - INFO - __main__ -   step 69100 loss 0.11856
03/25/2025 06:13:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 69100 loss: -0.115001
03/25/2025 06:16:16 - INFO - __main__ -   step 69200 loss 0.12293
03/25/2025 06:16:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 69200 loss: -0.104145
03/25/2025 06:19:07 - INFO - __main__ -   step 69300 loss 0.11037
03/25/2025 06:19:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 69300 loss: -0.098102
03/25/2025 06:21:58 - INFO - __main__ -   step 69400 loss 0.09691
03/25/2025 06:21:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 69400 loss: -0.101
03/25/2025 06:24:48 - INFO - __main__ -   step 69500 loss 0.09631
03/25/2025 06:24:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 69500 loss: -0.112586
03/25/2025 06:27:39 - INFO - __main__ -   step 69600 loss 0.11107
03/25/2025 06:27:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 69600 loss: -0.11514
03/25/2025 06:30:29 - INFO - __main__ -   step 69700 loss 0.11957
03/25/2025 06:30:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 69700 loss: -0.10963
03/25/2025 06:33:20 - INFO - __main__ -   step 69800 loss 0.09775
03/25/2025 06:33:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 69800 loss: -0.101225
03/25/2025 06:36:11 - INFO - __main__ -   step 69900 loss 0.10187
03/25/2025 06:36:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 69900 loss: -0.102498
03/25/2025 06:39:01 - INFO - __main__ -   step 70000 loss 0.13543
03/25/2025 06:39:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 70000 loss: -0.138479
03/25/2025 06:39:15 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 06:39:15 - INFO - __main__ -     Num queries = 1400
03/25/2025 06:39:15 - INFO - __main__ -     Num codes = 4360
03/25/2025 06:39:15 - INFO - __main__ -     Batch size = 32
03/25/2025 06:39:43 - INFO - __main__ -     eval_mrr = 0.778
03/25/2025 06:39:43 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-70000-0.778
03/25/2025 06:39:46 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 06:42:35 - INFO - __main__ -   step 70100 loss 0.12356
03/25/2025 06:42:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 70100 loss: -0.11741
03/25/2025 06:45:25 - INFO - __main__ -   step 70200 loss 0.12577
03/25/2025 06:45:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 70200 loss: -0.130922
03/25/2025 06:48:16 - INFO - __main__ -   step 70300 loss 0.10878
03/25/2025 06:48:17 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 70300 loss: -0.107266
03/25/2025 06:51:06 - INFO - __main__ -   step 70400 loss 0.09834
03/25/2025 06:51:08 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 70400 loss: -0.092666
03/25/2025 06:53:57 - INFO - __main__ -   step 70500 loss 0.13443
03/25/2025 06:53:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 70500 loss: -0.144871
03/25/2025 06:56:48 - INFO - __main__ -   step 70600 loss 0.12482
03/25/2025 06:56:49 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 70600 loss: -0.125938
03/25/2025 06:59:38 - INFO - __main__ -   step 70700 loss 0.10439
03/25/2025 06:59:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 70700 loss: -0.093108
03/25/2025 07:02:29 - INFO - __main__ -   step 70800 loss 0.13575
03/25/2025 07:02:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 70800 loss: -0.136926
03/25/2025 07:05:20 - INFO - __main__ -   step 70900 loss 0.12663
03/25/2025 07:05:21 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 70900 loss: -0.134017
03/25/2025 07:08:10 - INFO - __main__ -   step 71000 loss 0.11004
03/25/2025 07:08:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 71000 loss: -0.092895
03/25/2025 07:08:17 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 07:08:17 - INFO - __main__ -     Num queries = 1400
03/25/2025 07:08:17 - INFO - __main__ -     Num codes = 4360
03/25/2025 07:08:17 - INFO - __main__ -     Batch size = 32
03/25/2025 07:08:45 - INFO - __main__ -     eval_mrr = 0.78
03/25/2025 07:08:46 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-71000-0.78
03/25/2025 07:08:48 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 07:11:37 - INFO - __main__ -   step 71100 loss 0.12409
03/25/2025 07:11:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 71100 loss: -0.121596
03/25/2025 07:14:28 - INFO - __main__ -   step 71200 loss 0.10315
03/25/2025 07:14:29 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 71200 loss: -0.10999
03/25/2025 07:17:18 - INFO - __main__ -   step 71300 loss 0.10212
03/25/2025 07:17:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 71300 loss: -0.090834
03/25/2025 07:20:09 - INFO - __main__ -   step 71400 loss 0.09568
03/25/2025 07:20:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 71400 loss: -0.0832
03/25/2025 07:22:59 - INFO - __main__ -   step 71500 loss 0.09907
03/25/2025 07:23:01 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 71500 loss: -0.091974
03/25/2025 07:25:50 - INFO - __main__ -   step 71600 loss 0.12727
03/25/2025 07:25:51 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 71600 loss: -0.112243
03/25/2025 07:28:40 - INFO - __main__ -   step 71700 loss 0.10745
03/25/2025 07:28:42 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 71700 loss: -0.127388
03/25/2025 07:31:31 - INFO - __main__ -   step 71800 loss 0.12359
03/25/2025 07:31:32 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 71800 loss: -0.126201
03/25/2025 07:34:21 - INFO - __main__ -   step 71900 loss 0.10181
03/25/2025 07:34:23 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 71900 loss: -0.085212
03/25/2025 07:37:12 - INFO - __main__ -   step 72000 loss 0.11749
03/25/2025 07:37:14 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 72000 loss: -0.098056
03/25/2025 07:37:18 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 07:37:18 - INFO - __main__ -     Num queries = 1400
03/25/2025 07:37:18 - INFO - __main__ -     Num codes = 4360
03/25/2025 07:37:18 - INFO - __main__ -     Batch size = 32
03/25/2025 07:37:46 - INFO - __main__ -     eval_mrr = 0.779
03/25/2025 07:37:47 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-72000-0.779
03/25/2025 07:37:49 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 07:40:38 - INFO - __main__ -   step 72100 loss 0.11598
03/25/2025 07:40:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 72100 loss: -0.132177
03/25/2025 07:43:29 - INFO - __main__ -   step 72200 loss 0.10326
03/25/2025 07:43:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 72200 loss: -0.098381
03/25/2025 07:46:19 - INFO - __main__ -   step 72300 loss 0.1433
03/25/2025 07:46:21 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 72300 loss: -0.121759
03/25/2025 07:49:10 - INFO - __main__ -   step 72400 loss 0.11284
03/25/2025 07:49:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 72400 loss: -0.100851
03/25/2025 07:52:00 - INFO - __main__ -   step 72500 loss 0.12966
03/25/2025 07:52:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 72500 loss: -0.109346
03/25/2025 07:54:51 - INFO - __main__ -   step 72600 loss 0.11484
03/25/2025 07:54:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 72600 loss: -0.108792
03/25/2025 07:57:42 - INFO - __main__ -   step 72700 loss 0.12106
03/25/2025 07:57:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 72700 loss: -0.097536
03/25/2025 08:00:32 - INFO - __main__ -   step 72800 loss 0.11733
03/25/2025 08:00:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 72800 loss: -0.127876
03/25/2025 08:03:23 - INFO - __main__ -   step 72900 loss 0.10355
03/25/2025 08:03:24 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 72900 loss: -0.094861
03/25/2025 08:06:13 - INFO - __main__ -   step 73000 loss 0.11686
03/25/2025 08:06:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 73000 loss: -0.13377
03/25/2025 08:06:20 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 08:06:20 - INFO - __main__ -     Num queries = 1400
03/25/2025 08:06:20 - INFO - __main__ -     Num codes = 4360
03/25/2025 08:06:20 - INFO - __main__ -     Batch size = 32
03/25/2025 08:06:48 - INFO - __main__ -     eval_mrr = 0.781
03/25/2025 08:06:48 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-73000-0.781
03/25/2025 08:06:51 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 08:09:40 - INFO - __main__ -   step 73100 loss 0.10946
03/25/2025 08:09:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 73100 loss: -0.110353
03/25/2025 08:12:30 - INFO - __main__ -   step 73200 loss 0.11599
03/25/2025 08:12:32 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 73200 loss: -0.116295
03/25/2025 08:15:21 - INFO - __main__ -   step 73300 loss 0.11649
03/25/2025 08:15:23 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 73300 loss: -0.125721
03/25/2025 08:18:12 - INFO - __main__ -   step 73400 loss 0.10732
03/25/2025 08:18:14 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 73400 loss: -0.135661
03/25/2025 08:21:02 - INFO - __main__ -   step 73500 loss 0.10685
03/25/2025 08:21:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 73500 loss: -0.108609
03/25/2025 08:23:53 - INFO - __main__ -   step 73600 loss 0.10762
03/25/2025 08:23:55 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 73600 loss: -0.107431
03/25/2025 08:26:44 - INFO - __main__ -   step 73700 loss 0.11929
03/25/2025 08:26:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 73700 loss: -0.12634
03/25/2025 08:29:34 - INFO - __main__ -   step 73800 loss 0.12036
03/25/2025 08:29:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 73800 loss: -0.125635
03/25/2025 08:32:25 - INFO - __main__ -   step 73900 loss 0.09443
03/25/2025 08:32:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 73900 loss: -0.09537
03/25/2025 08:35:16 - INFO - __main__ -   step 74000 loss 0.11151
03/25/2025 08:35:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 74000 loss: -0.124726
03/25/2025 08:35:22 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 08:35:22 - INFO - __main__ -     Num queries = 1400
03/25/2025 08:35:22 - INFO - __main__ -     Num codes = 4360
03/25/2025 08:35:22 - INFO - __main__ -     Batch size = 32
03/25/2025 08:35:50 - INFO - __main__ -     eval_mrr = 0.775
03/25/2025 08:35:51 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-74000-0.775
03/25/2025 08:35:54 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 08:38:42 - INFO - __main__ -   step 74100 loss 0.11271
03/25/2025 08:38:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 74100 loss: -0.13988
03/25/2025 08:41:33 - INFO - __main__ -   step 74200 loss 0.11924
03/25/2025 08:41:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 74200 loss: -0.147078
03/25/2025 08:44:24 - INFO - __main__ -   step 74300 loss 0.11177
03/25/2025 08:44:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 74300 loss: -0.097435
03/25/2025 08:47:14 - INFO - __main__ -   step 74400 loss 0.09879
03/25/2025 08:47:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 74400 loss: -0.098074
03/25/2025 08:50:05 - INFO - __main__ -   step 74500 loss 0.10859
03/25/2025 08:50:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 74500 loss: -0.099366
03/25/2025 08:52:56 - INFO - __main__ -   step 74600 loss 0.10167
03/25/2025 08:52:57 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 74600 loss: -0.098928
03/25/2025 08:55:46 - INFO - __main__ -   step 74700 loss 0.11446
03/25/2025 08:55:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 74700 loss: -0.116655
03/25/2025 08:58:37 - INFO - __main__ -   step 74800 loss 0.12128
03/25/2025 08:58:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 74800 loss: -0.12946
03/25/2025 09:01:28 - INFO - __main__ -   step 74900 loss 0.10111
03/25/2025 09:01:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 74900 loss: -0.107741
03/25/2025 09:04:18 - INFO - __main__ -   step 75000 loss 0.11161
03/25/2025 09:04:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 75000 loss: -0.115882
03/25/2025 09:04:25 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 09:04:25 - INFO - __main__ -     Num queries = 1400
03/25/2025 09:04:25 - INFO - __main__ -     Num codes = 4360
03/25/2025 09:04:25 - INFO - __main__ -     Batch size = 32
03/25/2025 09:04:53 - INFO - __main__ -     eval_mrr = 0.775
03/25/2025 09:04:54 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-75000-0.775
03/25/2025 09:04:56 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 09:07:45 - INFO - __main__ -   step 75100 loss 0.10517
03/25/2025 09:07:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 75100 loss: -0.115191
03/25/2025 09:10:36 - INFO - __main__ -   step 75200 loss 0.11758
03/25/2025 09:10:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 75200 loss: -0.116593
03/25/2025 09:13:26 - INFO - __main__ -   step 75300 loss 0.10177
03/25/2025 09:13:28 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 75300 loss: -0.104563
03/25/2025 09:16:17 - INFO - __main__ -   step 75400 loss 0.11079
03/25/2025 09:16:19 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 75400 loss: -0.105312
03/25/2025 09:19:08 - INFO - __main__ -   step 75500 loss 0.11265
03/25/2025 09:19:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 75500 loss: -0.10292
03/25/2025 09:21:58 - INFO - __main__ -   step 75600 loss 0.08504
03/25/2025 09:22:00 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 75600 loss: -0.092375
03/25/2025 09:24:49 - INFO - __main__ -   step 75700 loss 0.10395
03/25/2025 09:24:51 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 75700 loss: -0.100756
03/25/2025 09:27:40 - INFO - __main__ -   step 75800 loss 0.09756
03/25/2025 09:27:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 75800 loss: -0.097515
03/25/2025 09:30:30 - INFO - __main__ -   step 75900 loss 0.10749
03/25/2025 09:30:32 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 75900 loss: -0.082847
03/25/2025 09:33:21 - INFO - __main__ -   step 76000 loss 0.11033
03/25/2025 09:33:23 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 76000 loss: -0.116299
03/25/2025 09:33:27 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 09:33:27 - INFO - __main__ -     Num queries = 1400
03/25/2025 09:33:27 - INFO - __main__ -     Num codes = 4360
03/25/2025 09:33:27 - INFO - __main__ -     Batch size = 32
03/25/2025 09:33:55 - INFO - __main__ -     eval_mrr = 0.775
03/25/2025 09:33:56 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-76000-0.775
03/25/2025 09:33:59 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 09:36:47 - INFO - __main__ -   step 76100 loss 0.11355
03/25/2025 09:36:49 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 76100 loss: -0.114561
03/25/2025 09:39:38 - INFO - __main__ -   step 76200 loss 0.12477
03/25/2025 09:39:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 76200 loss: -0.138638
03/25/2025 09:42:29 - INFO - __main__ -   step 76300 loss 0.12601
03/25/2025 09:42:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 76300 loss: -0.110978
03/25/2025 09:45:19 - INFO - __main__ -   step 76400 loss 0.10497
03/25/2025 09:45:21 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 76400 loss: -0.094376
03/25/2025 09:48:10 - INFO - __main__ -   step 76500 loss 0.13269
03/25/2025 09:48:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 76500 loss: -0.119025
03/25/2025 09:51:00 - INFO - __main__ -   step 76600 loss 0.11926
03/25/2025 09:51:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 76600 loss: -0.120813
03/25/2025 09:53:51 - INFO - __main__ -   step 76700 loss 0.12396
03/25/2025 09:53:52 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 76700 loss: -0.14824
03/25/2025 09:56:41 - INFO - __main__ -   step 76800 loss 0.11395
03/25/2025 09:56:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 76800 loss: -0.11946
03/25/2025 09:59:32 - INFO - __main__ -   step 76900 loss 0.12768
03/25/2025 09:59:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 76900 loss: -0.137998
03/25/2025 10:02:23 - INFO - __main__ -   step 77000 loss 0.09451
03/25/2025 10:02:24 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 77000 loss: -0.099356
03/25/2025 10:02:29 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 10:02:29 - INFO - __main__ -     Num queries = 1400
03/25/2025 10:02:29 - INFO - __main__ -     Num codes = 4360
03/25/2025 10:02:29 - INFO - __main__ -     Batch size = 32
03/25/2025 10:02:57 - INFO - __main__ -     eval_mrr = 0.782
03/25/2025 10:02:58 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-77000-0.782
03/25/2025 10:03:00 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 10:05:49 - INFO - __main__ -   step 77100 loss 0.10453
03/25/2025 10:05:51 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 77100 loss: -0.099966
03/25/2025 10:08:40 - INFO - __main__ -   step 77200 loss 0.09591
03/25/2025 10:08:42 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 77200 loss: -0.100398
03/25/2025 10:11:31 - INFO - __main__ -   step 77300 loss 0.12036
03/25/2025 10:11:32 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 77300 loss: -0.115877
03/25/2025 10:14:21 - INFO - __main__ -   step 77400 loss 0.11614
03/25/2025 10:14:23 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 77400 loss: -0.120345
03/25/2025 10:17:12 - INFO - __main__ -   step 77500 loss 0.12042
03/25/2025 10:17:14 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 77500 loss: -0.118146
03/25/2025 10:20:02 - INFO - __main__ -   step 77600 loss 0.10873
03/25/2025 10:20:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 77600 loss: -0.105157
03/25/2025 10:22:53 - INFO - __main__ -   step 77700 loss 0.11547
03/25/2025 10:22:55 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 77700 loss: -0.118353
03/25/2025 10:25:44 - INFO - __main__ -   step 77800 loss 0.10868
03/25/2025 10:25:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 77800 loss: -0.097718
03/25/2025 10:28:34 - INFO - __main__ -   step 77900 loss 0.11301
03/25/2025 10:28:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 77900 loss: -0.109154
03/25/2025 10:31:25 - INFO - __main__ -   step 78000 loss 0.10316
03/25/2025 10:31:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 78000 loss: -0.086306
03/25/2025 10:31:32 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 10:31:32 - INFO - __main__ -     Num queries = 1400
03/25/2025 10:31:32 - INFO - __main__ -     Num codes = 4360
03/25/2025 10:31:32 - INFO - __main__ -     Batch size = 32
03/25/2025 10:31:59 - INFO - __main__ -     eval_mrr = 0.774
03/25/2025 10:32:00 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-78000-0.774
03/25/2025 10:32:03 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 10:34:51 - INFO - __main__ -   step 78100 loss 0.09233
03/25/2025 10:34:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 78100 loss: -0.085033
03/25/2025 10:37:42 - INFO - __main__ -   step 78200 loss 0.09897
03/25/2025 10:37:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 78200 loss: -0.099132
03/25/2025 10:40:33 - INFO - __main__ -   step 78300 loss 0.10221
03/25/2025 10:40:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 78300 loss: -0.103108
03/25/2025 10:43:24 - INFO - __main__ -   step 78400 loss 0.11009
03/25/2025 10:43:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 78400 loss: -0.112998
03/25/2025 10:46:14 - INFO - __main__ -   step 78500 loss 0.10756
03/25/2025 10:46:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 78500 loss: -0.116787
03/25/2025 10:49:05 - INFO - __main__ -   step 78600 loss 0.09757
03/25/2025 10:49:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 78600 loss: -0.104308
03/25/2025 10:51:56 - INFO - __main__ -   step 78700 loss 0.08545
03/25/2025 10:51:57 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 78700 loss: -0.10108
03/25/2025 10:54:46 - INFO - __main__ -   step 78800 loss 0.09891
03/25/2025 10:54:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 78800 loss: -0.087227
03/25/2025 10:57:37 - INFO - __main__ -   step 78900 loss 0.10404
03/25/2025 10:57:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 78900 loss: -0.111185
03/25/2025 11:00:28 - INFO - __main__ -   step 79000 loss 0.11279
03/25/2025 11:00:29 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 79000 loss: -0.108721
03/25/2025 11:00:34 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 11:00:34 - INFO - __main__ -     Num queries = 1400
03/25/2025 11:00:34 - INFO - __main__ -     Num codes = 4360
03/25/2025 11:00:34 - INFO - __main__ -     Batch size = 32
03/25/2025 11:01:02 - INFO - __main__ -     eval_mrr = 0.775
03/25/2025 11:01:03 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-79000-0.775
03/25/2025 11:01:05 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 11:03:54 - INFO - __main__ -   step 79100 loss 0.12164
03/25/2025 11:03:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 79100 loss: -0.124904
03/25/2025 11:06:45 - INFO - __main__ -   step 79200 loss 0.12641
03/25/2025 11:06:46 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 79200 loss: -0.136723
03/25/2025 11:09:35 - INFO - __main__ -   step 79300 loss 0.11375
03/25/2025 11:09:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 79300 loss: -0.112652
03/25/2025 11:12:26 - INFO - __main__ -   step 79400 loss 0.09312
03/25/2025 11:12:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 79400 loss: -0.105527
03/25/2025 11:15:16 - INFO - __main__ -   step 79500 loss 0.11335
03/25/2025 11:15:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 79500 loss: -0.100883
03/25/2025 11:18:07 - INFO - __main__ -   step 79600 loss 0.09672
03/25/2025 11:18:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 79600 loss: -0.104636
03/25/2025 11:20:58 - INFO - __main__ -   step 79700 loss 0.11081
03/25/2025 11:20:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 79700 loss: -0.117626
03/25/2025 11:23:48 - INFO - __main__ -   step 79800 loss 0.1093
03/25/2025 11:23:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 79800 loss: -0.12322
03/25/2025 11:26:39 - INFO - __main__ -   step 79900 loss 0.09221
03/25/2025 11:26:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 79900 loss: -0.10882
03/25/2025 11:29:30 - INFO - __main__ -   step 80000 loss 0.0952
03/25/2025 11:29:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 80000 loss: -0.10862
03/25/2025 11:29:36 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 11:29:36 - INFO - __main__ -     Num queries = 1400
03/25/2025 11:29:36 - INFO - __main__ -     Num codes = 4360
03/25/2025 11:29:36 - INFO - __main__ -     Batch size = 32
03/25/2025 11:30:04 - INFO - __main__ -     eval_mrr = 0.777
03/25/2025 11:30:05 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-80000-0.777
03/25/2025 11:30:07 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 11:32:56 - INFO - __main__ -   step 80100 loss 0.09504
03/25/2025 11:32:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 80100 loss: -0.083546
03/25/2025 11:35:47 - INFO - __main__ -   step 80200 loss 0.10367
03/25/2025 11:35:49 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 80200 loss: -0.098195
03/25/2025 11:38:38 - INFO - __main__ -   step 80300 loss 0.1075
03/25/2025 11:38:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 80300 loss: -0.11021
03/25/2025 11:41:29 - INFO - __main__ -   step 80400 loss 0.10858
03/25/2025 11:41:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 80400 loss: -0.09781
03/25/2025 11:44:20 - INFO - __main__ -   step 80500 loss 0.10338
03/25/2025 11:44:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 80500 loss: -0.097282
03/25/2025 11:47:11 - INFO - __main__ -   step 80600 loss 0.11003
03/25/2025 11:47:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 80600 loss: -0.113669
03/25/2025 11:50:02 - INFO - __main__ -   step 80700 loss 0.0969
03/25/2025 11:50:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 80700 loss: -0.097459
03/25/2025 11:52:53 - INFO - __main__ -   step 80800 loss 0.10336
03/25/2025 11:52:55 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 80800 loss: -0.097249
03/25/2025 11:55:44 - INFO - __main__ -   step 80900 loss 0.07786
03/25/2025 11:55:46 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 80900 loss: -0.075743
03/25/2025 11:58:35 - INFO - __main__ -   step 81000 loss 0.09712
03/25/2025 11:58:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 81000 loss: -0.109057
03/25/2025 11:58:42 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 11:58:42 - INFO - __main__ -     Num queries = 1400
03/25/2025 11:58:42 - INFO - __main__ -     Num codes = 4360
03/25/2025 11:58:42 - INFO - __main__ -     Batch size = 32
03/25/2025 11:59:10 - INFO - __main__ -     eval_mrr = 0.775
03/25/2025 11:59:10 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-81000-0.775
03/25/2025 11:59:14 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 12:02:03 - INFO - __main__ -   step 81100 loss 0.09997
03/25/2025 12:02:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 81100 loss: -0.111202
03/25/2025 12:04:53 - INFO - __main__ -   step 81200 loss 0.10227
03/25/2025 12:04:55 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 81200 loss: -0.095266
03/25/2025 12:07:44 - INFO - __main__ -   step 81300 loss 0.08524
03/25/2025 12:07:46 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 81300 loss: -0.092958
03/25/2025 12:10:35 - INFO - __main__ -   step 81400 loss 0.08997
03/25/2025 12:10:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 81400 loss: -0.079256
03/25/2025 12:13:25 - INFO - __main__ -   step 81500 loss 0.07958
03/25/2025 12:13:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 81500 loss: -0.069543
03/25/2025 12:16:16 - INFO - __main__ -   step 81600 loss 0.12349
03/25/2025 12:16:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 81600 loss: -0.128333
03/25/2025 12:19:07 - INFO - __main__ -   step 81700 loss 0.11055
03/25/2025 12:19:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 81700 loss: -0.106309
03/25/2025 12:21:58 - INFO - __main__ -   step 81800 loss 0.08528
03/25/2025 12:21:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 81800 loss: -0.100529
03/25/2025 12:24:48 - INFO - __main__ -   step 81900 loss 0.09241
03/25/2025 12:24:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 81900 loss: -0.082441
03/25/2025 12:27:39 - INFO - __main__ -   step 82000 loss 0.09521
03/25/2025 12:27:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 82000 loss: -0.100126
03/25/2025 12:27:46 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 12:27:46 - INFO - __main__ -     Num queries = 1400
03/25/2025 12:27:46 - INFO - __main__ -     Num codes = 4360
03/25/2025 12:27:46 - INFO - __main__ -     Batch size = 32
03/25/2025 12:28:14 - INFO - __main__ -     eval_mrr = 0.774
03/25/2025 12:28:15 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-82000-0.774
03/25/2025 12:28:18 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 12:31:07 - INFO - __main__ -   step 82100 loss 0.09591
03/25/2025 12:31:08 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 82100 loss: -0.083154
03/25/2025 12:33:57 - INFO - __main__ -   step 82200 loss 0.09239
03/25/2025 12:33:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 82200 loss: -0.099305
03/25/2025 12:36:48 - INFO - __main__ -   step 82300 loss 0.08736
03/25/2025 12:36:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 82300 loss: -0.101128
03/25/2025 12:39:38 - INFO - __main__ -   step 82400 loss 0.0917
03/25/2025 12:39:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 82400 loss: -0.086322
03/25/2025 12:42:29 - INFO - __main__ -   step 82500 loss 0.09947
03/25/2025 12:42:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 82500 loss: -0.108014
03/25/2025 12:45:20 - INFO - __main__ -   step 82600 loss 0.09788
03/25/2025 12:45:21 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 82600 loss: -0.098362
03/25/2025 12:48:10 - INFO - __main__ -   step 82700 loss 0.1048
03/25/2025 12:48:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 82700 loss: -0.109144
03/25/2025 12:51:01 - INFO - __main__ -   step 82800 loss 0.10385
03/25/2025 12:51:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 82800 loss: -0.092132
03/25/2025 12:53:52 - INFO - __main__ -   step 82900 loss 0.0852
03/25/2025 12:53:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 82900 loss: -0.090372
03/25/2025 12:56:42 - INFO - __main__ -   step 83000 loss 0.09331
03/25/2025 12:56:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 83000 loss: -0.094134
03/25/2025 12:56:49 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 12:56:49 - INFO - __main__ -     Num queries = 1400
03/25/2025 12:56:49 - INFO - __main__ -     Num codes = 4360
03/25/2025 12:56:49 - INFO - __main__ -     Batch size = 32
03/25/2025 12:57:17 - INFO - __main__ -     eval_mrr = 0.775
03/25/2025 12:57:18 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-83000-0.775
03/25/2025 12:57:20 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 13:00:09 - INFO - __main__ -   step 83100 loss 0.08463
03/25/2025 13:00:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 83100 loss: -0.091354
03/25/2025 13:03:00 - INFO - __main__ -   step 83200 loss 0.10284
03/25/2025 13:03:01 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 83200 loss: -0.108355
03/25/2025 13:05:50 - INFO - __main__ -   step 83300 loss 0.09777
03/25/2025 13:05:52 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 83300 loss: -0.08453
03/25/2025 13:08:41 - INFO - __main__ -   step 83400 loss 0.10588
03/25/2025 13:08:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 83400 loss: -0.104206
03/25/2025 13:11:32 - INFO - __main__ -   step 83500 loss 0.08288
03/25/2025 13:11:33 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 83500 loss: -0.076887
03/25/2025 13:14:22 - INFO - __main__ -   step 83600 loss 0.09592
03/25/2025 13:14:24 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 83600 loss: -0.082791
03/25/2025 13:17:13 - INFO - __main__ -   step 83700 loss 0.10682
03/25/2025 13:17:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 83700 loss: -0.110451
03/25/2025 13:20:04 - INFO - __main__ -   step 83800 loss 0.09028
03/25/2025 13:20:05 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 83800 loss: -0.081666
03/25/2025 13:22:54 - INFO - __main__ -   step 83900 loss 0.08611
03/25/2025 13:22:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 83900 loss: -0.089195
03/25/2025 13:25:45 - INFO - __main__ -   step 84000 loss 0.10292
03/25/2025 13:25:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 84000 loss: -0.107218
03/25/2025 13:25:52 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 13:25:52 - INFO - __main__ -     Num queries = 1400
03/25/2025 13:25:52 - INFO - __main__ -     Num codes = 4360
03/25/2025 13:25:52 - INFO - __main__ -     Batch size = 32
03/25/2025 13:26:20 - INFO - __main__ -     eval_mrr = 0.778
03/25/2025 13:26:21 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-84000-0.778
03/25/2025 13:26:23 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 13:29:12 - INFO - __main__ -   step 84100 loss 0.08893
03/25/2025 13:29:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 84100 loss: -0.095794
03/25/2025 13:32:02 - INFO - __main__ -   step 84200 loss 0.08933
03/25/2025 13:32:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 84200 loss: -0.090056
03/25/2025 13:34:53 - INFO - __main__ -   step 84300 loss 0.09558
03/25/2025 13:34:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 84300 loss: -0.088543
03/25/2025 13:37:43 - INFO - __main__ -   step 84400 loss 0.0994
03/25/2025 13:37:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 84400 loss: -0.106529
03/25/2025 13:40:34 - INFO - __main__ -   step 84500 loss 0.09151
03/25/2025 13:40:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 84500 loss: -0.092561
03/25/2025 13:43:24 - INFO - __main__ -   step 84600 loss 0.09952
03/25/2025 13:43:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 84600 loss: -0.104325
03/25/2025 13:46:15 - INFO - __main__ -   step 84700 loss 0.0844
03/25/2025 13:46:17 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 84700 loss: -0.087234
03/25/2025 13:49:05 - INFO - __main__ -   step 84800 loss 0.09477
03/25/2025 13:49:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 84800 loss: -0.091962
03/25/2025 13:51:56 - INFO - __main__ -   step 84900 loss 0.09492
03/25/2025 13:51:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 84900 loss: -0.096729
03/25/2025 13:54:46 - INFO - __main__ -   step 85000 loss 0.09385
03/25/2025 13:54:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 85000 loss: -0.09662
03/25/2025 13:54:53 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 13:54:53 - INFO - __main__ -     Num queries = 1400
03/25/2025 13:54:53 - INFO - __main__ -     Num codes = 4360
03/25/2025 13:54:53 - INFO - __main__ -     Batch size = 32
03/25/2025 13:55:21 - INFO - __main__ -     eval_mrr = 0.774
03/25/2025 13:55:22 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-85000-0.774
03/25/2025 13:55:24 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 13:58:13 - INFO - __main__ -   step 85100 loss 0.089
03/25/2025 13:58:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 85100 loss: -0.094343
03/25/2025 14:01:04 - INFO - __main__ -   step 85200 loss 0.09368
03/25/2025 14:01:05 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 85200 loss: -0.084492
03/25/2025 14:03:54 - INFO - __main__ -   step 85300 loss 0.09241
03/25/2025 14:03:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 85300 loss: -0.093378
03/25/2025 14:06:45 - INFO - __main__ -   step 85400 loss 0.09163
03/25/2025 14:06:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 85400 loss: -0.089079
03/25/2025 14:09:35 - INFO - __main__ -   step 85500 loss 0.08691
03/25/2025 14:09:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 85500 loss: -0.068594
03/25/2025 14:12:26 - INFO - __main__ -   step 85600 loss 0.08285
03/25/2025 14:12:28 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 85600 loss: -0.078355
03/25/2025 14:15:16 - INFO - __main__ -   step 85700 loss 0.09797
03/25/2025 14:15:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 85700 loss: -0.081915
03/25/2025 14:18:07 - INFO - __main__ -   step 85800 loss 0.10351
03/25/2025 14:18:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 85800 loss: -0.099401
03/25/2025 14:20:58 - INFO - __main__ -   step 85900 loss 0.1106
03/25/2025 14:20:59 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 85900 loss: -0.109547
03/25/2025 14:23:48 - INFO - __main__ -   step 86000 loss 0.08802
03/25/2025 14:23:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 86000 loss: -0.093958
03/25/2025 14:23:55 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 14:23:55 - INFO - __main__ -     Num queries = 1400
03/25/2025 14:23:55 - INFO - __main__ -     Num codes = 4360
03/25/2025 14:23:55 - INFO - __main__ -     Batch size = 32
03/25/2025 14:24:23 - INFO - __main__ -     eval_mrr = 0.777
03/25/2025 14:24:36 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-86000-0.777
03/25/2025 14:24:38 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 14:27:27 - INFO - __main__ -   step 86100 loss 0.08006
03/25/2025 14:27:29 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 86100 loss: -0.085011
03/25/2025 14:30:18 - INFO - __main__ -   step 86200 loss 0.1014
03/25/2025 14:30:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 86200 loss: -0.103096
03/25/2025 14:33:09 - INFO - __main__ -   step 86300 loss 0.09199
03/25/2025 14:33:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 86300 loss: -0.106986
03/25/2025 14:35:59 - INFO - __main__ -   step 86400 loss 0.09292
03/25/2025 14:36:01 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 86400 loss: -0.099638
03/25/2025 14:38:50 - INFO - __main__ -   step 86500 loss 0.10466
03/25/2025 14:38:52 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 86500 loss: -0.129255
03/25/2025 14:41:41 - INFO - __main__ -   step 86600 loss 0.09991
03/25/2025 14:41:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 86600 loss: -0.110227
03/25/2025 14:44:31 - INFO - __main__ -   step 86700 loss 0.11003
03/25/2025 14:44:33 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 86700 loss: -0.101055
03/25/2025 14:47:22 - INFO - __main__ -   step 86800 loss 0.08953
03/25/2025 14:47:24 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 86800 loss: -0.100111
03/25/2025 14:50:13 - INFO - __main__ -   step 86900 loss 0.11006
03/25/2025 14:50:14 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 86900 loss: -0.127956
03/25/2025 14:53:03 - INFO - __main__ -   step 87000 loss 0.0849
03/25/2025 14:53:05 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 87000 loss: -0.090105
03/25/2025 14:53:10 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 14:53:10 - INFO - __main__ -     Num queries = 1400
03/25/2025 14:53:10 - INFO - __main__ -     Num codes = 4360
03/25/2025 14:53:10 - INFO - __main__ -     Batch size = 32
03/25/2025 14:53:37 - INFO - __main__ -     eval_mrr = 0.778
03/25/2025 14:53:38 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-87000-0.778
03/25/2025 14:53:41 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 14:56:30 - INFO - __main__ -   step 87100 loss 0.08779
03/25/2025 14:56:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 87100 loss: -0.085596
03/25/2025 14:59:20 - INFO - __main__ -   step 87200 loss 0.10003
03/25/2025 14:59:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 87200 loss: -0.096903
03/25/2025 15:02:11 - INFO - __main__ -   step 87300 loss 0.08983
03/25/2025 15:02:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 87300 loss: -0.093364
03/25/2025 15:05:02 - INFO - __main__ -   step 87400 loss 0.09285
03/25/2025 15:05:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 87400 loss: -0.071765
03/25/2025 15:07:52 - INFO - __main__ -   step 87500 loss 0.09824
03/25/2025 15:07:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 87500 loss: -0.088159
03/25/2025 15:10:43 - INFO - __main__ -   step 87600 loss 0.09514
03/25/2025 15:10:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 87600 loss: -0.094119
03/25/2025 15:13:34 - INFO - __main__ -   step 87700 loss 0.09483
03/25/2025 15:13:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 87700 loss: -0.080068
03/25/2025 15:16:24 - INFO - __main__ -   step 87800 loss 0.10428
03/25/2025 15:16:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 87800 loss: -0.112072
03/25/2025 15:19:15 - INFO - __main__ -   step 87900 loss 0.09121
03/25/2025 15:19:17 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 87900 loss: -0.095342
03/25/2025 15:22:06 - INFO - __main__ -   step 88000 loss 0.08073
03/25/2025 15:22:08 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 88000 loss: -0.087706
03/25/2025 15:22:12 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 15:22:12 - INFO - __main__ -     Num queries = 1400
03/25/2025 15:22:12 - INFO - __main__ -     Num codes = 4360
03/25/2025 15:22:12 - INFO - __main__ -     Batch size = 32
03/25/2025 15:22:40 - INFO - __main__ -     eval_mrr = 0.776
03/25/2025 15:22:41 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-88000-0.776
03/25/2025 15:22:43 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 15:25:32 - INFO - __main__ -   step 88100 loss 0.08469
03/25/2025 15:25:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 88100 loss: -0.08022
03/25/2025 15:28:23 - INFO - __main__ -   step 88200 loss 0.12527
03/25/2025 15:28:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 88200 loss: -0.123945
03/25/2025 15:31:14 - INFO - __main__ -   step 88300 loss 0.1041
03/25/2025 15:31:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 88300 loss: -0.097973
03/25/2025 15:34:04 - INFO - __main__ -   step 88400 loss 0.08528
03/25/2025 15:34:06 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 88400 loss: -0.099843
03/25/2025 15:36:55 - INFO - __main__ -   step 88500 loss 0.09417
03/25/2025 15:36:57 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 88500 loss: -0.091126
03/25/2025 15:39:46 - INFO - __main__ -   step 88600 loss 0.09133
03/25/2025 15:39:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 88600 loss: -0.097613
03/25/2025 15:42:36 - INFO - __main__ -   step 88700 loss 0.09618
03/25/2025 15:42:38 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 88700 loss: -0.089896
03/25/2025 15:45:27 - INFO - __main__ -   step 88800 loss 0.07912
03/25/2025 15:45:29 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 88800 loss: -0.086106
03/25/2025 15:48:18 - INFO - __main__ -   step 88900 loss 0.10483
03/25/2025 15:48:19 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 88900 loss: -0.110053
03/25/2025 15:51:08 - INFO - __main__ -   step 89000 loss 0.08485
03/25/2025 15:51:10 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 89000 loss: -0.089483
03/25/2025 15:51:15 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 15:51:15 - INFO - __main__ -     Num queries = 1400
03/25/2025 15:51:15 - INFO - __main__ -     Num codes = 4360
03/25/2025 15:51:15 - INFO - __main__ -     Batch size = 32
03/25/2025 15:51:43 - INFO - __main__ -     eval_mrr = 0.779
03/25/2025 15:51:44 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-89000-0.779
03/25/2025 15:51:46 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 15:54:35 - INFO - __main__ -   step 89100 loss 0.08401
03/25/2025 15:54:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 89100 loss: -0.078259
03/25/2025 15:57:26 - INFO - __main__ -   step 89200 loss 0.08243
03/25/2025 15:57:28 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 89200 loss: -0.077773
03/25/2025 16:00:17 - INFO - __main__ -   step 89300 loss 0.1012
03/25/2025 16:00:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 89300 loss: -0.107114
03/25/2025 16:03:07 - INFO - __main__ -   step 89400 loss 0.09293
03/25/2025 16:03:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 89400 loss: -0.104547
03/25/2025 16:05:58 - INFO - __main__ -   step 89500 loss 0.09378
03/25/2025 16:06:00 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 89500 loss: -0.088577
03/25/2025 16:08:49 - INFO - __main__ -   step 89600 loss 0.1042
03/25/2025 16:08:51 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 89600 loss: -0.108179
03/25/2025 16:11:40 - INFO - __main__ -   step 89700 loss 0.0832
03/25/2025 16:11:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 89700 loss: -0.089812
03/25/2025 16:14:30 - INFO - __main__ -   step 89800 loss 0.09331
03/25/2025 16:14:32 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 89800 loss: -0.096481
03/25/2025 16:17:21 - INFO - __main__ -   step 89900 loss 0.0923
03/25/2025 16:17:23 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 89900 loss: -0.079406
03/25/2025 16:20:12 - INFO - __main__ -   step 90000 loss 0.09574
03/25/2025 16:20:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 90000 loss: -0.099468
03/25/2025 16:20:18 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 16:20:18 - INFO - __main__ -     Num queries = 1400
03/25/2025 16:20:18 - INFO - __main__ -     Num codes = 4360
03/25/2025 16:20:18 - INFO - __main__ -     Batch size = 32
03/25/2025 16:20:46 - INFO - __main__ -     eval_mrr = 0.775
03/25/2025 16:20:47 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-90000-0.775
03/25/2025 16:20:50 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 16:23:38 - INFO - __main__ -   step 90100 loss 0.08915
03/25/2025 16:23:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 90100 loss: -0.081164
03/25/2025 16:26:29 - INFO - __main__ -   step 90200 loss 0.10662
03/25/2025 16:26:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 90200 loss: -0.123549
03/25/2025 16:29:20 - INFO - __main__ -   step 90300 loss 0.09513
03/25/2025 16:29:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 90300 loss: -0.083324
03/25/2025 16:32:11 - INFO - __main__ -   step 90400 loss 0.09511
03/25/2025 16:32:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 90400 loss: -0.10576
03/25/2025 16:35:01 - INFO - __main__ -   step 90500 loss 0.09709
03/25/2025 16:35:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 90500 loss: -0.100133
03/25/2025 16:37:52 - INFO - __main__ -   step 90600 loss 0.08854
03/25/2025 16:37:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 90600 loss: -0.098101
03/25/2025 16:40:43 - INFO - __main__ -   step 90700 loss 0.10457
03/25/2025 16:40:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 90700 loss: -0.116022
03/25/2025 16:43:33 - INFO - __main__ -   step 90800 loss 0.11855
03/25/2025 16:43:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 90800 loss: -0.110826
03/25/2025 16:46:24 - INFO - __main__ -   step 90900 loss 0.09923
03/25/2025 16:46:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 90900 loss: -0.101599
03/25/2025 16:49:15 - INFO - __main__ -   step 91000 loss 0.0855
03/25/2025 16:49:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 91000 loss: -0.082453
03/25/2025 16:49:21 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 16:49:21 - INFO - __main__ -     Num queries = 1400
03/25/2025 16:49:21 - INFO - __main__ -     Num codes = 4360
03/25/2025 16:49:21 - INFO - __main__ -     Batch size = 32
03/25/2025 16:49:49 - INFO - __main__ -     eval_mrr = 0.775
03/25/2025 16:49:50 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-91000-0.775
03/25/2025 16:49:53 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 16:52:41 - INFO - __main__ -   step 91100 loss 0.10518
03/25/2025 16:52:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 91100 loss: -0.107486
03/25/2025 16:55:32 - INFO - __main__ -   step 91200 loss 0.07656
03/25/2025 16:55:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 91200 loss: -0.071212
03/25/2025 16:58:23 - INFO - __main__ -   step 91300 loss 0.11447
03/25/2025 16:58:24 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 91300 loss: -0.117128
03/25/2025 17:01:13 - INFO - __main__ -   step 91400 loss 0.10371
03/25/2025 17:01:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 91400 loss: -0.098883
03/25/2025 17:04:04 - INFO - __main__ -   step 91500 loss 0.09282
03/25/2025 17:04:06 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 91500 loss: -0.075402
03/25/2025 17:06:55 - INFO - __main__ -   step 91600 loss 0.08927
03/25/2025 17:06:57 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 91600 loss: -0.087444
03/25/2025 17:09:45 - INFO - __main__ -   step 91700 loss 0.10536
03/25/2025 17:09:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 91700 loss: -0.115888
03/25/2025 17:12:36 - INFO - __main__ -   step 91800 loss 0.09323
03/25/2025 17:12:38 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 91800 loss: -0.104066
03/25/2025 17:15:27 - INFO - __main__ -   step 91900 loss 0.09658
03/25/2025 17:15:28 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 91900 loss: -0.10037
03/25/2025 17:18:17 - INFO - __main__ -   step 92000 loss 0.10986
03/25/2025 17:18:19 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 92000 loss: -0.112912
03/25/2025 17:18:24 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 17:18:24 - INFO - __main__ -     Num queries = 1400
03/25/2025 17:18:24 - INFO - __main__ -     Num codes = 4360
03/25/2025 17:18:24 - INFO - __main__ -     Batch size = 32
03/25/2025 17:18:52 - INFO - __main__ -     eval_mrr = 0.777
03/25/2025 17:18:53 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-92000-0.777
03/25/2025 17:18:55 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 17:21:44 - INFO - __main__ -   step 92100 loss 0.11958
03/25/2025 17:21:46 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 92100 loss: -0.101845
03/25/2025 17:24:35 - INFO - __main__ -   step 92200 loss 0.11236
03/25/2025 17:24:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 92200 loss: -0.125969
03/25/2025 17:27:26 - INFO - __main__ -   step 92300 loss 0.09327
03/25/2025 17:27:27 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 92300 loss: -0.088204
03/25/2025 17:30:16 - INFO - __main__ -   step 92400 loss 0.10137
03/25/2025 17:30:18 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 92400 loss: -0.119903
03/25/2025 17:33:07 - INFO - __main__ -   step 92500 loss 0.07963
03/25/2025 17:33:09 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 92500 loss: -0.070238
03/25/2025 17:35:58 - INFO - __main__ -   step 92600 loss 0.10066
03/25/2025 17:36:00 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 92600 loss: -0.098186
03/25/2025 17:38:49 - INFO - __main__ -   step 92700 loss 0.10045
03/25/2025 17:38:50 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 92700 loss: -0.104015
03/25/2025 17:41:39 - INFO - __main__ -   step 92800 loss 0.0898
03/25/2025 17:41:41 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 92800 loss: -0.097756
03/25/2025 17:44:30 - INFO - __main__ -   step 92900 loss 0.10233
03/25/2025 17:44:32 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 92900 loss: -0.101057
03/25/2025 17:47:21 - INFO - __main__ -   step 93000 loss 0.10068
03/25/2025 17:47:23 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 93000 loss: -0.094823
03/25/2025 17:47:27 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 17:47:27 - INFO - __main__ -     Num queries = 1400
03/25/2025 17:47:27 - INFO - __main__ -     Num codes = 4360
03/25/2025 17:47:27 - INFO - __main__ -     Batch size = 32
03/25/2025 17:47:55 - INFO - __main__ -     eval_mrr = 0.777
03/25/2025 17:47:56 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-93000-0.777
03/25/2025 17:47:58 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 17:50:47 - INFO - __main__ -   step 93100 loss 0.0969
03/25/2025 17:50:49 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 93100 loss: -0.092054
03/25/2025 17:53:38 - INFO - __main__ -   step 93200 loss 0.08685
03/25/2025 17:53:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 93200 loss: -0.071181
03/25/2025 17:56:29 - INFO - __main__ -   step 93300 loss 0.10128
03/25/2025 17:56:31 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 93300 loss: -0.098207
03/25/2025 17:59:20 - INFO - __main__ -   step 93400 loss 0.09276
03/25/2025 17:59:22 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 93400 loss: -0.113057
03/25/2025 18:02:10 - INFO - __main__ -   step 93500 loss 0.09961
03/25/2025 18:02:12 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 93500 loss: -0.092112
03/25/2025 18:05:01 - INFO - __main__ -   step 93600 loss 0.08723
03/25/2025 18:05:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 93600 loss: -0.082907
03/25/2025 18:07:52 - INFO - __main__ -   step 93700 loss 0.08825
03/25/2025 18:07:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 93700 loss: -0.096843
03/25/2025 18:10:43 - INFO - __main__ -   step 93800 loss 0.0883
03/25/2025 18:10:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 93800 loss: -0.077484
03/25/2025 18:13:34 - INFO - __main__ -   step 93900 loss 0.08255
03/25/2025 18:13:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 93900 loss: -0.087306
03/25/2025 18:16:25 - INFO - __main__ -   step 94000 loss 0.09648
03/25/2025 18:16:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 94000 loss: -0.092041
03/25/2025 18:16:31 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 18:16:31 - INFO - __main__ -     Num queries = 1400
03/25/2025 18:16:31 - INFO - __main__ -     Num codes = 4360
03/25/2025 18:16:31 - INFO - __main__ -     Batch size = 32
03/25/2025 18:16:59 - INFO - __main__ -     eval_mrr = 0.775
03/25/2025 18:17:00 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-94000-0.775
03/25/2025 18:17:02 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 18:19:51 - INFO - __main__ -   step 94100 loss 0.07987
03/25/2025 18:19:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 94100 loss: -0.083874
03/25/2025 18:22:42 - INFO - __main__ -   step 94200 loss 0.09359
03/25/2025 18:22:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 94200 loss: -0.098064
03/25/2025 18:25:33 - INFO - __main__ -   step 94300 loss 0.09039
03/25/2025 18:25:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 94300 loss: -0.085901
03/25/2025 18:28:24 - INFO - __main__ -   step 94400 loss 0.08117
03/25/2025 18:28:25 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 94400 loss: -0.082528
03/25/2025 18:31:15 - INFO - __main__ -   step 94500 loss 0.08459
03/25/2025 18:31:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 94500 loss: -0.078317
03/25/2025 18:34:05 - INFO - __main__ -   step 94600 loss 0.09476
03/25/2025 18:34:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 94600 loss: -0.109814
03/25/2025 18:36:56 - INFO - __main__ -   step 94700 loss 0.1036
03/25/2025 18:36:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 94700 loss: -0.109438
03/25/2025 18:39:47 - INFO - __main__ -   step 94800 loss 0.08245
03/25/2025 18:39:49 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 94800 loss: -0.075552
03/25/2025 18:42:38 - INFO - __main__ -   step 94900 loss 0.09012
03/25/2025 18:42:40 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 94900 loss: -0.097977
03/25/2025 18:45:29 - INFO - __main__ -   step 95000 loss 0.08767
03/25/2025 18:45:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 95000 loss: -0.063954
03/25/2025 18:45:35 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 18:45:35 - INFO - __main__ -     Num queries = 1400
03/25/2025 18:45:35 - INFO - __main__ -     Num codes = 4360
03/25/2025 18:45:35 - INFO - __main__ -     Batch size = 32
03/25/2025 18:46:04 - INFO - __main__ -     eval_mrr = 0.773
03/25/2025 18:46:04 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-95000-0.773
03/25/2025 18:46:07 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 18:48:56 - INFO - __main__ -   step 95100 loss 0.08343
03/25/2025 18:48:57 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 95100 loss: -0.075494
03/25/2025 18:51:46 - INFO - __main__ -   step 95200 loss 0.08053
03/25/2025 18:51:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 95200 loss: -0.074432
03/25/2025 18:54:37 - INFO - __main__ -   step 95300 loss 0.09005
03/25/2025 18:54:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 95300 loss: -0.078792
03/25/2025 18:57:28 - INFO - __main__ -   step 95400 loss 0.09141
03/25/2025 18:57:30 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 95400 loss: -0.091324
03/25/2025 19:00:19 - INFO - __main__ -   step 95500 loss 0.07697
03/25/2025 19:00:20 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 95500 loss: -0.072637
03/25/2025 19:03:09 - INFO - __main__ -   step 95600 loss 0.10469
03/25/2025 19:03:11 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 95600 loss: -0.096426
03/25/2025 19:06:00 - INFO - __main__ -   step 95700 loss 0.08226
03/25/2025 19:06:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 95700 loss: -0.070927
03/25/2025 19:08:51 - INFO - __main__ -   step 95800 loss 0.0929
03/25/2025 19:08:52 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 95800 loss: -0.092943
03/25/2025 19:11:41 - INFO - __main__ -   step 95900 loss 0.07751
03/25/2025 19:11:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 95900 loss: -0.071741
03/25/2025 19:14:32 - INFO - __main__ -   step 96000 loss 0.09334
03/25/2025 19:14:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 96000 loss: -0.072781
03/25/2025 19:14:39 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 19:14:39 - INFO - __main__ -     Num queries = 1400
03/25/2025 19:14:39 - INFO - __main__ -     Num codes = 4360
03/25/2025 19:14:39 - INFO - __main__ -     Batch size = 32
03/25/2025 19:15:07 - INFO - __main__ -     eval_mrr = 0.777
03/25/2025 19:15:07 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-96000-0.777
03/25/2025 19:15:10 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 19:17:59 - INFO - __main__ -   step 96100 loss 0.10389
03/25/2025 19:18:00 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 96100 loss: -0.09517
03/25/2025 19:20:49 - INFO - __main__ -   step 96200 loss 0.08023
03/25/2025 19:20:51 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 96200 loss: -0.081549
03/25/2025 19:23:40 - INFO - __main__ -   step 96300 loss 0.08038
03/25/2025 19:23:42 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 96300 loss: -0.091747
03/25/2025 19:26:30 - INFO - __main__ -   step 96400 loss 0.08866
03/25/2025 19:26:32 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 96400 loss: -0.08374
03/25/2025 19:29:21 - INFO - __main__ -   step 96500 loss 0.07455
03/25/2025 19:29:23 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 96500 loss: -0.05987
03/25/2025 19:32:11 - INFO - __main__ -   step 96600 loss 0.08405
03/25/2025 19:32:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 96600 loss: -0.087028
03/25/2025 19:35:02 - INFO - __main__ -   step 96700 loss 0.07806
03/25/2025 19:35:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 96700 loss: -0.070663
03/25/2025 19:37:53 - INFO - __main__ -   step 96800 loss 0.08337
03/25/2025 19:37:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 96800 loss: -0.095904
03/25/2025 19:40:43 - INFO - __main__ -   step 96900 loss 0.07388
03/25/2025 19:40:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 96900 loss: -0.074155
03/25/2025 19:43:34 - INFO - __main__ -   step 97000 loss 0.06215
03/25/2025 19:43:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 97000 loss: -0.066528
03/25/2025 19:43:40 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 19:43:40 - INFO - __main__ -     Num queries = 1400
03/25/2025 19:43:40 - INFO - __main__ -     Num codes = 4360
03/25/2025 19:43:40 - INFO - __main__ -     Batch size = 32
03/25/2025 19:44:08 - INFO - __main__ -     eval_mrr = 0.774
03/25/2025 19:44:09 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-97000-0.774
03/25/2025 19:44:11 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 19:47:00 - INFO - __main__ -   step 97100 loss 0.09789
03/25/2025 19:47:02 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 97100 loss: -0.095694
03/25/2025 19:49:51 - INFO - __main__ -   step 97200 loss 0.09273
03/25/2025 19:49:53 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 97200 loss: -0.093276
03/25/2025 19:52:42 - INFO - __main__ -   step 97300 loss 0.07976
03/25/2025 19:52:43 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 97300 loss: -0.075106
03/25/2025 19:55:32 - INFO - __main__ -   step 97400 loss 0.09621
03/25/2025 19:55:34 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 97400 loss: -0.080215
03/25/2025 19:58:23 - INFO - __main__ -   step 97500 loss 0.07601
03/25/2025 19:58:24 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 97500 loss: -0.064364
03/25/2025 20:01:13 - INFO - __main__ -   step 97600 loss 0.0945
03/25/2025 20:01:15 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 97600 loss: -0.094938
03/25/2025 20:04:04 - INFO - __main__ -   step 97700 loss 0.07701
03/25/2025 20:04:06 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 97700 loss: -0.075082
03/25/2025 20:06:54 - INFO - __main__ -   step 97800 loss 0.07652
03/25/2025 20:06:56 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 97800 loss: -0.080312
03/25/2025 20:09:45 - INFO - __main__ -   step 97900 loss 0.08544
03/25/2025 20:09:47 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 97900 loss: -0.088241
03/25/2025 20:12:35 - INFO - __main__ -   step 98000 loss 0.07357
03/25/2025 20:12:37 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 98000 loss: -0.071736
03/25/2025 20:12:42 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 20:12:42 - INFO - __main__ -     Num queries = 1400
03/25/2025 20:12:42 - INFO - __main__ -     Num codes = 4360
03/25/2025 20:12:42 - INFO - __main__ -     Batch size = 32
03/25/2025 20:13:10 - INFO - __main__ -     eval_mrr = 0.776
03/25/2025 20:13:11 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-98000-0.776
03/25/2025 20:13:13 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 20:16:02 - INFO - __main__ -   step 98100 loss 0.07736
03/25/2025 20:16:04 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 98100 loss: -0.058901
03/25/2025 20:18:53 - INFO - __main__ -   step 98200 loss 0.08357
03/25/2025 20:18:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 98200 loss: -0.084658
03/25/2025 20:21:43 - INFO - __main__ -   step 98300 loss 0.07718
03/25/2025 20:21:45 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 98300 loss: -0.077235
03/25/2025 20:24:34 - INFO - __main__ -   step 98400 loss 0.07583
03/25/2025 20:24:36 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 98400 loss: -0.091993
03/25/2025 20:27:24 - INFO - __main__ -   step 98500 loss 0.06615
03/25/2025 20:27:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 98500 loss: -0.0823
03/25/2025 20:30:15 - INFO - __main__ -   step 98600 loss 0.09351
03/25/2025 20:30:17 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 98600 loss: -0.090051
03/25/2025 20:33:06 - INFO - __main__ -   step 98700 loss 0.07774
03/25/2025 20:33:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 98700 loss: -0.067502
03/25/2025 20:35:56 - INFO - __main__ -   step 98800 loss 0.07813
03/25/2025 20:35:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 98800 loss: -0.074207
03/25/2025 20:38:47 - INFO - __main__ -   step 98900 loss 0.08863
03/25/2025 20:38:49 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 98900 loss: -0.082297
03/25/2025 20:41:37 - INFO - __main__ -   step 99000 loss 0.07198
03/25/2025 20:41:39 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 99000 loss: -0.072153
03/25/2025 20:41:51 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 20:41:51 - INFO - __main__ -     Num queries = 1400
03/25/2025 20:41:51 - INFO - __main__ -     Num codes = 4360
03/25/2025 20:41:51 - INFO - __main__ -     Batch size = 32
03/25/2025 20:42:19 - INFO - __main__ -     eval_mrr = 0.775
03/25/2025 20:42:20 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-99000-0.775
03/25/2025 20:42:22 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 20:45:11 - INFO - __main__ -   step 99100 loss 0.07053
03/25/2025 20:45:13 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 99100 loss: -0.065796
03/25/2025 20:48:01 - INFO - __main__ -   step 99200 loss 0.08888
03/25/2025 20:48:03 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 99200 loss: -0.101448
03/25/2025 20:50:52 - INFO - __main__ -   step 99300 loss 0.08336
03/25/2025 20:50:54 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 99300 loss: -0.082166
03/25/2025 20:53:43 - INFO - __main__ -   step 99400 loss 0.07195
03/25/2025 20:53:44 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 99400 loss: -0.075262
03/25/2025 20:56:33 - INFO - __main__ -   step 99500 loss 0.10421
03/25/2025 20:56:35 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 99500 loss: -0.105793
03/25/2025 20:59:24 - INFO - __main__ -   step 99600 loss 0.0968
03/25/2025 20:59:26 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 99600 loss: -0.085776
03/25/2025 21:02:15 - INFO - __main__ -   step 99700 loss 0.08282
03/25/2025 21:02:16 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 99700 loss: -0.071739
03/25/2025 21:05:05 - INFO - __main__ -   step 99800 loss 0.08182
03/25/2025 21:05:07 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 99800 loss: -0.079109
03/25/2025 21:07:56 - INFO - __main__ -   step 99900 loss 0.0829
03/25/2025 21:07:58 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 99900 loss: -0.079545
03/25/2025 21:10:47 - INFO - __main__ -   step 100000 loss 0.09034
03/25/2025 21:10:48 - INFO - __main__ -    global steps (step*gradient_accumulation_steps ): 100000 loss: -0.074005
03/25/2025 21:10:53 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 21:10:53 - INFO - __main__ -     Num queries = 1400
03/25/2025 21:10:53 - INFO - __main__ -     Num codes = 4360
03/25/2025 21:10:53 - INFO - __main__ -     Batch size = 32
03/25/2025 21:11:21 - INFO - __main__ -     eval_mrr = 0.776
03/25/2025 21:11:22 - INFO - __main__ -   Saving model checkpoint to ./saved_models/cocosoda/checkpoint-mrr-100000-0.776
03/25/2025 21:11:25 - INFO - __main__ -   Saving optimizer and scheduler states to ./saved_models/cocosoda/checkpoint-last
03/25/2025 21:12:04 - INFO - __main__ -   runnning test
03/25/2025 21:12:13 - INFO - __main__ -   ***** Running evaluation on ruby *****
03/25/2025 21:12:13 - INFO - __main__ -     Num queries = 1261
03/25/2025 21:12:13 - INFO - __main__ -     Num codes = 4360
03/25/2025 21:12:13 - INFO - __main__ -     Batch size = 32
03/25/2025 21:12:40 - INFO - __main__ -   ***** Eval test results *****
03/25/2025 21:12:40 - INFO - __main__ -     R@1 = 0.655
03/25/2025 21:12:40 - INFO - __main__ -     R@10 = 0.898
03/25/2025 21:12:40 - INFO - __main__ -     R@5 = 0.856
03/25/2025 21:12:40 - INFO - __main__ -     eval_mrr = 0.745
03/25/2025 21:12:40 - INFO - utils -   saved dataset in ./saved_models/cocosoda/result.jsonl
